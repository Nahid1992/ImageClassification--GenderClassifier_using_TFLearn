
C:\Users\Nahid\Documents\MachineLearningProjects\#GenderCLassification\GenderClassification_TFLearn>python TFLearn_CNN_GenderClassification.py
C:\Users\Nahid\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
curses is not supported on this machine (please install/reinstall curses for an optimal experience)
Kernel Restarting..
Kernel Restarted..
Training + Validation Data ready to Train
x_train =  (28000, 128, 128, 1)                                                                                                             y_train =  (28000, 2)
x_val =  (12000, 128, 128, 1)
y_val =  (12000, 2)
Data is Ready...
WARNING:tensorflow:From C:\Users\Nahid\AppData\Local\Programs\Python\Python36\lib\site-packages\tflearn\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.
WARNING:tensorflow:From C:\Users\Nahid\AppData\Local\Programs\Python\Python36\lib\site-packages\tflearn\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead                                                                                               Model Created...

Training Started...                                                                                                                         2018-03-12 23:43:35.448153: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
---------------------------------
Run id: Gender_tflearn_run01
Log directory: /tmp/tflearn_logs/
---------------------------------                                                                                                           Training samples: 28000
Validation samples: 12000                                                                                                                   --                                                                                                                                          Training Step: 1  | time: 1.083s
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 00050/28000
Training Step: 2  | total loss: [1m[32m0.61583[0m[0m | time: 1.981s
| Adam | epoch: 001 | loss: 0.61583 - acc: 0.5220 -- iter: 00100/28000
Training Step: 3  | total loss: [1m[32m0.78558[0m[0m | time: 2.859s
| Adam | epoch: 001 | loss: 0.78558 - acc: 0.5040 -- iter: 00150/28000
Training Step: 4  | total loss: [1m[32m0.70613[0m[0m | time: 3.728s
| Adam | epoch: 001 | loss: 0.70613 - acc: 0.5460 -- iter: 00200/28000
Training Step: 5  | total loss: [1m[32m0.70380[0m[0m | time: 4.589s
| Adam | epoch: 001 | loss: 0.70380 - acc: 0.4726 -- iter: 00250/28000
Training Step: 6  | total loss: [1m[32m0.69811[0m[0m | time: 5.429s
| Adam | epoch: 001 | loss: 0.69811 - acc: 0.4774 -- iter: 00300/28000
Training Step: 7  | total loss: [1m[32m0.68932[0m[0m | time: 6.289s
| Adam | epoch: 001 | loss: 0.68932 - acc: 0.5389 -- iter: 00350/28000
Training Step: 8  | total loss: [1m[32m0.68876[0m[0m | time: 7.162s
| Adam | epoch: 001 | loss: 0.68876 - acc: 0.5508 -- iter: 00400/28000
Training Step: 9  | total loss: [1m[32m0.68667[0m[0m | time: 8.033s
| Adam | epoch: 001 | loss: 0.68667 - acc: 0.5663 -- iter: 00450/28000
Training Step: 10  | total loss: [1m[32m0.69913[0m[0m | time: 8.908s
| Adam | epoch: 001 | loss: 0.69913 - acc: 0.4531 -- iter: 00500/28000
Training Step: 11  | total loss: [1m[32m0.69078[0m[0m | time: 9.774s
| Adam | epoch: 001 | loss: 0.69078 - acc: 0.5227 -- iter: 00550/28000
Training Step: 12  | total loss: [1m[32m0.69260[0m[0m | time: 10.644s
| Adam | epoch: 001 | loss: 0.69260 - acc: 0.4765 -- iter: 00600/28000
Training Step: 13  | total loss: [1m[32m0.69253[0m[0m | time: 11.489s
| Adam | epoch: 001 | loss: 0.69253 - acc: 0.4437 -- iter: 00650/28000
Training Step: 14  | total loss: [1m[32m0.68982[0m[0m | time: 12.346s
| Adam | epoch: 001 | loss: 0.68982 - acc: 0.5486 -- iter: 00700/28000
Training Step: 15  | total loss: [1m[32m0.68859[0m[0m | time: 13.291s
| Adam | epoch: 001 | loss: 0.68859 - acc: 0.6000 -- iter: 00750/28000
Training Step: 16  | total loss: [1m[32m0.68789[0m[0m | time: 14.253s
| Adam | epoch: 001 | loss: 0.68789 - acc: 0.6150 -- iter: 00800/28000
Training Step: 17  | total loss: [1m[32m0.68716[0m[0m | time: 15.234s
| Adam | epoch: 001 | loss: 0.68716 - acc: 0.6528 -- iter: 00850/28000
Training Step: 18  | total loss: [1m[32m0.68584[0m[0m | time: 16.228s
| Adam | epoch: 001 | loss: 0.68584 - acc: 0.6761 -- iter: 00900/28000
Training Step: 19  | total loss: [1m[32m0.68656[0m[0m | time: 17.214s
| Adam | epoch: 001 | loss: 0.68656 - acc: 0.6307 -- iter: 00950/28000
Training Step: 20  | total loss: [1m[32m0.68342[0m[0m | time: 18.191s
| Adam | epoch: 001 | loss: 0.68342 - acc: 0.6466 -- iter: 01000/28000
Training Step: 21  | total loss: [1m[32m0.68135[0m[0m | time: 19.192s
| Adam | epoch: 001 | loss: 0.68135 - acc: 0.6321 -- iter: 01050/28000
Training Step: 22  | total loss: [1m[32m0.68281[0m[0m | time: 20.183s
| Adam | epoch: 001 | loss: 0.68281 - acc: 0.6045 -- iter: 01100/28000
Training Step: 23  | total loss: [1m[32m0.67281[0m[0m | time: 21.185s
| Adam | epoch: 001 | loss: 0.67281 - acc: 0.6148 -- iter: 01150/28000
Training Step: 24  | total loss: [1m[32m0.67444[0m[0m | time: 22.178s
| Adam | epoch: 001 | loss: 0.67444 - acc: 0.5938 -- iter: 01200/28000
Training Step: 25  | total loss: [1m[32m0.67840[0m[0m | time: 49.102s
| Adam | epoch: 001 | loss: 0.67840 - acc: 0.5682 | val_loss: 0.66808 - val_acc: 0.6388 -- iter: 01250/28000
--
Training Step: 26  | total loss: [1m[32m0.67414[0m[0m | time: 50.713s
| Adam | epoch: 001 | loss: 0.67414 - acc: 0.5978 -- iter: 01300/28000
Training Step: 27  | total loss: [1m[32m0.67456[0m[0m | time: 51.910s
| Adam | epoch: 001 | loss: 0.67456 - acc: 0.6035 -- iter: 01350/28000
Training Step: 28  | total loss: [1m[32m0.67380[0m[0m | time: 53.062s
| Adam | epoch: 001 | loss: 0.67380 - acc: 0.6026 -- iter: 01400/28000
Training Step: 29  | total loss: [1m[32m0.66983[0m[0m | time: 54.251s
| Adam | epoch: 001 | loss: 0.66983 - acc: 0.6166 -- iter: 01450/28000
Training Step: 30  | total loss: [1m[32m0.67141[0m[0m | time: 55.405s
| Adam | epoch: 001 | loss: 0.67141 - acc: 0.6127 -- iter: 01500/28000
Training Step: 31  | total loss: [1m[32m0.67370[0m[0m | time: 56.780s
| Adam | epoch: 001 | loss: 0.67370 - acc: 0.6005 -- iter: 01550/28000
Training Step: 32  | total loss: [1m[32m0.66458[0m[0m | time: 58.104s
| Adam | epoch: 001 | loss: 0.66458 - acc: 0.6229 -- iter: 01600/28000
Training Step: 33  | total loss: [1m[32m0.65523[0m[0m | time: 59.676s
| Adam | epoch: 001 | loss: 0.65523 - acc: 0.6442 -- iter: 01650/28000
Training Step: 34  | total loss: [1m[32m0.65953[0m[0m | time: 61.050s
| Adam | epoch: 001 | loss: 0.65953 - acc: 0.6304 -- iter: 01700/28000
Training Step: 35  | total loss: [1m[32m0.64104[0m[0m | time: 62.429s
| Adam | epoch: 001 | loss: 0.64104 - acc: 0.6576 -- iter: 01750/28000
Training Step: 36  | total loss: [1m[32m0.63286[0m[0m | time: 63.694s
| Adam | epoch: 001 | loss: 0.63286 - acc: 0.6581 -- iter: 01800/28000
Training Step: 37  | total loss: [1m[32m0.63550[0m[0m | time: 65.177s
| Adam | epoch: 001 | loss: 0.63550 - acc: 0.6544 -- iter: 01850/28000
Training Step: 38  | total loss: [1m[32m0.64046[0m[0m | time: 66.574s
| Adam | epoch: 001 | loss: 0.64046 - acc: 0.6321 -- iter: 01900/28000
Training Step: 39  | total loss: [1m[32m0.67412[0m[0m | time: 67.690s
| Adam | epoch: 001 | loss: 0.67412 - acc: 0.5953 -- iter: 01950/28000
Training Step: 40  | total loss: [1m[32m0.66362[0m[0m | time: 69.094s
| Adam | epoch: 001 | loss: 0.66362 - acc: 0.6224 -- iter: 02000/28000
Training Step: 41  | total loss: [1m[32m0.65774[0m[0m | time: 70.267s
| Adam | epoch: 001 | loss: 0.65774 - acc: 0.6330 -- iter: 02050/28000
Training Step: 42  | total loss: [1m[32m0.66955[0m[0m | time: 71.472s
| Adam | epoch: 001 | loss: 0.66955 - acc: 0.6199 -- iter: 02100/28000
Training Step: 43  | total loss: [1m[32m0.66867[0m[0m | time: 72.796s
| Adam | epoch: 001 | loss: 0.66867 - acc: 0.6163 -- iter: 02150/28000
Training Step: 44  | total loss: [1m[32m0.66408[0m[0m | time: 74.094s
| Adam | epoch: 001 | loss: 0.66408 - acc: 0.6274 -- iter: 02200/28000
Training Step: 45  | total loss: [1m[32m0.66472[0m[0m | time: 75.400s
| Adam | epoch: 001 | loss: 0.66472 - acc: 0.6261 -- iter: 02250/28000
Training Step: 46  | total loss: [1m[32m0.65188[0m[0m | time: 76.654s
| Adam | epoch: 001 | loss: 0.65188 - acc: 0.6318 -- iter: 02300/28000
Training Step: 47  | total loss: [1m[32m0.64217[0m[0m | time: 77.863s
| Adam | epoch: 001 | loss: 0.64217 - acc: 0.6560 -- iter: 02350/28000
Training Step: 48  | total loss: [1m[32m0.63325[0m[0m | time: 79.163s
| Adam | epoch: 001 | loss: 0.63325 - acc: 0.6727 -- iter: 02400/28000
Training Step: 49  | total loss: [1m[32m0.62853[0m[0m | time: 80.338s
| Adam | epoch: 001 | loss: 0.62853 - acc: 0.6770 -- iter: 02450/28000
Training Step: 50  | total loss: [1m[32m0.62888[0m[0m | time: 114.865s
| Adam | epoch: 001 | loss: 0.62888 - acc: 0.6682 | val_loss: 0.61312 - val_acc: 0.6613 -- iter: 02500/28000
--
Training Step: 51  | total loss: [1m[32m0.62544[0m[0m | time: 116.155s
| Adam | epoch: 001 | loss: 0.62544 - acc: 0.6608 -- iter: 02550/28000
Training Step: 52  | total loss: [1m[32m0.62157[0m[0m | time: 117.361s
| Adam | epoch: 001 | loss: 0.62157 - acc: 0.6607 -- iter: 02600/28000
Training Step: 53  | total loss: [1m[32m0.62489[0m[0m | time: 118.502s
| Adam | epoch: 001 | loss: 0.62489 - acc: 0.6606 -- iter: 02650/28000
Training Step: 54  | total loss: [1m[32m0.62671[0m[0m | time: 119.622s
| Adam | epoch: 001 | loss: 0.62671 - acc: 0.6576 -- iter: 02700/28000
Training Step: 55  | total loss: [1m[32m0.61291[0m[0m | time: 120.809s
| Adam | epoch: 001 | loss: 0.61291 - acc: 0.6751 -- iter: 02750/28000
Training Step: 56  | total loss: [1m[32m0.61088[0m[0m | time: 126.741s
| Adam | epoch: 001 | loss: 0.61088 - acc: 0.6842 -- iter: 02800/28000
Training Step: 57  | total loss: [1m[32m0.60755[0m[0m | time: 133.135s
| Adam | epoch: 001 | loss: 0.60755 - acc: 0.6753 -- iter: 02850/28000
Training Step: 58  | total loss: [1m[32m0.60458[0m[0m | time: 139.329s
| Adam | epoch: 001 | loss: 0.60458 - acc: 0.6732 -- iter: 02900/28000
Training Step: 59  | total loss: [1m[32m0.61050[0m[0m | time: 145.605s
| Adam | epoch: 001 | loss: 0.61050 - acc: 0.6715 -- iter: 02950/28000
Training Step: 60  | total loss: [1m[32m0.60192[0m[0m | time: 151.714s
| Adam | epoch: 001 | loss: 0.60192 - acc: 0.6832 -- iter: 03000/28000
Training Step: 61  | total loss: [1m[32m0.61626[0m[0m | time: 158.527s
| Adam | epoch: 001 | loss: 0.61626 - acc: 0.6697 -- iter: 03050/28000
Training Step: 62  | total loss: [1m[32m0.61959[0m[0m | time: 161.988s
| Adam | epoch: 001 | loss: 0.61959 - acc: 0.6556 -- iter: 03100/28000
Training Step: 63  | total loss: [1m[32m0.61751[0m[0m | time: 165.141s
| Adam | epoch: 001 | loss: 0.61751 - acc: 0.6435 -- iter: 03150/28000
Training Step: 64  | total loss: [1m[32m0.60637[0m[0m | time: 166.058s
| Adam | epoch: 001 | loss: 0.60637 - acc: 0.6606 -- iter: 03200/28000
Training Step: 65  | total loss: [1m[32m0.61178[0m[0m | time: 166.968s
| Adam | epoch: 001 | loss: 0.61178 - acc: 0.6531 -- iter: 03250/28000
Training Step: 66  | total loss: [1m[32m0.61144[0m[0m | time: 167.903s
| Adam | epoch: 001 | loss: 0.61144 - acc: 0.6588 -- iter: 03300/28000
Training Step: 67  | total loss: [1m[32m0.60851[0m[0m | time: 168.847s
| Adam | epoch: 001 | loss: 0.60851 - acc: 0.6589 -- iter: 03350/28000
Training Step: 68  | total loss: [1m[32m0.60849[0m[0m | time: 169.750s
| Adam | epoch: 001 | loss: 0.60849 - acc: 0.6567 -- iter: 03400/28000                                                                      Training Step: 69  | total loss: [1m[32m0.59657[0m[0m | time: 170.656s
| Adam | epoch: 001 | loss: 0.59657 - acc: 0.6781 -- iter: 03450/28000
Training Step: 70  | total loss: [1m[32m0.58911[0m[0m | time: 171.653s
| Adam | epoch: 001 | loss: 0.58911 - acc: 0.6806 -- iter: 03500/28000
Training Step: 71  | total loss: [1m[32m0.58269[0m[0m | time: 172.580s
| Adam | epoch: 001 | loss: 0.58269 - acc: 0.6874 -- iter: 03550/28000
Training Step: 72  | total loss: [1m[32m0.57774[0m[0m | time: 173.533s
| Adam | epoch: 001 | loss: 0.57774 - acc: 0.6933 -- iter: 03600/28000
Training Step: 73  | total loss: [1m[32m0.56157[0m[0m | time: 174.473s
| Adam | epoch: 001 | loss: 0.56157 - acc: 0.7141 -- iter: 03650/28000
Training Step: 74  | total loss: [1m[32m0.55588[0m[0m | time: 175.413s
| Adam | epoch: 001 | loss: 0.55588 - acc: 0.7191 -- iter: 03700/28000
Training Step: 75  | total loss: [1m[32m0.54978[0m[0m | time: 200.039s
| Adam | epoch: 001 | loss: 0.54978 - acc: 0.7344 | val_loss: 0.49399 - val_acc: 0.7922 -- iter: 03750/28000
--
Training Step: 76  | total loss: [1m[32m0.54853[0m[0m | time: 201.154s
| Adam | epoch: 001 | loss: 0.54853 - acc: 0.7350 -- iter: 03800/28000
Training Step: 77  | total loss: [1m[32m0.54261[0m[0m | time: 202.264s
| Adam | epoch: 001 | loss: 0.54261 - acc: 0.7398 -- iter: 03850/28000
Training Step: 78  | total loss: [1m[32m0.54281[0m[0m | time: 203.433s
| Adam | epoch: 001 | loss: 0.54281 - acc: 0.7398 -- iter: 03900/28000
Training Step: 79  | total loss: [1m[32m0.54286[0m[0m | time: 204.494s
| Adam | epoch: 001 | loss: 0.54286 - acc: 0.7357 -- iter: 03950/28000
Training Step: 80  | total loss: [1m[32m0.53628[0m[0m | time: 205.556s
| Adam | epoch: 001 | loss: 0.53628 - acc: 0.7463 -- iter: 04000/28000
Training Step: 81  | total loss: [1m[32m0.53916[0m[0m | time: 206.597s
| Adam | epoch: 001 | loss: 0.53916 - acc: 0.7437 -- iter: 04050/28000
Training Step: 82  | total loss: [1m[32m0.52914[0m[0m | time: 207.695s
| Adam | epoch: 001 | loss: 0.52914 - acc: 0.7513 -- iter: 04100/28000
Training Step: 83  | total loss: [1m[32m0.55979[0m[0m | time: 208.819s
| Adam | epoch: 001 | loss: 0.55979 - acc: 0.7282 -- iter: 04150/28000
Training Step: 84  | total loss: [1m[32m0.56421[0m[0m | time: 209.842s
| Adam | epoch: 001 | loss: 0.56421 - acc: 0.7254 -- iter: 04200/28000                                                                      Training Step: 85  | total loss: [1m[32m0.55180[0m[0m | time: 210.882s
| Adam | epoch: 001 | loss: 0.55180 - acc: 0.7348 -- iter: 04250/28000
Training Step: 86  | total loss: [1m[32m0.53985[0m[0m | time: 212.034s
| Adam | epoch: 001 | loss: 0.53985 - acc: 0.7413 -- iter: 04300/28000
Training Step: 87  | total loss: [1m[32m0.57690[0m[0m | time: 213.106s
| Adam | epoch: 001 | loss: 0.57690 - acc: 0.7152 -- iter: 04350/28000
Training Step: 88  | total loss: [1m[32m0.56590[0m[0m | time: 214.149s
| Adam | epoch: 001 | loss: 0.56590 - acc: 0.7217 -- iter: 04400/28000
Training Step: 89  | total loss: [1m[32m0.55218[0m[0m | time: 215.255s
| Adam | epoch: 001 | loss: 0.55218 - acc: 0.7315 -- iter: 04450/28000
Training Step: 90  | total loss: [1m[32m0.54658[0m[0m | time: 216.357s
| Adam | epoch: 001 | loss: 0.54658 - acc: 0.7284 -- iter: 04500/28000
Training Step: 91  | total loss: [1m[32m0.54868[0m[0m | time: 217.439s
| Adam | epoch: 001 | loss: 0.54868 - acc: 0.7235 -- iter: 04550/28000
Training Step: 92  | total loss: [1m[32m0.54668[0m[0m | time: 218.495s
| Adam | epoch: 001 | loss: 0.54668 - acc: 0.7252 -- iter: 04600/28000
Training Step: 93  | total loss: [1m[32m0.53427[0m[0m | time: 219.519s
| Adam | epoch: 001 | loss: 0.53427 - acc: 0.7367 -- iter: 04650/28000
Training Step: 94  | total loss: [1m[32m0.52626[0m[0m | time: 220.593s
| Adam | epoch: 001 | loss: 0.52626 - acc: 0.7430 -- iter: 04700/28000
Training Step: 95  | total loss: [1m[32m0.52268[0m[0m | time: 221.633s
| Adam | epoch: 001 | loss: 0.52268 - acc: 0.7507 -- iter: 04750/28000
Training Step: 96  | total loss: [1m[32m0.51757[0m[0m | time: 222.712s
| Adam | epoch: 001 | loss: 0.51757 - acc: 0.7516 -- iter: 04800/28000
Training Step: 97  | total loss: [1m[32m0.51217[0m[0m | time: 223.783s
| Adam | epoch: 001 | loss: 0.51217 - acc: 0.7565 -- iter: 04850/28000
Training Step: 98  | total loss: [1m[32m0.51429[0m[0m | time: 224.828s
| Adam | epoch: 001 | loss: 0.51429 - acc: 0.7548 -- iter: 04900/28000
Training Step: 99  | total loss: [1m[32m0.49678[0m[0m | time: 225.884s
| Adam | epoch: 001 | loss: 0.49678 - acc: 0.7733 -- iter: 04950/28000
Training Step: 100  | total loss: [1m[32m0.49073[0m[0m | time: 257.753s
| Adam | epoch: 001 | loss: 0.49073 - acc: 0.7720 | val_loss: 0.43031 - val_acc: 0.8156 -- iter: 05000/28000
--
Training Step: 101  | total loss: [1m[32m0.48908[0m[0m | time: 258.964s
| Adam | epoch: 001 | loss: 0.48908 - acc: 0.7728 -- iter: 05050/28000
Training Step: 102  | total loss: [1m[32m0.48925[0m[0m | time: 260.094s
| Adam | epoch: 001 | loss: 0.48925 - acc: 0.7795 -- iter: 05100/28000
Training Step: 103  | total loss: [1m[32m0.48456[0m[0m | time: 261.217s
| Adam | epoch: 001 | loss: 0.48456 - acc: 0.7856 -- iter: 05150/28000
Training Step: 104  | total loss: [1m[32m0.47292[0m[0m | time: 262.327s
| Adam | epoch: 001 | loss: 0.47292 - acc: 0.7950 -- iter: 05200/28000
Training Step: 105  | total loss: [1m[32m0.47274[0m[0m | time: 263.490s
| Adam | epoch: 001 | loss: 0.47274 - acc: 0.7915 -- iter: 05250/28000
Training Step: 106  | total loss: [1m[32m0.46117[0m[0m | time: 264.597s
| Adam | epoch: 001 | loss: 0.46117 - acc: 0.7944 -- iter: 05300/28000
Training Step: 107  | total loss: [1m[32m0.45879[0m[0m | time: 265.743s
| Adam | epoch: 001 | loss: 0.45879 - acc: 0.7989 -- iter: 05350/28000
Training Step: 108  | total loss: [1m[32m0.44657[0m[0m | time: 266.909s
| Adam | epoch: 001 | loss: 0.44657 - acc: 0.8110 -- iter: 05400/28000
Training Step: 109  | total loss: [1m[32m0.43224[0m[0m | time: 268.029s
| Adam | epoch: 001 | loss: 0.43224 - acc: 0.8239 -- iter: 05450/28000
Training Step: 110  | total loss: [1m[32m0.43660[0m[0m | time: 269.174s
| Adam | epoch: 001 | loss: 0.43660 - acc: 0.8175 -- iter: 05500/28000
Training Step: 111  | total loss: [1m[32m0.43782[0m[0m | time: 270.326s
| Adam | epoch: 001 | loss: 0.43782 - acc: 0.8158 -- iter: 05550/28000
Training Step: 112  | total loss: [1m[32m0.42712[0m[0m | time: 271.486s
| Adam | epoch: 001 | loss: 0.42712 - acc: 0.8182 -- iter: 05600/28000
Training Step: 113  | total loss: [1m[32m0.42007[0m[0m | time: 272.686s
| Adam | epoch: 001 | loss: 0.42007 - acc: 0.8244 -- iter: 05650/28000
Training Step: 114  | total loss: [1m[32m0.41704[0m[0m | time: 273.873s
| Adam | epoch: 001 | loss: 0.41704 - acc: 0.8259 -- iter: 05700/28000
Training Step: 115  | total loss: [1m[32m0.40704[0m[0m | time: 274.996s
| Adam | epoch: 001 | loss: 0.40704 - acc: 0.8254 -- iter: 05750/28000
Training Step: 116  | total loss: [1m[32m0.41017[0m[0m | time: 276.117s
| Adam | epoch: 001 | loss: 0.41017 - acc: 0.8248 -- iter: 05800/28000
Training Step: 117  | total loss: [1m[32m0.41091[0m[0m | time: 277.261s
| Adam | epoch: 001 | loss: 0.41091 - acc: 0.8243 -- iter: 05850/28000
Training Step: 118  | total loss: [1m[32m0.40893[0m[0m | time: 278.458s
| Adam | epoch: 001 | loss: 0.40893 - acc: 0.8259 -- iter: 05900/28000
Training Step: 119  | total loss: [1m[32m0.40339[0m[0m | time: 279.755s
| Adam | epoch: 001 | loss: 0.40339 - acc: 0.8313 -- iter: 05950/28000
Training Step: 120  | total loss: [1m[32m0.37991[0m[0m | time: 280.972s
| Adam | epoch: 001 | loss: 0.37991 - acc: 0.8482 -- iter: 06000/28000
Training Step: 121  | total loss: [1m[32m0.38853[0m[0m | time: 282.115s
| Adam | epoch: 001 | loss: 0.38853 - acc: 0.8454 -- iter: 06050/28000
Training Step: 122  | total loss: [1m[32m0.38022[0m[0m | time: 283.281s
| Adam | epoch: 001 | loss: 0.38022 - acc: 0.8528 -- iter: 06100/28000
Training Step: 123  | total loss: [1m[32m0.37250[0m[0m | time: 284.454s
| Adam | epoch: 001 | loss: 0.37250 - acc: 0.8535 -- iter: 06150/28000
Training Step: 124  | total loss: [1m[32m0.36627[0m[0m | time: 285.653s
| Adam | epoch: 001 | loss: 0.36627 - acc: 0.8542 -- iter: 06200/28000
Training Step: 125  | total loss: [1m[32m0.37213[0m[0m | time: 315.504s
| Adam | epoch: 001 | loss: 0.37213 - acc: 0.8428 | val_loss: 0.36386 - val_acc: 0.8472 -- iter: 06250/28000
--
Training Step: 126  | total loss: [1m[32m0.37541[0m[0m | time: 316.738s
| Adam | epoch: 001 | loss: 0.37541 - acc: 0.8385 -- iter: 06300/28000
Training Step: 127  | total loss: [1m[32m0.37239[0m[0m | time: 317.886s
| Adam | epoch: 001 | loss: 0.37239 - acc: 0.8366 -- iter: 06350/28000
Training Step: 128  | total loss: [1m[32m0.39428[0m[0m | time: 319.075s
| Adam | epoch: 001 | loss: 0.39428 - acc: 0.8290 -- iter: 06400/28000
Training Step: 129  | total loss: [1m[32m0.39941[0m[0m | time: 320.356s
| Adam | epoch: 001 | loss: 0.39941 - acc: 0.8321 -- iter: 06450/28000
Training Step: 130  | total loss: [1m[32m0.38804[0m[0m | time: 321.497s
| Adam | epoch: 001 | loss: 0.38804 - acc: 0.8329 -- iter: 06500/28000
Training Step: 131  | total loss: [1m[32m0.37698[0m[0m | time: 322.756s
| Adam | epoch: 001 | loss: 0.37698 - acc: 0.8396 -- iter: 06550/28000
Training Step: 132  | total loss: [1m[32m0.37058[0m[0m | time: 323.975s
| Adam | epoch: 001 | loss: 0.37058 - acc: 0.8436 -- iter: 06600/28000
Training Step: 133  | total loss: [1m[32m0.37254[0m[0m | time: 325.112s
| Adam | epoch: 001 | loss: 0.37254 - acc: 0.8433 -- iter: 06650/28000
Training Step: 134  | total loss: [1m[32m0.37461[0m[0m | time: 326.231s
| Adam | epoch: 001 | loss: 0.37461 - acc: 0.8429 -- iter: 06700/28000
Training Step: 135  | total loss: [1m[32m0.37407[0m[0m | time: 327.370s
| Adam | epoch: 001 | loss: 0.37407 - acc: 0.8466 -- iter: 06750/28000
Training Step: 136  | total loss: [1m[32m0.38211[0m[0m | time: 328.505s
| Adam | epoch: 001 | loss: 0.38211 - acc: 0.8420 -- iter: 06800/28000
Training Step: 137  | total loss: [1m[32m0.37632[0m[0m | time: 329.752s
| Adam | epoch: 001 | loss: 0.37632 - acc: 0.8418 -- iter: 06850/28000
Training Step: 138  | total loss: [1m[32m0.38834[0m[0m | time: 330.915s
| Adam | epoch: 001 | loss: 0.38834 - acc: 0.8376 -- iter: 06900/28000
Training Step: 139  | total loss: [1m[32m0.38998[0m[0m | time: 332.069s
| Adam | epoch: 001 | loss: 0.38998 - acc: 0.8318 -- iter: 06950/28000
Training Step: 140  | total loss: [1m[32m0.39189[0m[0m | time: 333.327s
| Adam | epoch: 001 | loss: 0.39189 - acc: 0.8247 -- iter: 07000/28000
Training Step: 141  | total loss: [1m[32m0.39381[0m[0m | time: 334.476s
| Adam | epoch: 001 | loss: 0.39381 - acc: 0.8262 -- iter: 07050/28000
Training Step: 142  | total loss: [1m[32m0.39499[0m[0m | time: 335.680s
| Adam | epoch: 001 | loss: 0.39499 - acc: 0.8296 -- iter: 07100/28000
Training Step: 143  | total loss: [1m[32m0.40872[0m[0m | time: 336.918s
| Adam | epoch: 001 | loss: 0.40872 - acc: 0.8206 -- iter: 07150/28000
Training Step: 144  | total loss: [1m[32m0.40614[0m[0m | time: 338.071s
| Adam | epoch: 001 | loss: 0.40614 - acc: 0.8226 -- iter: 07200/28000
Training Step: 145  | total loss: [1m[32m0.39225[0m[0m | time: 339.246s
| Adam | epoch: 001 | loss: 0.39225 - acc: 0.8303 -- iter: 07250/28000
Training Step: 146  | total loss: [1m[32m0.39869[0m[0m | time: 340.365s
| Adam | epoch: 001 | loss: 0.39869 - acc: 0.8273 -- iter: 07300/28000
Training Step: 147  | total loss: [1m[32m0.40231[0m[0m | time: 341.528s
| Adam | epoch: 001 | loss: 0.40231 - acc: 0.8285 -- iter: 07350/28000
Training Step: 148  | total loss: [1m[32m0.39233[0m[0m | time: 342.693s
| Adam | epoch: 001 | loss: 0.39233 - acc: 0.8317 -- iter: 07400/28000
Training Step: 149  | total loss: [1m[32m0.39173[0m[0m | time: 343.868s
| Adam | epoch: 001 | loss: 0.39173 - acc: 0.8325 -- iter: 07450/28000
Training Step: 150  | total loss: [1m[32m0.38449[0m[0m | time: 374.635s
| Adam | epoch: 001 | loss: 0.38449 - acc: 0.8353 | val_loss: 0.35764 - val_acc: 0.8527 -- iter: 07500/28000
--
Training Step: 151  | total loss: [1m[32m0.36863[0m[0m | time: 375.862s
| Adam | epoch: 001 | loss: 0.36863 - acc: 0.8437 -- iter: 07550/28000
Training Step: 152  | total loss: [1m[32m0.37237[0m[0m | time: 377.026s
| Adam | epoch: 001 | loss: 0.37237 - acc: 0.8434 -- iter: 07600/28000
Training Step: 153  | total loss: [1m[32m0.37018[0m[0m | time: 378.267s
| Adam | epoch: 001 | loss: 0.37018 - acc: 0.8510 -- iter: 07650/28000
Training Step: 154  | total loss: [1m[32m0.37017[0m[0m | time: 379.420s
| Adam | epoch: 001 | loss: 0.37017 - acc: 0.8479 -- iter: 07700/28000
Training Step: 155  | total loss: [1m[32m0.36443[0m[0m | time: 380.630s
| Adam | epoch: 001 | loss: 0.36443 - acc: 0.8511 -- iter: 07750/28000
Training Step: 156  | total loss: [1m[32m0.37073[0m[0m | time: 381.778s
| Adam | epoch: 001 | loss: 0.37073 - acc: 0.8520 -- iter: 07800/28000
Training Step: 157  | total loss: [1m[32m0.37082[0m[0m | time: 382.970s
| Adam | epoch: 001 | loss: 0.37082 - acc: 0.8528 -- iter: 07850/28000
Training Step: 158  | total loss: [1m[32m0.36034[0m[0m | time: 384.192s
| Adam | epoch: 001 | loss: 0.36034 - acc: 0.8595 -- iter: 07900/28000
Training Step: 159  | total loss: [1m[32m0.35500[0m[0m | time: 385.391s
| Adam | epoch: 001 | loss: 0.35500 - acc: 0.8576 -- iter: 07950/28000
Training Step: 160  | total loss: [1m[32m0.34097[0m[0m | time: 386.626s
| Adam | epoch: 001 | loss: 0.34097 - acc: 0.8658 -- iter: 08000/28000
Training Step: 161  | total loss: [1m[32m0.33991[0m[0m | time: 387.793s
| Adam | epoch: 001 | loss: 0.33991 - acc: 0.8652 -- iter: 08050/28000
Training Step: 162  | total loss: [1m[32m0.34161[0m[0m | time: 388.967s
| Adam | epoch: 001 | loss: 0.34161 - acc: 0.8687 -- iter: 08100/28000
Training Step: 163  | total loss: [1m[32m0.35319[0m[0m | time: 390.152s
| Adam | epoch: 001 | loss: 0.35319 - acc: 0.8598 -- iter: 08150/28000
Training Step: 164  | total loss: [1m[32m0.35550[0m[0m | time: 391.352s
| Adam | epoch: 001 | loss: 0.35550 - acc: 0.8559 -- iter: 08200/28000
Training Step: 165  | total loss: [1m[32m0.35433[0m[0m | time: 392.507s
| Adam | epoch: 001 | loss: 0.35433 - acc: 0.8563 -- iter: 08250/28000
Training Step: 166  | total loss: [1m[32m0.34568[0m[0m | time: 393.660s
| Adam | epoch: 001 | loss: 0.34568 - acc: 0.8606 -- iter: 08300/28000
Training Step: 167  | total loss: [1m[32m0.34737[0m[0m | time: 394.854s
| Adam | epoch: 001 | loss: 0.34737 - acc: 0.8606 -- iter: 08350/28000
Training Step: 168  | total loss: [1m[32m0.33798[0m[0m | time: 396.007s
| Adam | epoch: 001 | loss: 0.33798 - acc: 0.8665 -- iter: 08400/28000
Training Step: 169  | total loss: [1m[32m0.32737[0m[0m | time: 397.186s
| Adam | epoch: 001 | loss: 0.32737 - acc: 0.8679 -- iter: 08450/28000
Training Step: 170  | total loss: [1m[32m0.32668[0m[0m | time: 398.387s
| Adam | epoch: 001 | loss: 0.32668 - acc: 0.8631 -- iter: 08500/28000
Training Step: 171  | total loss: [1m[32m0.32892[0m[0m | time: 399.594s
| Adam | epoch: 001 | loss: 0.32892 - acc: 0.8648 -- iter: 08550/28000
Training Step: 172  | total loss: [1m[32m0.32547[0m[0m | time: 400.784s
| Adam | epoch: 001 | loss: 0.32547 - acc: 0.8703 -- iter: 08600/28000
Training Step: 173  | total loss: [1m[32m0.32395[0m[0m | time: 401.939s
| Adam | epoch: 001 | loss: 0.32395 - acc: 0.8753 -- iter: 08650/28000
Training Step: 174  | total loss: [1m[32m0.32007[0m[0m | time: 403.082s
| Adam | epoch: 001 | loss: 0.32007 - acc: 0.8737 -- iter: 08700/28000
Training Step: 175  | total loss: [1m[32m0.32768[0m[0m | time: 434.978s
| Adam | epoch: 001 | loss: 0.32768 - acc: 0.8704 | val_loss: 0.31536 - val_acc: 0.8750 -- iter: 08750/28000
--
Training Step: 176  | total loss: [1m[32m0.31588[0m[0m | time: 436.330s
| Adam | epoch: 001 | loss: 0.31588 - acc: 0.8773 -- iter: 08800/28000
Training Step: 177  | total loss: [1m[32m0.30800[0m[0m | time: 437.531s
| Adam | epoch: 001 | loss: 0.30800 - acc: 0.8816 -- iter: 08850/28000
Training Step: 178  | total loss: [1m[32m0.31603[0m[0m | time: 438.708s
| Adam | epoch: 001 | loss: 0.31603 - acc: 0.8774 -- iter: 08900/28000
Training Step: 179  | total loss: [1m[32m0.31143[0m[0m | time: 439.881s
| Adam | epoch: 001 | loss: 0.31143 - acc: 0.8817 -- iter: 08950/28000
Training Step: 180  | total loss: [1m[32m0.30352[0m[0m | time: 441.119s
| Adam | epoch: 001 | loss: 0.30352 - acc: 0.8915 -- iter: 09000/28000
Training Step: 181  | total loss: [1m[32m0.31054[0m[0m | time: 442.275s
| Adam | epoch: 001 | loss: 0.31054 - acc: 0.8804 -- iter: 09050/28000
Training Step: 182  | total loss: [1m[32m0.30798[0m[0m | time: 443.465s
| Adam | epoch: 001 | loss: 0.30798 - acc: 0.8823 -- iter: 09100/28000
Training Step: 183  | total loss: [1m[32m0.29114[0m[0m | time: 444.599s
| Adam | epoch: 001 | loss: 0.29114 - acc: 0.8901 -- iter: 09150/28000
Training Step: 184  | total loss: [1m[32m0.30976[0m[0m | time: 445.741s
| Adam | epoch: 001 | loss: 0.30976 - acc: 0.8751 -- iter: 09200/28000
Training Step: 185  | total loss: [1m[32m0.31244[0m[0m | time: 446.873s
| Adam | epoch: 001 | loss: 0.31244 - acc: 0.8696 -- iter: 09250/28000
Training Step: 186  | total loss: [1m[32m0.32411[0m[0m | time: 448.044s
| Adam | epoch: 001 | loss: 0.32411 - acc: 0.8646 -- iter: 09300/28000
Training Step: 187  | total loss: [1m[32m0.31096[0m[0m | time: 449.189s
| Adam | epoch: 001 | loss: 0.31096 - acc: 0.8682 -- iter: 09350/28000
Training Step: 188  | total loss: [1m[32m0.31102[0m[0m | time: 450.355s
| Adam | epoch: 001 | loss: 0.31102 - acc: 0.8693 -- iter: 09400/28000
Training Step: 189  | total loss: [1m[32m0.31930[0m[0m | time: 451.543s
| Adam | epoch: 001 | loss: 0.31930 - acc: 0.8604 -- iter: 09450/28000
Training Step: 190  | total loss: [1m[32m0.30561[0m[0m | time: 452.736s
| Adam | epoch: 001 | loss: 0.30561 - acc: 0.8704 -- iter: 09500/28000
Training Step: 191  | total loss: [1m[32m0.30379[0m[0m | time: 453.931s
| Adam | epoch: 001 | loss: 0.30379 - acc: 0.8713 -- iter: 09550/28000
Training Step: 192  | total loss: [1m[32m0.30241[0m[0m | time: 455.094s
| Adam | epoch: 001 | loss: 0.30241 - acc: 0.8762 -- iter: 09600/28000
Training Step: 193  | total loss: [1m[32m0.29789[0m[0m | time: 456.231s
| Adam | epoch: 001 | loss: 0.29789 - acc: 0.8786 -- iter: 09650/28000
Training Step: 194  | total loss: [1m[32m0.28885[0m[0m | time: 457.363s
| Adam | epoch: 001 | loss: 0.28885 - acc: 0.8807 -- iter: 09700/28000
Training Step: 195  | total loss: [1m[32m0.28087[0m[0m | time: 458.514s
| Adam | epoch: 001 | loss: 0.28087 - acc: 0.8826 -- iter: 09750/28000
Training Step: 196  | total loss: [1m[32m0.29158[0m[0m | time: 459.704s
| Adam | epoch: 001 | loss: 0.29158 - acc: 0.8804 -- iter: 09800/28000
Training Step: 197  | total loss: [1m[32m0.28027[0m[0m | time: 460.896s
| Adam | epoch: 001 | loss: 0.28027 - acc: 0.8863 -- iter: 09850/28000
Training Step: 198  | total loss: [1m[32m0.28438[0m[0m | time: 462.088s
| Adam | epoch: 001 | loss: 0.28438 - acc: 0.8837 -- iter: 09900/28000
Training Step: 199  | total loss: [1m[32m0.28614[0m[0m | time: 463.287s
| Adam | epoch: 001 | loss: 0.28614 - acc: 0.8813 -- iter: 09950/28000
Training Step: 200  | total loss: [1m[32m0.28197[0m[0m | time: 493.475s
| Adam | epoch: 001 | loss: 0.28197 - acc: 0.8812 | val_loss: 0.30217 - val_acc: 0.8732 -- iter: 10000/28000
--
Training Step: 201  | total loss: [1m[32m0.27548[0m[0m | time: 494.734s
| Adam | epoch: 001 | loss: 0.27548 - acc: 0.8851 -- iter: 10050/28000
Training Step: 202  | total loss: [1m[32m0.27748[0m[0m | time: 495.935s
| Adam | epoch: 001 | loss: 0.27748 - acc: 0.8846 -- iter: 10100/28000
Training Step: 203  | total loss: [1m[32m0.27685[0m[0m | time: 497.090s
| Adam | epoch: 001 | loss: 0.27685 - acc: 0.8861 -- iter: 10150/28000
Training Step: 204  | total loss: [1m[32m0.27576[0m[0m | time: 498.393s
| Adam | epoch: 001 | loss: 0.27576 - acc: 0.8835 -- iter: 10200/28000
Training Step: 205  | total loss: [1m[32m0.27857[0m[0m | time: 499.500s
| Adam | epoch: 001 | loss: 0.27857 - acc: 0.8792 -- iter: 10250/28000
Training Step: 206  | total loss: [1m[32m0.29169[0m[0m | time: 500.664s
| Adam | epoch: 001 | loss: 0.29169 - acc: 0.8712 -- iter: 10300/28000
Training Step: 207  | total loss: [1m[32m0.30051[0m[0m | time: 501.810s
| Adam | epoch: 001 | loss: 0.30051 - acc: 0.8681 -- iter: 10350/28000
Training Step: 208  | total loss: [1m[32m0.30547[0m[0m | time: 503.001s
| Adam | epoch: 001 | loss: 0.30547 - acc: 0.8673 -- iter: 10400/28000
Training Step: 209  | total loss: [1m[32m0.29409[0m[0m | time: 504.208s
| Adam | epoch: 001 | loss: 0.29409 - acc: 0.8746 -- iter: 10450/28000
Training Step: 210  | total loss: [1m[32m0.28613[0m[0m | time: 505.404s
| Adam | epoch: 001 | loss: 0.28613 - acc: 0.8771 -- iter: 10500/28000
Training Step: 211  | total loss: [1m[32m0.27283[0m[0m | time: 506.562s
| Adam | epoch: 001 | loss: 0.27283 - acc: 0.8854 -- iter: 10550/28000
Training Step: 212  | total loss: [1m[32m0.28651[0m[0m | time: 507.722s
| Adam | epoch: 001 | loss: 0.28651 - acc: 0.8809 -- iter: 10600/28000
Training Step: 213  | total loss: [1m[32m0.27864[0m[0m | time: 508.865s
| Adam | epoch: 001 | loss: 0.27864 - acc: 0.8848 -- iter: 10650/28000
Training Step: 214  | total loss: [1m[32m0.27671[0m[0m | time: 510.050s
| Adam | epoch: 001 | loss: 0.27671 - acc: 0.8803 -- iter: 10700/28000
Training Step: 215  | total loss: [1m[32m0.27307[0m[0m | time: 511.223s
| Adam | epoch: 001 | loss: 0.27307 - acc: 0.8863 -- iter: 10750/28000
Training Step: 216  | total loss: [1m[32m0.27662[0m[0m | time: 512.376s
| Adam | epoch: 001 | loss: 0.27662 - acc: 0.8856 -- iter: 10800/28000
Training Step: 217  | total loss: [1m[32m0.26993[0m[0m | time: 513.655s
| Adam | epoch: 001 | loss: 0.26993 - acc: 0.8891 -- iter: 10850/28000
Training Step: 218  | total loss: [1m[32m0.26731[0m[0m | time: 514.865s
| Adam | epoch: 001 | loss: 0.26731 - acc: 0.8902 -- iter: 10900/28000
Training Step: 219  | total loss: [1m[32m0.28227[0m[0m | time: 516.049s
| Adam | epoch: 001 | loss: 0.28227 - acc: 0.8852 -- iter: 10950/28000
Training Step: 220  | total loss: [1m[32m0.27850[0m[0m | time: 517.225s
| Adam | epoch: 001 | loss: 0.27850 - acc: 0.8866 -- iter: 11000/28000
Training Step: 221  | total loss: [1m[32m0.27559[0m[0m | time: 518.472s
| Adam | epoch: 001 | loss: 0.27559 - acc: 0.8840 -- iter: 11050/28000
Training Step: 222  | total loss: [1m[32m0.27466[0m[0m | time: 519.613s
| Adam | epoch: 001 | loss: 0.27466 - acc: 0.8836 -- iter: 11100/28000
Training Step: 223  | total loss: [1m[32m0.27557[0m[0m | time: 520.760s
| Adam | epoch: 001 | loss: 0.27557 - acc: 0.8832 -- iter: 11150/28000
Training Step: 224  | total loss: [1m[32m0.27749[0m[0m | time: 521.975s
| Adam | epoch: 001 | loss: 0.27749 - acc: 0.8869 -- iter: 11200/28000
Training Step: 225  | total loss: [1m[32m0.27898[0m[0m | time: 551.845s
| Adam | epoch: 001 | loss: 0.27898 - acc: 0.8862 | val_loss: 0.27098 - val_acc: 0.8947 -- iter: 11250/28000
--
Training Step: 226  | total loss: [1m[32m0.27345[0m[0m | time: 553.056s
| Adam | epoch: 001 | loss: 0.27345 - acc: 0.8876 -- iter: 11300/28000
Training Step: 227  | total loss: [1m[32m0.27160[0m[0m | time: 554.209s
| Adam | epoch: 001 | loss: 0.27160 - acc: 0.8888 -- iter: 11350/28000
Training Step: 228  | total loss: [1m[32m0.26114[0m[0m | time: 555.343s
| Adam | epoch: 001 | loss: 0.26114 - acc: 0.8939 -- iter: 11400/28000
Training Step: 229  | total loss: [1m[32m0.26226[0m[0m | time: 556.584s
| Adam | epoch: 001 | loss: 0.26226 - acc: 0.8926 -- iter: 11450/28000
Training Step: 230  | total loss: [1m[32m0.26385[0m[0m | time: 557.939s
| Adam | epoch: 001 | loss: 0.26385 - acc: 0.8913 -- iter: 11500/28000
Training Step: 231  | total loss: [1m[32m0.27008[0m[0m | time: 559.659s
| Adam | epoch: 001 | loss: 0.27008 - acc: 0.8882 -- iter: 11550/28000
Training Step: 232  | total loss: [1m[32m0.26068[0m[0m | time: 561.187s
| Adam | epoch: 001 | loss: 0.26068 - acc: 0.8953 -- iter: 11600/28000
Training Step: 233  | total loss: [1m[32m0.27566[0m[0m | time: 562.596s
| Adam | epoch: 001 | loss: 0.27566 - acc: 0.8958 -- iter: 11650/28000
Training Step: 234  | total loss: [1m[32m0.27055[0m[0m | time: 563.907s
| Adam | epoch: 001 | loss: 0.27055 - acc: 0.9042 -- iter: 11700/28000
Training Step: 235  | total loss: [1m[32m0.26914[0m[0m | time: 565.143s
| Adam | epoch: 001 | loss: 0.26914 - acc: 0.9058 -- iter: 11750/28000
Training Step: 236  | total loss: [1m[32m0.27464[0m[0m | time: 566.188s
| Adam | epoch: 001 | loss: 0.27464 - acc: 0.9032 -- iter: 11800/28000
Training Step: 237  | total loss: [1m[32m0.28073[0m[0m | time: 567.394s
| Adam | epoch: 001 | loss: 0.28073 - acc: 0.9029 -- iter: 11850/28000
Training Step: 238  | total loss: [1m[32m0.28295[0m[0m | time: 568.546s
| Adam | epoch: 001 | loss: 0.28295 - acc: 0.8986 -- iter: 11900/28000
Training Step: 239  | total loss: [1m[32m0.28413[0m[0m | time: 569.636s
| Adam | epoch: 001 | loss: 0.28413 - acc: 0.8948 -- iter: 11950/28000
Training Step: 240  | total loss: [1m[32m0.28598[0m[0m | time: 570.787s
| Adam | epoch: 001 | loss: 0.28598 - acc: 0.8933 -- iter: 12000/28000
Training Step: 241  | total loss: [1m[32m0.30633[0m[0m | time: 571.905s
| Adam | epoch: 001 | loss: 0.30633 - acc: 0.8880 -- iter: 12050/28000
Training Step: 242  | total loss: [1m[32m0.31138[0m[0m | time: 573.106s
| Adam | epoch: 001 | loss: 0.31138 - acc: 0.8812 -- iter: 12100/28000
Training Step: 243  | total loss: [1m[32m0.30580[0m[0m | time: 574.291s
| Adam | epoch: 001 | loss: 0.30580 - acc: 0.8830 -- iter: 12150/28000
Training Step: 244  | total loss: [1m[32m0.31029[0m[0m | time: 575.577s
| Adam | epoch: 001 | loss: 0.31029 - acc: 0.8767 -- iter: 12200/28000
Training Step: 245  | total loss: [1m[32m0.30197[0m[0m | time: 576.867s
| Adam | epoch: 001 | loss: 0.30197 - acc: 0.8791 -- iter: 12250/28000
Training Step: 246  | total loss: [1m[32m0.29298[0m[0m | time: 578.131s
| Adam | epoch: 001 | loss: 0.29298 - acc: 0.8832 -- iter: 12300/28000
Training Step: 247  | total loss: [1m[32m0.29770[0m[0m | time: 579.398s
| Adam | epoch: 001 | loss: 0.29770 - acc: 0.8788 -- iter: 12350/28000
Training Step: 248  | total loss: [1m[32m0.29557[0m[0m | time: 580.562s
| Adam | epoch: 001 | loss: 0.29557 - acc: 0.8850 -- iter: 12400/28000
Training Step: 249  | total loss: [1m[32m0.28837[0m[0m | time: 581.715s
| Adam | epoch: 001 | loss: 0.28837 - acc: 0.8905 -- iter: 12450/28000
Training Step: 250  | total loss: [1m[32m0.29952[0m[0m | time: 612.178s
| Adam | epoch: 001 | loss: 0.29952 - acc: 0.8894 | val_loss: 0.27117 - val_acc: 0.8982 -- iter: 12500/28000
--
Training Step: 251  | total loss: [1m[32m0.29170[0m[0m | time: 613.368s
| Adam | epoch: 001 | loss: 0.29170 - acc: 0.8945 -- iter: 12550/28000
Training Step: 252  | total loss: [1m[32m0.28720[0m[0m | time: 614.509s
| Adam | epoch: 001 | loss: 0.28720 - acc: 0.8930 -- iter: 12600/28000
Training Step: 253  | total loss: [1m[32m0.27872[0m[0m | time: 615.659s
| Adam | epoch: 001 | loss: 0.27872 - acc: 0.8977 -- iter: 12650/28000
Training Step: 254  | total loss: [1m[32m0.27970[0m[0m | time: 617.074s
| Adam | epoch: 001 | loss: 0.27970 - acc: 0.8940 -- iter: 12700/28000
Training Step: 255  | total loss: [1m[32m0.28077[0m[0m | time: 618.233s
| Adam | epoch: 001 | loss: 0.28077 - acc: 0.8946 -- iter: 12750/28000
Training Step: 256  | total loss: [1m[32m0.28169[0m[0m | time: 619.355s
| Adam | epoch: 001 | loss: 0.28169 - acc: 0.8971 -- iter: 12800/28000
Training Step: 257  | total loss: [1m[32m0.26763[0m[0m | time: 620.483s
| Adam | epoch: 001 | loss: 0.26763 - acc: 0.9034 -- iter: 12850/28000
Training Step: 258  | total loss: [1m[32m0.26831[0m[0m | time: 621.627s
| Adam | epoch: 001 | loss: 0.26831 - acc: 0.9091 -- iter: 12900/28000
Training Step: 259  | total loss: [1m[32m0.26952[0m[0m | time: 622.761s
| Adam | epoch: 001 | loss: 0.26952 - acc: 0.9121 -- iter: 12950/28000
Training Step: 260  | total loss: [1m[32m0.26522[0m[0m | time: 623.992s
| Adam | epoch: 001 | loss: 0.26522 - acc: 0.9129 -- iter: 13000/28000
Training Step: 261  | total loss: [1m[32m0.26519[0m[0m | time: 625.243s
| Adam | epoch: 001 | loss: 0.26519 - acc: 0.9116 -- iter: 13050/28000
Training Step: 262  | total loss: [1m[32m0.26365[0m[0m | time: 626.516s
| Adam | epoch: 001 | loss: 0.26365 - acc: 0.9145 -- iter: 13100/28000
Training Step: 263  | total loss: [1m[32m0.27129[0m[0m | time: 627.754s
| Adam | epoch: 001 | loss: 0.27129 - acc: 0.9110 -- iter: 13150/28000
Training Step: 264  | total loss: [1m[32m0.27017[0m[0m | time: 628.928s
| Adam | epoch: 001 | loss: 0.27017 - acc: 0.9079 -- iter: 13200/28000
Training Step: 265  | total loss: [1m[32m0.28204[0m[0m | time: 630.153s
| Adam | epoch: 001 | loss: 0.28204 - acc: 0.9011 -- iter: 13250/28000
Training Step: 266  | total loss: [1m[32m0.28080[0m[0m | time: 631.381s
| Adam | epoch: 001 | loss: 0.28080 - acc: 0.9030 -- iter: 13300/28000
Training Step: 267  | total loss: [1m[32m0.29276[0m[0m | time: 632.522s
| Adam | epoch: 001 | loss: 0.29276 - acc: 0.9007 -- iter: 13350/28000
Training Step: 268  | total loss: [1m[32m0.30488[0m[0m | time: 633.654s
| Adam | epoch: 001 | loss: 0.30488 - acc: 0.8926 -- iter: 13400/28000
Training Step: 269  | total loss: [1m[32m0.29861[0m[0m | time: 634.904s
| Adam | epoch: 001 | loss: 0.29861 - acc: 0.8914 -- iter: 13450/28000
Training Step: 270  | total loss: [1m[32m0.29298[0m[0m | time: 636.123s
| Adam | epoch: 001 | loss: 0.29298 - acc: 0.8902 -- iter: 13500/28000
Training Step: 271  | total loss: [1m[32m0.28687[0m[0m | time: 637.311s
| Adam | epoch: 001 | loss: 0.28687 - acc: 0.8912 -- iter: 13550/28000
Training Step: 272  | total loss: [1m[32m0.28940[0m[0m | time: 638.564s
| Adam | epoch: 001 | loss: 0.28940 - acc: 0.8921 -- iter: 13600/28000
Training Step: 273  | total loss: [1m[32m0.28758[0m[0m | time: 639.717s
| Adam | epoch: 001 | loss: 0.28758 - acc: 0.8889 -- iter: 13650/28000
Training Step: 274  | total loss: [1m[32m0.27214[0m[0m | time: 640.836s
| Adam | epoch: 001 | loss: 0.27214 - acc: 0.8960 -- iter: 13700/28000
Training Step: 275  | total loss: [1m[32m0.27944[0m[0m | time: 672.028s
| Adam | epoch: 001 | loss: 0.27944 - acc: 0.8864 | val_loss: 0.28962 - val_acc: 0.8827 -- iter: 13750/28000
--
Training Step: 276  | total loss: [1m[32m0.28526[0m[0m | time: 673.343s
| Adam | epoch: 001 | loss: 0.28526 - acc: 0.8798 -- iter: 13800/28000
Training Step: 277  | total loss: [1m[32m0.29290[0m[0m | time: 674.592s
| Adam | epoch: 001 | loss: 0.29290 - acc: 0.8758 -- iter: 13850/28000
Training Step: 278  | total loss: [1m[32m0.28424[0m[0m | time: 675.753s
| Adam | epoch: 001 | loss: 0.28424 - acc: 0.8802 -- iter: 13900/28000
Training Step: 279  | total loss: [1m[32m0.27965[0m[0m | time: 676.865s
| Adam | epoch: 001 | loss: 0.27965 - acc: 0.8862 -- iter: 13950/28000
Training Step: 280  | total loss: [1m[32m0.26891[0m[0m | time: 678.038s
| Adam | epoch: 001 | loss: 0.26891 - acc: 0.8916 -- iter: 14000/28000
Training Step: 281  | total loss: [1m[32m0.26835[0m[0m | time: 679.184s
| Adam | epoch: 001 | loss: 0.26835 - acc: 0.8924 -- iter: 14050/28000
Training Step: 282  | total loss: [1m[32m0.27175[0m[0m | time: 680.358s
| Adam | epoch: 001 | loss: 0.27175 - acc: 0.8912 -- iter: 14100/28000
Training Step: 283  | total loss: [1m[32m0.26556[0m[0m | time: 681.533s
| Adam | epoch: 001 | loss: 0.26556 - acc: 0.8941 -- iter: 14150/28000
Training Step: 284  | total loss: [1m[32m0.26553[0m[0m | time: 682.873s
| Adam | epoch: 001 | loss: 0.26553 - acc: 0.8946 -- iter: 14200/28000
Training Step: 285  | total loss: [1m[32m0.26902[0m[0m | time: 684.237s
| Adam | epoch: 001 | loss: 0.26902 - acc: 0.8912 -- iter: 14250/28000
Training Step: 286  | total loss: [1m[32m0.25320[0m[0m | time: 685.851s
| Adam | epoch: 001 | loss: 0.25320 - acc: 0.9001 -- iter: 14300/28000
Training Step: 287  | total loss: [1m[32m0.24878[0m[0m | time: 687.140s
| Adam | epoch: 001 | loss: 0.24878 - acc: 0.9041 -- iter: 14350/28000
Training Step: 288  | total loss: [1m[32m0.25374[0m[0m | time: 688.472s
| Adam | epoch: 001 | loss: 0.25374 - acc: 0.9057 -- iter: 14400/28000
Training Step: 289  | total loss: [1m[32m0.24420[0m[0m | time: 689.853s
| Adam | epoch: 001 | loss: 0.24420 - acc: 0.9091 -- iter: 14450/28000
Training Step: 290  | total loss: [1m[32m0.23428[0m[0m | time: 691.058s
| Adam | epoch: 001 | loss: 0.23428 - acc: 0.9102 -- iter: 14500/28000
Training Step: 291  | total loss: [1m[32m0.24880[0m[0m | time: 692.270s
| Adam | epoch: 001 | loss: 0.24880 - acc: 0.9092 -- iter: 14550/28000
Training Step: 292  | total loss: [1m[32m0.25155[0m[0m | time: 693.479s
| Adam | epoch: 001 | loss: 0.25155 - acc: 0.9082 -- iter: 14600/28000
Training Step: 293  | total loss: [1m[32m0.24488[0m[0m | time: 694.680s
| Adam | epoch: 001 | loss: 0.24488 - acc: 0.9094 -- iter: 14650/28000
Training Step: 294  | total loss: [1m[32m0.26254[0m[0m | time: 695.801s
| Adam | epoch: 001 | loss: 0.26254 - acc: 0.9005 -- iter: 14700/28000
Training Step: 295  | total loss: [1m[32m0.26747[0m[0m | time: 696.932s
| Adam | epoch: 001 | loss: 0.26747 - acc: 0.9004 -- iter: 14750/28000
Training Step: 296  | total loss: [1m[32m0.28003[0m[0m | time: 698.208s
| Adam | epoch: 001 | loss: 0.28003 - acc: 0.8924 -- iter: 14800/28000
Training Step: 297  | total loss: [1m[32m0.26875[0m[0m | time: 699.423s
| Adam | epoch: 001 | loss: 0.26875 - acc: 0.8991 -- iter: 14850/28000
Training Step: 298  | total loss: [1m[32m0.25750[0m[0m | time: 700.737s
| Adam | epoch: 001 | loss: 0.25750 - acc: 0.9052 -- iter: 14900/28000
Training Step: 299  | total loss: [1m[32m0.25034[0m[0m | time: 701.959s
| Adam | epoch: 001 | loss: 0.25034 - acc: 0.9087 -- iter: 14950/28000
Training Step: 300  | total loss: [1m[32m0.25885[0m[0m | time: 733.028s
| Adam | epoch: 001 | loss: 0.25885 - acc: 0.9058 | val_loss: 0.29691 - val_acc: 0.8779 -- iter: 15000/28000
--
Training Step: 301  | total loss: [1m[32m0.27681[0m[0m | time: 734.336s
| Adam | epoch: 001 | loss: 0.27681 - acc: 0.9013 -- iter: 15050/28000
Training Step: 302  | total loss: [1m[32m0.29775[0m[0m | time: 735.639s
| Adam | epoch: 001 | loss: 0.29775 - acc: 0.8971 -- iter: 15100/28000
Training Step: 303  | total loss: [1m[32m0.30730[0m[0m | time: 737.077s
| Adam | epoch: 001 | loss: 0.30730 - acc: 0.8954 -- iter: 15150/28000
Training Step: 304  | total loss: [1m[32m0.30830[0m[0m | time: 738.372s
| Adam | epoch: 001 | loss: 0.30830 - acc: 0.8939 -- iter: 15200/28000
Training Step: 305  | total loss: [1m[32m0.29236[0m[0m | time: 739.605s
| Adam | epoch: 001 | loss: 0.29236 - acc: 0.8965 -- iter: 15250/28000
Training Step: 306  | total loss: [1m[32m0.28702[0m[0m | time: 740.837s
| Adam | epoch: 001 | loss: 0.28702 - acc: 0.8968 -- iter: 15300/28000
Training Step: 307  | total loss: [1m[32m0.28807[0m[0m | time: 741.891s
| Adam | epoch: 001 | loss: 0.28807 - acc: 0.8952 -- iter: 15350/28000
Training Step: 308  | total loss: [1m[32m0.27748[0m[0m | time: 743.234s
| Adam | epoch: 001 | loss: 0.27748 - acc: 0.9036 -- iter: 15400/28000
Training Step: 309  | total loss: [1m[32m0.26629[0m[0m | time: 744.484s
| Adam | epoch: 001 | loss: 0.26629 - acc: 0.9113 -- iter: 15450/28000
Training Step: 310  | total loss: [1m[32m0.24773[0m[0m | time: 745.695s
| Adam | epoch: 001 | loss: 0.24773 - acc: 0.9201 -- iter: 15500/28000
Training Step: 311  | total loss: [1m[32m0.23761[0m[0m | time: 746.873s
| Adam | epoch: 001 | loss: 0.23761 - acc: 0.9261 -- iter: 15550/28000
Training Step: 312  | total loss: [1m[32m0.24963[0m[0m | time: 748.122s
| Adam | epoch: 001 | loss: 0.24963 - acc: 0.9195 -- iter: 15600/28000
Training Step: 313  | total loss: [1m[32m0.24776[0m[0m | time: 749.555s
| Adam | epoch: 001 | loss: 0.24776 - acc: 0.9176 -- iter: 15650/28000
Training Step: 314  | total loss: [1m[32m0.23979[0m[0m | time: 750.769s
| Adam | epoch: 001 | loss: 0.23979 - acc: 0.9158 -- iter: 15700/28000
Training Step: 315  | total loss: [1m[32m0.23960[0m[0m | time: 751.973s
| Adam | epoch: 001 | loss: 0.23960 - acc: 0.9162 -- iter: 15750/28000
Training Step: 316  | total loss: [1m[32m0.24757[0m[0m | time: 753.173s
| Adam | epoch: 001 | loss: 0.24757 - acc: 0.9126 -- iter: 15800/28000
Training Step: 317  | total loss: [1m[32m0.24254[0m[0m | time: 754.348s
| Adam | epoch: 001 | loss: 0.24254 - acc: 0.9133 -- iter: 15850/28000
Training Step: 318  | total loss: [1m[32m0.23448[0m[0m | time: 755.520s
| Adam | epoch: 001 | loss: 0.23448 - acc: 0.9140 -- iter: 15900/28000
Training Step: 319  | total loss: [1m[32m0.23249[0m[0m | time: 756.698s
| Adam | epoch: 001 | loss: 0.23249 - acc: 0.9146 -- iter: 15950/28000
Training Step: 320  | total loss: [1m[32m0.22616[0m[0m | time: 758.106s
| Adam | epoch: 001 | loss: 0.22616 - acc: 0.9151 -- iter: 16000/28000
Training Step: 321  | total loss: [1m[32m0.24973[0m[0m | time: 759.171s
| Adam | epoch: 001 | loss: 0.24973 - acc: 0.9036 -- iter: 16050/28000
Training Step: 322  | total loss: [1m[32m0.27077[0m[0m | time: 760.339s
| Adam | epoch: 001 | loss: 0.27077 - acc: 0.8993 -- iter: 16100/28000
Training Step: 323  | total loss: [1m[32m0.27494[0m[0m | time: 761.487s
| Adam | epoch: 001 | loss: 0.27494 - acc: 0.8953 -- iter: 16150/28000
Training Step: 324  | total loss: [1m[32m0.26277[0m[0m | time: 762.666s
| Adam | epoch: 001 | loss: 0.26277 - acc: 0.8978 -- iter: 16200/28000
Training Step: 325  | total loss: [1m[32m0.26563[0m[0m | time: 795.481s
| Adam | epoch: 001 | loss: 0.26563 - acc: 0.8940 | val_loss: 0.26176 - val_acc: 0.8977 -- iter: 16250/28000
--
Training Step: 326  | total loss: [1m[32m0.25818[0m[0m | time: 796.803s
| Adam | epoch: 001 | loss: 0.25818 - acc: 0.9006 -- iter: 16300/28000
Training Step: 327  | total loss: [1m[32m0.26938[0m[0m | time: 798.004s
| Adam | epoch: 001 | loss: 0.26938 - acc: 0.9006 -- iter: 16350/28000
Training Step: 328  | total loss: [1m[32m0.26219[0m[0m | time: 799.189s
| Adam | epoch: 001 | loss: 0.26219 - acc: 0.9045 -- iter: 16400/28000
Training Step: 329  | total loss: [1m[32m0.25947[0m[0m | time: 800.423s
| Adam | epoch: 001 | loss: 0.25947 - acc: 0.9081 -- iter: 16450/28000
Training Step: 330  | total loss: [1m[32m0.25919[0m[0m | time: 801.615s
| Adam | epoch: 001 | loss: 0.25919 - acc: 0.9033 -- iter: 16500/28000
Training Step: 331  | total loss: [1m[32m0.25703[0m[0m | time: 803.159s
| Adam | epoch: 001 | loss: 0.25703 - acc: 0.9029 -- iter: 16550/28000
Training Step: 332  | total loss: [1m[32m0.25829[0m[0m | time: 804.586s
| Adam | epoch: 001 | loss: 0.25829 - acc: 0.9086 -- iter: 16600/28000
Training Step: 333  | total loss: [1m[32m0.24819[0m[0m | time: 805.636s
| Adam | epoch: 001 | loss: 0.24819 - acc: 0.9118 -- iter: 16650/28000
Training Step: 334  | total loss: [1m[32m0.24855[0m[0m | time: 806.773s
| Adam | epoch: 001 | loss: 0.24855 - acc: 0.9126 -- iter: 16700/28000
Training Step: 335  | total loss: [1m[32m0.26029[0m[0m | time: 807.908s
| Adam | epoch: 001 | loss: 0.26029 - acc: 0.9073 -- iter: 16750/28000
Training Step: 336  | total loss: [1m[32m0.25006[0m[0m | time: 809.193s
| Adam | epoch: 001 | loss: 0.25006 - acc: 0.9126 -- iter: 16800/28000
Training Step: 337  | total loss: [1m[32m0.24468[0m[0m | time: 810.235s
| Adam | epoch: 001 | loss: 0.24468 - acc: 0.9113 -- iter: 16850/28000
Training Step: 338  | total loss: [1m[32m0.24396[0m[0m | time: 811.335s
| Adam | epoch: 001 | loss: 0.24396 - acc: 0.9102 -- iter: 16900/28000
Training Step: 339  | total loss: [1m[32m0.23762[0m[0m | time: 812.450s
| Adam | epoch: 001 | loss: 0.23762 - acc: 0.9152 -- iter: 16950/28000
Training Step: 340  | total loss: [1m[32m0.23732[0m[0m | time: 813.601s
| Adam | epoch: 001 | loss: 0.23732 - acc: 0.9197 -- iter: 17000/28000
Training Step: 341  | total loss: [1m[32m0.24860[0m[0m | time: 814.776s
| Adam | epoch: 001 | loss: 0.24860 - acc: 0.9097 -- iter: 17050/28000
Training Step: 342  | total loss: [1m[32m0.24052[0m[0m | time: 816.502s
| Adam | epoch: 001 | loss: 0.24052 - acc: 0.9127 -- iter: 17100/28000
Training Step: 343  | total loss: [1m[32m0.23877[0m[0m | time: 817.883s
| Adam | epoch: 001 | loss: 0.23877 - acc: 0.9155 -- iter: 17150/28000
Training Step: 344  | total loss: [1m[32m0.22628[0m[0m | time: 819.686s
| Adam | epoch: 001 | loss: 0.22628 - acc: 0.9199 -- iter: 17200/28000
Training Step: 345  | total loss: [1m[32m0.22420[0m[0m | time: 821.203s
| Adam | epoch: 001 | loss: 0.22420 - acc: 0.9159 -- iter: 17250/28000
Training Step: 346  | total loss: [1m[32m0.22812[0m[0m | time: 822.631s
| Adam | epoch: 001 | loss: 0.22812 - acc: 0.9143 -- iter: 17300/28000
Training Step: 347  | total loss: [1m[32m0.22210[0m[0m | time: 823.897s
| Adam | epoch: 001 | loss: 0.22210 - acc: 0.9169 -- iter: 17350/28000
Training Step: 348  | total loss: [1m[32m0.23268[0m[0m | time: 825.171s
| Adam | epoch: 001 | loss: 0.23268 - acc: 0.9152 -- iter: 17400/28000
Training Step: 349  | total loss: [1m[32m0.26098[0m[0m | time: 826.608s
| Adam | epoch: 001 | loss: 0.26098 - acc: 0.9037 -- iter: 17450/28000
Training Step: 350  | total loss: [1m[32m0.26501[0m[0m | time: 858.832s
| Adam | epoch: 001 | loss: 0.26501 - acc: 0.8993 | val_loss: 0.38322 - val_acc: 0.8417 -- iter: 17500/28000
--
Training Step: 351  | total loss: [1m[32m0.25969[0m[0m | time: 860.015s
| Adam | epoch: 001 | loss: 0.25969 - acc: 0.8974 -- iter: 17550/28000
Training Step: 352  | total loss: [1m[32m0.26412[0m[0m | time: 861.211s
| Adam | epoch: 001 | loss: 0.26412 - acc: 0.8976 -- iter: 17600/28000
Training Step: 353  | total loss: [1m[32m0.26090[0m[0m | time: 862.384s
| Adam | epoch: 001 | loss: 0.26090 - acc: 0.8959 -- iter: 17650/28000
Training Step: 354  | total loss: [1m[32m0.25967[0m[0m | time: 863.570s
| Adam | epoch: 001 | loss: 0.25967 - acc: 0.8943 -- iter: 17700/28000
Training Step: 355  | total loss: [1m[32m0.27936[0m[0m | time: 864.764s
| Adam | epoch: 001 | loss: 0.27936 - acc: 0.8849 -- iter: 17750/28000
Training Step: 356  | total loss: [1m[32m0.27947[0m[0m | time: 866.050s
| Adam | epoch: 001 | loss: 0.27947 - acc: 0.8904 -- iter: 17800/28000
Training Step: 357  | total loss: [1m[32m0.28021[0m[0m | time: 867.656s
| Adam | epoch: 001 | loss: 0.28021 - acc: 0.8933 -- iter: 17850/28000
Training Step: 358  | total loss: [1m[32m0.27439[0m[0m | time: 869.236s
| Adam | epoch: 001 | loss: 0.27439 - acc: 0.8980 -- iter: 17900/28000
Training Step: 359  | total loss: [1m[32m0.29007[0m[0m | time: 870.600s
| Adam | epoch: 001 | loss: 0.29007 - acc: 0.8902 -- iter: 17950/28000
Training Step: 360  | total loss: [1m[32m0.29082[0m[0m | time: 871.758s
| Adam | epoch: 001 | loss: 0.29082 - acc: 0.8892 -- iter: 18000/28000
Training Step: 361  | total loss: [1m[32m0.27412[0m[0m | time: 872.947s
| Adam | epoch: 001 | loss: 0.27412 - acc: 0.8963 -- iter: 18050/28000
Training Step: 362  | total loss: [1m[32m0.27306[0m[0m | time: 874.325s
| Adam | epoch: 001 | loss: 0.27306 - acc: 0.8986 -- iter: 18100/28000
Training Step: 363  | total loss: [1m[32m0.26684[0m[0m | time: 875.649s
| Adam | epoch: 001 | loss: 0.26684 - acc: 0.9008 -- iter: 18150/28000
Training Step: 364  | total loss: [1m[32m0.25484[0m[0m | time: 876.736s
| Adam | epoch: 001 | loss: 0.25484 - acc: 0.9047 -- iter: 18200/28000
Training Step: 365  | total loss: [1m[32m0.25254[0m[0m | time: 878.034s
| Adam | epoch: 001 | loss: 0.25254 - acc: 0.9062 -- iter: 18250/28000
Training Step: 366  | total loss: [1m[32m0.25904[0m[0m | time: 879.341s
| Adam | epoch: 001 | loss: 0.25904 - acc: 0.9016 -- iter: 18300/28000
Training Step: 367  | total loss: [1m[32m0.27276[0m[0m | time: 880.495s
| Adam | epoch: 001 | loss: 0.27276 - acc: 0.8954 -- iter: 18350/28000
Training Step: 368  | total loss: [1m[32m0.26247[0m[0m | time: 881.877s
| Adam | epoch: 001 | loss: 0.26247 - acc: 0.9019 -- iter: 18400/28000
Training Step: 369  | total loss: [1m[32m0.26695[0m[0m | time: 883.339s
| Adam | epoch: 001 | loss: 0.26695 - acc: 0.8957 -- iter: 18450/28000
Training Step: 370  | total loss: [1m[32m0.26252[0m[0m | time: 884.529s
| Adam | epoch: 001 | loss: 0.26252 - acc: 0.9021 -- iter: 18500/28000
Training Step: 371  | total loss: [1m[32m0.26649[0m[0m | time: 885.565s
| Adam | epoch: 001 | loss: 0.26649 - acc: 0.9019 -- iter: 18550/28000
Training Step: 372  | total loss: [1m[32m0.25534[0m[0m | time: 886.775s
| Adam | epoch: 001 | loss: 0.25534 - acc: 0.9057 -- iter: 18600/28000
Training Step: 373  | total loss: [1m[32m0.24666[0m[0m | time: 888.077s
| Adam | epoch: 001 | loss: 0.24666 - acc: 0.9072 -- iter: 18650/28000
Training Step: 374  | total loss: [1m[32m0.24062[0m[0m | time: 889.458s
| Adam | epoch: 001 | loss: 0.24062 - acc: 0.9084 -- iter: 18700/28000
Training Step: 375  | total loss: [1m[32m0.24359[0m[0m | time: 919.709s
| Adam | epoch: 001 | loss: 0.24359 - acc: 0.9076 | val_loss: 0.24047 - val_acc: 0.9105 -- iter: 18750/28000
--
Training Step: 376  | total loss: [1m[32m0.23458[0m[0m | time: 920.830s
| Adam | epoch: 001 | loss: 0.23458 - acc: 0.9088 -- iter: 18800/28000
Training Step: 377  | total loss: [1m[32m0.23226[0m[0m | time: 922.048s
| Adam | epoch: 001 | loss: 0.23226 - acc: 0.9080 -- iter: 18850/28000
Training Step: 378  | total loss: [1m[32m0.23156[0m[0m | time: 923.266s
| Adam | epoch: 001 | loss: 0.23156 - acc: 0.9132 -- iter: 18900/28000
Training Step: 379  | total loss: [1m[32m0.24213[0m[0m | time: 924.478s
| Adam | epoch: 001 | loss: 0.24213 - acc: 0.9078 -- iter: 18950/28000
Training Step: 380  | total loss: [1m[32m0.23870[0m[0m | time: 925.636s
| Adam | epoch: 001 | loss: 0.23870 - acc: 0.9071 -- iter: 19000/28000
Training Step: 381  | total loss: [1m[32m0.22410[0m[0m | time: 926.837s
| Adam | epoch: 001 | loss: 0.22410 - acc: 0.9144 -- iter: 19050/28000
Training Step: 382  | total loss: [1m[32m0.22413[0m[0m | time: 927.988s
| Adam | epoch: 001 | loss: 0.22413 - acc: 0.9129 -- iter: 19100/28000
Training Step: 383  | total loss: [1m[32m0.22596[0m[0m | time: 929.142s
| Adam | epoch: 001 | loss: 0.22596 - acc: 0.9096 -- iter: 19150/28000
Training Step: 384  | total loss: [1m[32m0.23315[0m[0m | time: 930.294s
| Adam | epoch: 001 | loss: 0.23315 - acc: 0.9087 -- iter: 19200/28000
Training Step: 385  | total loss: [1m[32m0.24093[0m[0m | time: 931.427s
| Adam | epoch: 001 | loss: 0.24093 - acc: 0.9058 -- iter: 19250/28000
Training Step: 386  | total loss: [1m[32m0.24222[0m[0m | time: 932.627s
| Adam | epoch: 001 | loss: 0.24222 - acc: 0.9052 -- iter: 19300/28000
Training Step: 387  | total loss: [1m[32m0.23166[0m[0m | time: 933.803s
| Adam | epoch: 001 | loss: 0.23166 - acc: 0.9127 -- iter: 19350/28000
Training Step: 388  | total loss: [1m[32m0.23118[0m[0m | time: 934.934s
| Adam | epoch: 001 | loss: 0.23118 - acc: 0.9114 -- iter: 19400/28000
Training Step: 389  | total loss: [1m[32m0.22994[0m[0m | time: 936.064s
| Adam | epoch: 001 | loss: 0.22994 - acc: 0.9103 -- iter: 19450/28000
Training Step: 390  | total loss: [1m[32m0.22944[0m[0m | time: 937.259s
| Adam | epoch: 001 | loss: 0.22944 - acc: 0.9093 -- iter: 19500/28000
Training Step: 391  | total loss: [1m[32m0.24549[0m[0m | time: 938.414s
| Adam | epoch: 001 | loss: 0.24549 - acc: 0.9003 -- iter: 19550/28000
Training Step: 392  | total loss: [1m[32m0.24335[0m[0m | time: 939.543s
| Adam | epoch: 001 | loss: 0.24335 - acc: 0.8983 -- iter: 19600/28000
Training Step: 393  | total loss: [1m[32m0.24269[0m[0m | time: 940.719s
| Adam | epoch: 001 | loss: 0.24269 - acc: 0.8965 -- iter: 19650/28000
Training Step: 394  | total loss: [1m[32m0.24123[0m[0m | time: 941.711s
| Adam | epoch: 001 | loss: 0.24123 - acc: 0.8968 -- iter: 19700/28000
Training Step: 395  | total loss: [1m[32m0.25164[0m[0m | time: 942.703s
| Adam | epoch: 001 | loss: 0.25164 - acc: 0.8971 -- iter: 19750/28000
Training Step: 396  | total loss: [1m[32m0.24887[0m[0m | time: 943.737s
| Adam | epoch: 001 | loss: 0.24887 - acc: 0.8954 -- iter: 19800/28000
Training Step: 397  | total loss: [1m[32m0.25428[0m[0m | time: 944.816s
| Adam | epoch: 001 | loss: 0.25428 - acc: 0.8959 -- iter: 19850/28000
Training Step: 398  | total loss: [1m[32m0.25941[0m[0m | time: 946.078s
| Adam | epoch: 001 | loss: 0.25941 - acc: 0.9023 -- iter: 19900/28000
Training Step: 399  | total loss: [1m[32m0.24448[0m[0m | time: 947.234s
| Adam | epoch: 001 | loss: 0.24448 - acc: 0.9081 -- iter: 19950/28000
Training Step: 400  | total loss: [1m[32m0.23561[0m[0m | time: 975.839s
| Adam | epoch: 001 | loss: 0.23561 - acc: 0.9113 | val_loss: 0.24346 - val_acc: 0.9087 -- iter: 20000/28000
--
Training Step: 401  | total loss: [1m[32m0.23077[0m[0m | time: 977.379s
| Adam | epoch: 001 | loss: 0.23077 - acc: 0.9141 -- iter: 20050/28000
Training Step: 402  | total loss: [1m[32m0.23171[0m[0m | time: 978.919s
| Adam | epoch: 001 | loss: 0.23171 - acc: 0.9107 -- iter: 20100/28000
Training Step: 403  | total loss: [1m[32m0.22046[0m[0m | time: 980.144s
| Adam | epoch: 001 | loss: 0.22046 - acc: 0.9156 -- iter: 20150/28000
Training Step: 404  | total loss: [1m[32m0.22554[0m[0m | time: 981.361s
| Adam | epoch: 001 | loss: 0.22554 - acc: 0.9081 -- iter: 20200/28000
Training Step: 405  | total loss: [1m[32m0.22278[0m[0m | time: 982.623s
| Adam | epoch: 001 | loss: 0.22278 - acc: 0.9073 -- iter: 20250/28000
Training Step: 406  | total loss: [1m[32m0.22404[0m[0m | time: 983.838s
| Adam | epoch: 001 | loss: 0.22404 - acc: 0.9045 -- iter: 20300/28000
Training Step: 407  | total loss: [1m[32m0.21570[0m[0m | time: 984.813s
| Adam | epoch: 001 | loss: 0.21570 - acc: 0.9081 -- iter: 20350/28000
Training Step: 408  | total loss: [1m[32m0.21006[0m[0m | time: 985.942s
| Adam | epoch: 001 | loss: 0.21006 - acc: 0.9093 -- iter: 20400/28000
Training Step: 409  | total loss: [1m[32m0.22176[0m[0m | time: 987.149s
| Adam | epoch: 001 | loss: 0.22176 - acc: 0.9064 -- iter: 20450/28000
Training Step: 410  | total loss: [1m[32m0.22380[0m[0m | time: 988.348s
| Adam | epoch: 001 | loss: 0.22380 - acc: 0.9017 -- iter: 20500/28000
Training Step: 411  | total loss: [1m[32m0.23445[0m[0m | time: 989.519s
| Adam | epoch: 001 | loss: 0.23445 - acc: 0.8915 -- iter: 20550/28000
Training Step: 412  | total loss: [1m[32m0.22804[0m[0m | time: 990.655s
| Adam | epoch: 001 | loss: 0.22804 - acc: 0.8944 -- iter: 20600/28000
Training Step: 413  | total loss: [1m[32m0.22874[0m[0m | time: 991.877s
| Adam | epoch: 001 | loss: 0.22874 - acc: 0.8890 -- iter: 20650/28000
Training Step: 414  | total loss: [1m[32m0.22155[0m[0m | time: 993.100s
| Adam | epoch: 001 | loss: 0.22155 - acc: 0.8941 -- iter: 20700/28000
Training Step: 415  | total loss: [1m[32m0.21735[0m[0m | time: 994.077s
| Adam | epoch: 001 | loss: 0.21735 - acc: 0.8967 -- iter: 20750/28000
Training Step: 416  | total loss: [1m[32m0.21224[0m[0m | time: 995.186s
| Adam | epoch: 001 | loss: 0.21224 - acc: 0.9010 -- iter: 20800/28000
Training Step: 417  | total loss: [1m[32m0.21473[0m[0m | time: 996.337s
| Adam | epoch: 001 | loss: 0.21473 - acc: 0.9029 -- iter: 20850/28000
Training Step: 418  | total loss: [1m[32m0.21064[0m[0m | time: 997.481s
| Adam | epoch: 001 | loss: 0.21064 - acc: 0.9046 -- iter: 20900/28000
Training Step: 419  | total loss: [1m[32m0.21882[0m[0m | time: 998.630s
| Adam | epoch: 001 | loss: 0.21882 - acc: 0.9021 -- iter: 20950/28000
Training Step: 420  | total loss: [1m[32m0.23948[0m[0m | time: 999.792s
| Adam | epoch: 001 | loss: 0.23948 - acc: 0.8919 -- iter: 21000/28000
Training Step: 421  | total loss: [1m[32m0.23347[0m[0m | time: 1000.978s
| Adam | epoch: 001 | loss: 0.23347 - acc: 0.8947 -- iter: 21050/28000
Training Step: 422  | total loss: [1m[32m0.22333[0m[0m | time: 1001.939s
| Adam | epoch: 001 | loss: 0.22333 - acc: 0.9013 -- iter: 21100/28000
Training Step: 423  | total loss: [1m[32m0.22200[0m[0m | time: 1003.425s
| Adam | epoch: 001 | loss: 0.22200 - acc: 0.8971 -- iter: 21150/28000
Training Step: 424  | total loss: [1m[32m0.21876[0m[0m | time: 1004.855s
| Adam | epoch: 001 | loss: 0.21876 - acc: 0.9034 -- iter: 21200/28000
Training Step: 425  | total loss: [1m[32m0.21123[0m[0m | time: 1034.487s
| Adam | epoch: 001 | loss: 0.21123 - acc: 0.9091 | val_loss: 0.24288 - val_acc: 0.9080 -- iter: 21250/28000
--
Training Step: 426  | total loss: [1m[32m0.21476[0m[0m | time: 1035.683s
| Adam | epoch: 001 | loss: 0.21476 - acc: 0.9062 -- iter: 21300/28000
Training Step: 427  | total loss: [1m[32m0.23798[0m[0m | time: 1036.812s
| Adam | epoch: 001 | loss: 0.23798 - acc: 0.8996 -- iter: 21350/28000
Training Step: 428  | total loss: [1m[32m0.25086[0m[0m | time: 1037.979s
| Adam | epoch: 001 | loss: 0.25086 - acc: 0.8976 -- iter: 21400/28000
Training Step: 429  | total loss: [1m[32m0.25554[0m[0m | time: 1039.165s
| Adam | epoch: 001 | loss: 0.25554 - acc: 0.8978 -- iter: 21450/28000
Training Step: 430  | total loss: [1m[32m0.25720[0m[0m | time: 1040.312s
| Adam | epoch: 001 | loss: 0.25720 - acc: 0.8961 -- iter: 21500/28000
Training Step: 431  | total loss: [1m[32m0.25114[0m[0m | time: 1041.455s
| Adam | epoch: 001 | loss: 0.25114 - acc: 0.8944 -- iter: 21550/28000
Training Step: 432  | total loss: [1m[32m0.25899[0m[0m | time: 1042.670s
| Adam | epoch: 001 | loss: 0.25899 - acc: 0.8910 -- iter: 21600/28000
Training Step: 433  | total loss: [1m[32m0.25667[0m[0m | time: 1043.666s
| Adam | epoch: 001 | loss: 0.25667 - acc: 0.8919 -- iter: 21650/28000
Training Step: 434  | total loss: [1m[32m0.25376[0m[0m | time: 1044.826s
| Adam | epoch: 001 | loss: 0.25376 - acc: 0.8967 -- iter: 21700/28000
Training Step: 435  | total loss: [1m[32m0.25843[0m[0m | time: 1045.930s
| Adam | epoch: 001 | loss: 0.25843 - acc: 0.8950 -- iter: 21750/28000
Training Step: 436  | total loss: [1m[32m0.25664[0m[0m | time: 1047.049s
| Adam | epoch: 001 | loss: 0.25664 - acc: 0.8975 -- iter: 21800/28000
Training Step: 437  | total loss: [1m[32m0.25853[0m[0m | time: 1048.207s
| Adam | epoch: 001 | loss: 0.25853 - acc: 0.8998 -- iter: 21850/28000
Training Step: 438  | total loss: [1m[32m0.26065[0m[0m | time: 1049.404s
| Adam | epoch: 001 | loss: 0.26065 - acc: 0.8998 -- iter: 21900/28000
Training Step: 439  | total loss: [1m[32m0.25716[0m[0m | time: 1050.607s
| Adam | epoch: 001 | loss: 0.25716 - acc: 0.9038 -- iter: 21950/28000
Training Step: 440  | total loss: [1m[32m0.24796[0m[0m | time: 1051.835s
| Adam | epoch: 001 | loss: 0.24796 - acc: 0.9074 -- iter: 22000/28000
Training Step: 441  | total loss: [1m[32m0.23944[0m[0m | time: 1053.085s
| Adam | epoch: 001 | loss: 0.23944 - acc: 0.9087 -- iter: 22050/28000
Training Step: 442  | total loss: [1m[32m0.22738[0m[0m | time: 1054.338s
| Adam | epoch: 001 | loss: 0.22738 - acc: 0.9138 -- iter: 22100/28000
Training Step: 443  | total loss: [1m[32m0.23067[0m[0m | time: 1055.487s
| Adam | epoch: 001 | loss: 0.23067 - acc: 0.9104 -- iter: 22150/28000
Training Step: 444  | total loss: [1m[32m0.22726[0m[0m | time: 1056.664s
| Adam | epoch: 001 | loss: 0.22726 - acc: 0.9134 -- iter: 22200/28000
Training Step: 445  | total loss: [1m[32m0.22504[0m[0m | time: 1057.634s
| Adam | epoch: 001 | loss: 0.22504 - acc: 0.9161 -- iter: 22250/28000
Training Step: 446  | total loss: [1m[32m0.22633[0m[0m | time: 1058.750s
| Adam | epoch: 001 | loss: 0.22633 - acc: 0.9145 -- iter: 22300/28000
Training Step: 447  | total loss: [1m[32m0.23623[0m[0m | time: 1059.901s
| Adam | epoch: 001 | loss: 0.23623 - acc: 0.9110 -- iter: 22350/28000
Training Step: 448  | total loss: [1m[32m0.22909[0m[0m | time: 1061.071s
| Adam | epoch: 001 | loss: 0.22909 - acc: 0.9179 -- iter: 22400/28000
Training Step: 449  | total loss: [1m[32m0.22528[0m[0m | time: 1062.319s
| Adam | epoch: 001 | loss: 0.22528 - acc: 0.9201 -- iter: 22450/28000
Training Step: 450  | total loss: [1m[32m0.22646[0m[0m | time: 1092.364s
| Adam | epoch: 001 | loss: 0.22646 - acc: 0.9201 | val_loss: 0.23527 - val_acc: 0.9124 -- iter: 22500/28000
--
Training Step: 451  | total loss: [1m[32m0.22362[0m[0m | time: 1093.584s
| Adam | epoch: 001 | loss: 0.22362 - acc: 0.9221 -- iter: 22550/28000
Training Step: 452  | total loss: [1m[32m0.21796[0m[0m | time: 1094.759s
| Adam | epoch: 001 | loss: 0.21796 - acc: 0.9239 -- iter: 22600/28000
Training Step: 453  | total loss: [1m[32m0.22808[0m[0m | time: 1095.910s
| Adam | epoch: 001 | loss: 0.22808 - acc: 0.9235 -- iter: 22650/28000
Training Step: 454  | total loss: [1m[32m0.23947[0m[0m | time: 1097.098s
| Adam | epoch: 001 | loss: 0.23947 - acc: 0.9191 -- iter: 22700/28000
Training Step: 455  | total loss: [1m[32m0.24232[0m[0m | time: 1098.255s
| Adam | epoch: 001 | loss: 0.24232 - acc: 0.9152 -- iter: 22750/28000
Training Step: 456  | total loss: [1m[32m0.24271[0m[0m | time: 1099.460s
| Adam | epoch: 001 | loss: 0.24271 - acc: 0.9137 -- iter: 22800/28000
Training Step: 457  | total loss: [1m[32m0.23562[0m[0m | time: 1100.816s
| Adam | epoch: 001 | loss: 0.23562 - acc: 0.9143 -- iter: 22850/28000
Training Step: 458  | total loss: [1m[32m0.23689[0m[0m | time: 1102.204s
| Adam | epoch: 001 | loss: 0.23689 - acc: 0.9109 -- iter: 22900/28000
Training Step: 459  | total loss: [1m[32m0.22586[0m[0m | time: 1103.394s
| Adam | epoch: 001 | loss: 0.22586 - acc: 0.9158 -- iter: 22950/28000
Training Step: 460  | total loss: [1m[32m0.24196[0m[0m | time: 1104.537s
| Adam | epoch: 001 | loss: 0.24196 - acc: 0.9122 -- iter: 23000/28000
Training Step: 461  | total loss: [1m[32m0.23294[0m[0m | time: 1105.724s
| Adam | epoch: 001 | loss: 0.23294 - acc: 0.9170 -- iter: 23050/28000
Training Step: 462  | total loss: [1m[32m0.22802[0m[0m | time: 1106.908s
| Adam | epoch: 001 | loss: 0.22802 - acc: 0.9193 -- iter: 23100/28000
Training Step: 463  | total loss: [1m[32m0.21925[0m[0m | time: 1108.174s
| Adam | epoch: 001 | loss: 0.21925 - acc: 0.9214 -- iter: 23150/28000
Training Step: 464  | total loss: [1m[32m0.21837[0m[0m | time: 1109.379s
| Adam | epoch: 001 | loss: 0.21837 - acc: 0.9232 -- iter: 23200/28000
Training Step: 465  | total loss: [1m[32m0.20761[0m[0m | time: 1110.819s
| Adam | epoch: 001 | loss: 0.20761 - acc: 0.9269 -- iter: 23250/28000
Training Step: 466  | total loss: [1m[32m0.21915[0m[0m | time: 1112.720s
| Adam | epoch: 001 | loss: 0.21915 - acc: 0.9242 -- iter: 23300/28000
Training Step: 467  | total loss: [1m[32m0.21566[0m[0m | time: 1113.948s
| Adam | epoch: 001 | loss: 0.21566 - acc: 0.9258 -- iter: 23350/28000
Training Step: 468  | total loss: [1m[32m0.20282[0m[0m | time: 1115.084s
| Adam | epoch: 001 | loss: 0.20282 - acc: 0.9332 -- iter: 23400/28000
Training Step: 469  | total loss: [1m[32m0.20588[0m[0m | time: 1116.139s
| Adam | epoch: 001 | loss: 0.20588 - acc: 0.9279 -- iter: 23450/28000
Training Step: 470  | total loss: [1m[32m0.22103[0m[0m | time: 1117.333s
| Adam | epoch: 001 | loss: 0.22103 - acc: 0.9251 -- iter: 23500/28000
Training Step: 471  | total loss: [1m[32m0.21704[0m[0m | time: 1118.720s
| Adam | epoch: 001 | loss: 0.21704 - acc: 0.9226 -- iter: 23550/28000
Training Step: 472  | total loss: [1m[32m0.20518[0m[0m | time: 1119.996s
| Adam | epoch: 001 | loss: 0.20518 - acc: 0.9283 -- iter: 23600/28000
Training Step: 473  | total loss: [1m[32m0.20113[0m[0m | time: 1121.001s
| Adam | epoch: 001 | loss: 0.20113 - acc: 0.9335 -- iter: 23650/28000
Training Step: 474  | total loss: [1m[32m0.19836[0m[0m | time: 1122.190s
| Adam | epoch: 001 | loss: 0.19836 - acc: 0.9302 -- iter: 23700/28000
Training Step: 475  | total loss: [1m[32m0.20613[0m[0m | time: 1151.325s
| Adam | epoch: 001 | loss: 0.20613 - acc: 0.9271 | val_loss: 0.24372 - val_acc: 0.9087 -- iter: 23750/28000
--
Training Step: 476  | total loss: [1m[32m0.21919[0m[0m | time: 1152.501s
| Adam | epoch: 001 | loss: 0.21919 - acc: 0.9224 -- iter: 23800/28000
Training Step: 477  | total loss: [1m[32m0.20637[0m[0m | time: 1153.463s
| Adam | epoch: 001 | loss: 0.20637 - acc: 0.9262 -- iter: 23850/28000
Training Step: 478  | total loss: [1m[32m0.21593[0m[0m | time: 1154.604s
| Adam | epoch: 001 | loss: 0.21593 - acc: 0.9296 -- iter: 23900/28000
Training Step: 479  | total loss: [1m[32m0.20963[0m[0m | time: 1155.705s
| Adam | epoch: 001 | loss: 0.20963 - acc: 0.9306 -- iter: 23950/28000
Training Step: 480  | total loss: [1m[32m0.21468[0m[0m | time: 1156.862s
| Adam | epoch: 001 | loss: 0.21468 - acc: 0.9235 -- iter: 24000/28000
Training Step: 481  | total loss: [1m[32m0.21771[0m[0m | time: 1157.990s
| Adam | epoch: 001 | loss: 0.21771 - acc: 0.9192 -- iter: 24050/28000
Training Step: 482  | total loss: [1m[32m0.22232[0m[0m | time: 1159.140s
| Adam | epoch: 001 | loss: 0.22232 - acc: 0.9193 -- iter: 24100/28000
Training Step: 483  | total loss: [1m[32m0.21754[0m[0m | time: 1160.335s
| Adam | epoch: 001 | loss: 0.21754 - acc: 0.9193 -- iter: 24150/28000
Training Step: 484  | total loss: [1m[32m0.23403[0m[0m | time: 1161.476s
| Adam | epoch: 001 | loss: 0.23403 - acc: 0.9054 -- iter: 24200/28000
Training Step: 485  | total loss: [1m[32m0.24428[0m[0m | time: 1162.811s
| Adam | epoch: 001 | loss: 0.24428 - acc: 0.9069 -- iter: 24250/28000
Training Step: 486  | total loss: [1m[32m0.24727[0m[0m | time: 1164.269s
| Adam | epoch: 001 | loss: 0.24727 - acc: 0.9082 -- iter: 24300/28000
Training Step: 487  | total loss: [1m[32m0.25275[0m[0m | time: 1165.781s
| Adam | epoch: 001 | loss: 0.25275 - acc: 0.9054 -- iter: 24350/28000
Training Step: 488  | total loss: [1m[32m0.26473[0m[0m | time: 1167.216s
| Adam | epoch: 001 | loss: 0.26473 - acc: 0.8988 -- iter: 24400/28000
Training Step: 489  | total loss: [1m[32m0.25621[0m[0m | time: 1168.806s
| Adam | epoch: 001 | loss: 0.25621 - acc: 0.9009 -- iter: 24450/28000
Training Step: 490  | total loss: [1m[32m0.25233[0m[0m | time: 1169.978s
| Adam | epoch: 001 | loss: 0.25233 - acc: 0.9049 -- iter: 24500/28000
Training Step: 491  | total loss: [1m[32m0.24885[0m[0m | time: 1171.022s
| Adam | epoch: 001 | loss: 0.24885 - acc: 0.9044 -- iter: 24550/28000
Training Step: 492  | total loss: [1m[32m0.24463[0m[0m | time: 1172.157s
| Adam | epoch: 001 | loss: 0.24463 - acc: 0.9059 -- iter: 24600/28000
Training Step: 493  | total loss: [1m[32m0.24563[0m[0m | time: 1173.346s
| Adam | epoch: 001 | loss: 0.24563 - acc: 0.9053 -- iter: 24650/28000
Training Step: 494  | total loss: [1m[32m0.23990[0m[0m | time: 1174.536s
| Adam | epoch: 001 | loss: 0.23990 - acc: 0.9068 -- iter: 24700/28000
Training Step: 495  | total loss: [1m[32m0.23546[0m[0m | time: 1175.823s
| Adam | epoch: 001 | loss: 0.23546 - acc: 0.9081 -- iter: 24750/28000
Training Step: 496  | total loss: [1m[32m0.23498[0m[0m | time: 1177.079s
| Adam | epoch: 001 | loss: 0.23498 - acc: 0.9053 -- iter: 24800/28000
Training Step: 497  | total loss: [1m[32m0.23245[0m[0m | time: 1178.500s
| Adam | epoch: 001 | loss: 0.23245 - acc: 0.9068 -- iter: 24850/28000
Training Step: 498  | total loss: [1m[32m0.23173[0m[0m | time: 1179.669s
| Adam | epoch: 001 | loss: 0.23173 - acc: 0.9101 -- iter: 24900/28000
Training Step: 499  | total loss: [1m[32m0.24974[0m[0m | time: 1180.814s
| Adam | epoch: 001 | loss: 0.24974 - acc: 0.9071 -- iter: 24950/28000
Training Step: 500  | total loss: [1m[32m0.25648[0m[0m | time: 1209.587s
| Adam | epoch: 001 | loss: 0.25648 - acc: 0.8984 | val_loss: 0.27137 - val_acc: 0.8957 -- iter: 25000/28000
--
Training Step: 501  | total loss: [1m[32m0.25697[0m[0m | time: 1210.810s
| Adam | epoch: 001 | loss: 0.25697 - acc: 0.9005 -- iter: 25050/28000
Training Step: 502  | total loss: [1m[32m0.25120[0m[0m | time: 1212.046s
| Adam | epoch: 001 | loss: 0.25120 - acc: 0.9045 -- iter: 25100/28000
Training Step: 503  | total loss: [1m[32m0.23931[0m[0m | time: 1213.409s
| Adam | epoch: 001 | loss: 0.23931 - acc: 0.9100 -- iter: 25150/28000
Training Step: 504  | total loss: [1m[32m0.23368[0m[0m | time: 1214.834s
| Adam | epoch: 001 | loss: 0.23368 - acc: 0.9150 -- iter: 25200/28000
Training Step: 505  | total loss: [1m[32m0.22495[0m[0m | time: 1216.045s
| Adam | epoch: 001 | loss: 0.22495 - acc: 0.9195 -- iter: 25250/28000
Training Step: 506  | total loss: [1m[32m0.24086[0m[0m | time: 1217.265s
| Adam | epoch: 001 | loss: 0.24086 - acc: 0.9136 -- iter: 25300/28000
Training Step: 507  | total loss: [1m[32m0.24739[0m[0m | time: 1218.411s
| Adam | epoch: 001 | loss: 0.24739 - acc: 0.9122 -- iter: 25350/28000
Training Step: 508  | total loss: [1m[32m0.24645[0m[0m | time: 1219.635s
| Adam | epoch: 001 | loss: 0.24645 - acc: 0.9110 -- iter: 25400/28000
Training Step: 509  | total loss: [1m[32m0.24149[0m[0m | time: 1220.742s
| Adam | epoch: 001 | loss: 0.24149 - acc: 0.9139 -- iter: 25450/28000
Training Step: 510  | total loss: [1m[32m0.23572[0m[0m | time: 1221.935s
| Adam | epoch: 001 | loss: 0.23572 - acc: 0.9145 -- iter: 25500/28000
Training Step: 511  | total loss: [1m[32m0.22591[0m[0m | time: 1222.926s
| Adam | epoch: 001 | loss: 0.22591 - acc: 0.9151 -- iter: 25550/28000
Training Step: 512  | total loss: [1m[32m0.23557[0m[0m | time: 1224.057s
| Adam | epoch: 001 | loss: 0.23557 - acc: 0.9116 -- iter: 25600/28000
Training Step: 513  | total loss: [1m[32m0.23517[0m[0m | time: 1225.219s
| Adam | epoch: 001 | loss: 0.23517 - acc: 0.9104 -- iter: 25650/28000
Training Step: 514  | total loss: [1m[32m0.22495[0m[0m | time: 1226.364s
| Adam | epoch: 001 | loss: 0.22495 - acc: 0.9134 -- iter: 25700/28000
Training Step: 515  | total loss: [1m[32m0.22239[0m[0m | time: 1227.512s
| Adam | epoch: 001 | loss: 0.22239 - acc: 0.9160 -- iter: 25750/28000
Training Step: 516  | total loss: [1m[32m0.21477[0m[0m | time: 1228.698s
| Adam | epoch: 001 | loss: 0.21477 - acc: 0.9224 -- iter: 25800/28000
Training Step: 517  | total loss: [1m[32m0.21158[0m[0m | time: 1229.867s
| Adam | epoch: 001 | loss: 0.21158 - acc: 0.9222 -- iter: 25850/28000
Training Step: 518  | total loss: [1m[32m0.21781[0m[0m | time: 1231.034s
| Adam | epoch: 001 | loss: 0.21781 - acc: 0.9220 -- iter: 25900/28000
Training Step: 519  | total loss: [1m[32m0.21486[0m[0m | time: 1232.267s
| Adam | epoch: 001 | loss: 0.21486 - acc: 0.9258 -- iter: 25950/28000
Training Step: 520  | total loss: [1m[32m0.22753[0m[0m | time: 1233.560s
| Adam | epoch: 001 | loss: 0.22753 - acc: 0.9192 -- iter: 26000/28000
Training Step: 521  | total loss: [1m[32m0.21745[0m[0m | time: 1234.754s
| Adam | epoch: 001 | loss: 0.21745 - acc: 0.9213 -- iter: 26050/28000
Training Step: 522  | total loss: [1m[32m0.20806[0m[0m | time: 1235.989s
| Adam | epoch: 001 | loss: 0.20806 - acc: 0.9251 -- iter: 26100/28000
Training Step: 523  | total loss: [1m[32m0.20233[0m[0m | time: 1236.952s
| Adam | epoch: 001 | loss: 0.20233 - acc: 0.9266 -- iter: 26150/28000
Training Step: 524  | total loss: [1m[32m0.19331[0m[0m | time: 1238.049s
| Adam | epoch: 001 | loss: 0.19331 - acc: 0.9280 -- iter: 26200/28000
Training Step: 525  | total loss: [1m[32m0.18083[0m[0m | time: 1269.016s
| Adam | epoch: 001 | loss: 0.18083 - acc: 0.9332 | val_loss: 0.23408 - val_acc: 0.9120 -- iter: 26250/28000
--
Training Step: 526  | total loss: [1m[32m0.18086[0m[0m | time: 1270.247s
| Adam | epoch: 001 | loss: 0.18086 - acc: 0.9339 -- iter: 26300/28000
Training Step: 527  | total loss: [1m[32m0.18508[0m[0m | time: 1271.388s
| Adam | epoch: 001 | loss: 0.18508 - acc: 0.9325 -- iter: 26350/28000
Training Step: 528  | total loss: [1m[32m0.20185[0m[0m | time: 1272.527s
| Adam | epoch: 001 | loss: 0.20185 - acc: 0.9212 -- iter: 26400/28000
Training Step: 529  | total loss: [1m[32m0.21139[0m[0m | time: 1273.807s
| Adam | epoch: 001 | loss: 0.21139 - acc: 0.9191 -- iter: 26450/28000
Training Step: 530  | total loss: [1m[32m0.20748[0m[0m | time: 1275.129s
| Adam | epoch: 001 | loss: 0.20748 - acc: 0.9212 -- iter: 26500/28000
Training Step: 531  | total loss: [1m[32m0.20282[0m[0m | time: 1276.426s
| Adam | epoch: 001 | loss: 0.20282 - acc: 0.9211 -- iter: 26550/28000
Training Step: 532  | total loss: [1m[32m0.20228[0m[0m | time: 1277.588s
| Adam | epoch: 001 | loss: 0.20228 - acc: 0.9250 -- iter: 26600/28000
Training Step: 533  | total loss: [1m[32m0.20120[0m[0m | time: 1278.993s
| Adam | epoch: 001 | loss: 0.20120 - acc: 0.9245 -- iter: 26650/28000
Training Step: 534  | total loss: [1m[32m0.19202[0m[0m | time: 1280.259s
| Adam | epoch: 001 | loss: 0.19202 - acc: 0.9280 -- iter: 26700/28000
Training Step: 535  | total loss: [1m[32m0.18207[0m[0m | time: 1281.559s
| Adam | epoch: 001 | loss: 0.18207 - acc: 0.9312 -- iter: 26750/28000
Training Step: 536  | total loss: [1m[32m0.18925[0m[0m | time: 1282.711s
| Adam | epoch: 001 | loss: 0.18925 - acc: 0.9301 -- iter: 26800/28000
Training Step: 537  | total loss: [1m[32m0.19486[0m[0m | time: 1283.858s
| Adam | epoch: 001 | loss: 0.19486 - acc: 0.9311 -- iter: 26850/28000
Training Step: 538  | total loss: [1m[32m0.20848[0m[0m | time: 1285.013s
| Adam | epoch: 001 | loss: 0.20848 - acc: 0.9280 -- iter: 26900/28000
Training Step: 539  | total loss: [1m[32m0.20456[0m[0m | time: 1286.193s
| Adam | epoch: 001 | loss: 0.20456 - acc: 0.9292 -- iter: 26950/28000
Training Step: 540  | total loss: [1m[32m0.21457[0m[0m | time: 1287.474s
| Adam | epoch: 001 | loss: 0.21457 - acc: 0.9223 -- iter: 27000/28000
Training Step: 541  | total loss: [1m[32m0.21127[0m[0m | time: 1288.729s
| Adam | epoch: 001 | loss: 0.21127 - acc: 0.9260 -- iter: 27050/28000
Training Step: 542  | total loss: [1m[32m0.23009[0m[0m | time: 1289.960s
| Adam | epoch: 001 | loss: 0.23009 - acc: 0.9194 -- iter: 27100/28000
Training Step: 543  | total loss: [1m[32m0.21916[0m[0m | time: 1291.149s
| Adam | epoch: 001 | loss: 0.21916 - acc: 0.9235 -- iter: 27150/28000
Training Step: 544  | total loss: [1m[32m0.22715[0m[0m | time: 1292.647s
| Adam | epoch: 001 | loss: 0.22715 - acc: 0.9191 -- iter: 27200/28000
Training Step: 545  | total loss: [1m[32m0.22081[0m[0m | time: 1293.710s
| Adam | epoch: 001 | loss: 0.22081 - acc: 0.9212 -- iter: 27250/28000
Training Step: 546  | total loss: [1m[32m0.23664[0m[0m | time: 1294.816s
| Adam | epoch: 001 | loss: 0.23664 - acc: 0.9151 -- iter: 27300/28000
Training Step: 547  | total loss: [1m[32m0.23607[0m[0m | time: 1296.048s
| Adam | epoch: 001 | loss: 0.23607 - acc: 0.9196 -- iter: 27350/28000
Training Step: 548  | total loss: [1m[32m0.23191[0m[0m | time: 1297.234s
| Adam | epoch: 001 | loss: 0.23191 - acc: 0.9236 -- iter: 27400/28000
Training Step: 549  | total loss: [1m[32m0.25138[0m[0m | time: 1298.477s
| Adam | epoch: 001 | loss: 0.25138 - acc: 0.9153 -- iter: 27450/28000
Training Step: 550  | total loss: [1m[32m0.23606[0m[0m | time: 1328.358s
| Adam | epoch: 001 | loss: 0.23606 - acc: 0.9197 | val_loss: 0.27974 - val_acc: 0.8919 -- iter: 27500/28000
--
Training Step: 551  | total loss: [1m[32m0.22803[0m[0m | time: 1329.740s
| Adam | epoch: 001 | loss: 0.22803 - acc: 0.9218 -- iter: 27550/28000
Training Step: 552  | total loss: [1m[32m0.21660[0m[0m | time: 1331.120s
| Adam | epoch: 001 | loss: 0.21660 - acc: 0.9276 -- iter: 27600/28000
Training Step: 553  | total loss: [1m[32m0.25022[0m[0m | time: 1332.466s
| Adam | epoch: 001 | loss: 0.25022 - acc: 0.9148 -- iter: 27650/28000
Training Step: 554  | total loss: [1m[32m0.26632[0m[0m | time: 1333.993s
| Adam | epoch: 001 | loss: 0.26632 - acc: 0.9093 -- iter: 27700/28000
Training Step: 555  | total loss: [1m[32m0.25625[0m[0m | time: 1335.212s
| Adam | epoch: 001 | loss: 0.25625 - acc: 0.9124 -- iter: 27750/28000
Training Step: 556  | total loss: [1m[32m0.24852[0m[0m | time: 1336.482s
| Adam | epoch: 001 | loss: 0.24852 - acc: 0.9132 -- iter: 27800/28000
Training Step: 557  | total loss: [1m[32m0.25043[0m[0m | time: 1337.789s
| Adam | epoch: 001 | loss: 0.25043 - acc: 0.9119 -- iter: 27850/28000
Training Step: 558  | total loss: [1m[32m0.24983[0m[0m | time: 1339.059s
| Adam | epoch: 001 | loss: 0.24983 - acc: 0.9087 -- iter: 27900/28000
Training Step: 559  | total loss: [1m[32m0.25523[0m[0m | time: 1340.323s
| Adam | epoch: 001 | loss: 0.25523 - acc: 0.9058 -- iter: 27950/28000
Training Step: 560  | total loss: [1m[32m0.25065[0m[0m | time: 1341.537s
| Adam | epoch: 001 | loss: 0.25065 - acc: 0.9092 -- iter: 28000/28000
Training Step: 561  | total loss: [1m[32m0.24514[0m[0m | time: 1.309s
| Adam | epoch: 002 | loss: 0.24514 - acc: 0.9083 -- iter: 00050/28000
Training Step: 562  | total loss: [1m[32m0.24038[0m[0m | time: 2.370s
| Adam | epoch: 002 | loss: 0.24038 - acc: 0.9075 -- iter: 00100/28000
Training Step: 563  | total loss: [1m[32m0.23292[0m[0m | time: 3.641s
| Adam | epoch: 002 | loss: 0.23292 - acc: 0.9107 -- iter: 00150/28000
Training Step: 564  | total loss: [1m[32m0.23434[0m[0m | time: 5.013s
| Adam | epoch: 002 | loss: 0.23434 - acc: 0.9137 -- iter: 00200/28000
Training Step: 565  | total loss: [1m[32m0.23417[0m[0m | time: 6.575s
| Adam | epoch: 002 | loss: 0.23417 - acc: 0.9123 -- iter: 00250/28000
Training Step: 566  | total loss: [1m[32m0.23602[0m[0m | time: 7.823s
| Adam | epoch: 002 | loss: 0.23602 - acc: 0.9091 -- iter: 00300/28000
Training Step: 567  | total loss: [1m[32m0.23373[0m[0m | time: 9.056s
| Adam | epoch: 002 | loss: 0.23373 - acc: 0.9142 -- iter: 00350/28000
Training Step: 568  | total loss: [1m[32m0.23780[0m[0m | time: 10.227s
| Adam | epoch: 002 | loss: 0.23780 - acc: 0.9127 -- iter: 00400/28000
Training Step: 569  | total loss: [1m[32m0.23818[0m[0m | time: 11.676s
| Adam | epoch: 002 | loss: 0.23818 - acc: 0.9135 -- iter: 00450/28000
Training Step: 570  | total loss: [1m[32m0.22578[0m[0m | time: 13.305s
| Adam | epoch: 002 | loss: 0.22578 - acc: 0.9201 -- iter: 00500/28000
Training Step: 571  | total loss: [1m[32m0.22136[0m[0m | time: 14.393s
| Adam | epoch: 002 | loss: 0.22136 - acc: 0.9221 -- iter: 00550/28000
Training Step: 572  | total loss: [1m[32m0.22094[0m[0m | time: 15.484s
| Adam | epoch: 002 | loss: 0.22094 - acc: 0.9199 -- iter: 00600/28000
Training Step: 573  | total loss: [1m[32m0.20997[0m[0m | time: 16.636s
| Adam | epoch: 002 | loss: 0.20997 - acc: 0.9259 -- iter: 00650/28000
Training Step: 574  | total loss: [1m[32m0.22326[0m[0m | time: 17.845s
| Adam | epoch: 002 | loss: 0.22326 - acc: 0.9233 -- iter: 00700/28000
Training Step: 575  | total loss: [1m[32m0.22632[0m[0m | time: 47.566s
| Adam | epoch: 002 | loss: 0.22632 - acc: 0.9250 | val_loss: 0.27239 - val_acc: 0.8950 -- iter: 00750/28000
--
Training Step: 576  | total loss: [1m[32m0.21839[0m[0m | time: 48.999s
| Adam | epoch: 002 | loss: 0.21839 - acc: 0.9245 -- iter: 00800/28000
Training Step: 577  | total loss: [1m[32m0.23257[0m[0m | time: 50.378s
| Adam | epoch: 002 | loss: 0.23257 - acc: 0.9180 -- iter: 00850/28000
Training Step: 578  | total loss: [1m[32m0.22864[0m[0m | time: 52.017s
| Adam | epoch: 002 | loss: 0.22864 - acc: 0.9202 -- iter: 00900/28000
Training Step: 579  | total loss: [1m[32m0.21363[0m[0m | time: 53.287s
| Adam | epoch: 002 | loss: 0.21363 - acc: 0.9262 -- iter: 00950/28000
Training Step: 580  | total loss: [1m[32m0.21256[0m[0m | time: 54.353s
| Adam | epoch: 002 | loss: 0.21256 - acc: 0.9276 -- iter: 01000/28000
Training Step: 581  | total loss: [1m[32m0.20497[0m[0m | time: 55.424s
| Adam | epoch: 002 | loss: 0.20497 - acc: 0.9308 -- iter: 01050/28000
Training Step: 582  | total loss: [1m[32m0.20477[0m[0m | time: 56.708s
| Adam | epoch: 002 | loss: 0.20477 - acc: 0.9297 -- iter: 01100/28000
Training Step: 583  | total loss: [1m[32m0.21649[0m[0m | time: 58.078s
| Adam | epoch: 002 | loss: 0.21649 - acc: 0.9228 -- iter: 01150/28000
Training Step: 584  | total loss: [1m[32m0.21908[0m[0m | time: 59.412s
| Adam | epoch: 002 | loss: 0.21908 - acc: 0.9245 -- iter: 01200/28000
Training Step: 585  | total loss: [1m[32m0.22316[0m[0m | time: 60.821s
| Adam | epoch: 002 | loss: 0.22316 - acc: 0.9180 -- iter: 01250/28000
Training Step: 586  | total loss: [1m[32m0.21781[0m[0m | time: 62.115s
| Adam | epoch: 002 | loss: 0.21781 - acc: 0.9222 -- iter: 01300/28000
Training Step: 587  | total loss: [1m[32m0.23298[0m[0m | time: 63.307s
| Adam | epoch: 002 | loss: 0.23298 - acc: 0.9200 -- iter: 01350/28000
Training Step: 588  | total loss: [1m[32m0.22581[0m[0m | time: 64.504s
| Adam | epoch: 002 | loss: 0.22581 - acc: 0.9240 -- iter: 01400/28000
Training Step: 589  | total loss: [1m[32m0.22676[0m[0m | time: 65.866s
| Adam | epoch: 002 | loss: 0.22676 - acc: 0.9236 -- iter: 01450/28000
Training Step: 590  | total loss: [1m[32m0.21569[0m[0m | time: 67.577s
| Adam | epoch: 002 | loss: 0.21569 - acc: 0.9273 -- iter: 01500/28000
Training Step: 591  | total loss: [1m[32m0.21556[0m[0m | time: 68.620s
| Adam | epoch: 002 | loss: 0.21556 - acc: 0.9245 -- iter: 01550/28000
Training Step: 592  | total loss: [1m[32m0.20622[0m[0m | time: 69.568s
| Adam | epoch: 002 | loss: 0.20622 - acc: 0.9281 -- iter: 01600/28000
Training Step: 593  | total loss: [1m[32m0.20452[0m[0m | time: 70.618s
| Adam | epoch: 002 | loss: 0.20452 - acc: 0.9313 -- iter: 01650/28000
Training Step: 594  | total loss: [1m[32m0.21242[0m[0m | time: 71.770s
| Adam | epoch: 002 | loss: 0.21242 - acc: 0.9281 -- iter: 01700/28000
Training Step: 595  | total loss: [1m[32m0.20466[0m[0m | time: 72.928s
| Adam | epoch: 002 | loss: 0.20466 - acc: 0.9293 -- iter: 01750/28000
Training Step: 596  | total loss: [1m[32m0.22412[0m[0m | time: 74.253s
| Adam | epoch: 002 | loss: 0.22412 - acc: 0.9284 -- iter: 01800/28000
Training Step: 597  | total loss: [1m[32m0.22869[0m[0m | time: 75.627s
| Adam | epoch: 002 | loss: 0.22869 - acc: 0.9236 -- iter: 01850/28000
Training Step: 598  | total loss: [1m[32m0.22052[0m[0m | time: 76.985s
| Adam | epoch: 002 | loss: 0.22052 - acc: 0.9252 -- iter: 01900/28000
Training Step: 599  | total loss: [1m[32m0.21983[0m[0m | time: 78.230s
| Adam | epoch: 002 | loss: 0.21983 - acc: 0.9287 -- iter: 01950/28000
Training Step: 600  | total loss: [1m[32m0.21337[0m[0m | time: 114.331s
| Adam | epoch: 002 | loss: 0.21337 - acc: 0.9278 | val_loss: 0.25424 - val_acc: 0.9044 -- iter: 02000/28000
--
Training Step: 601  | total loss: [1m[32m0.22485[0m[0m | time: 115.898s
| Adam | epoch: 002 | loss: 0.22485 - acc: 0.9250 -- iter: 02050/28000
Training Step: 602  | total loss: [1m[32m0.22264[0m[0m | time: 117.326s
| Adam | epoch: 002 | loss: 0.22264 - acc: 0.9245 -- iter: 02100/28000
Training Step: 603  | total loss: [1m[32m0.21807[0m[0m | time: 118.879s
| Adam | epoch: 002 | loss: 0.21807 - acc: 0.9281 -- iter: 02150/28000
Training Step: 604  | total loss: [1m[32m0.22308[0m[0m | time: 120.454s
| Adam | epoch: 002 | loss: 0.22308 - acc: 0.9293 -- iter: 02200/28000
Training Step: 605  | total loss: [1m[32m0.23906[0m[0m | time: 122.162s
| Adam | epoch: 002 | loss: 0.23906 - acc: 0.9243 -- iter: 02250/28000
Training Step: 606  | total loss: [1m[32m0.24385[0m[0m | time: 123.731s
| Adam | epoch: 002 | loss: 0.24385 - acc: 0.9159 -- iter: 02300/28000
Training Step: 607  | total loss: [1m[32m0.23934[0m[0m | time: 125.237s
| Adam | epoch: 002 | loss: 0.23934 - acc: 0.9163 -- iter: 02350/28000
Training Step: 608  | total loss: [1m[32m0.24044[0m[0m | time: 126.696s
| Adam | epoch: 002 | loss: 0.24044 - acc: 0.9127 -- iter: 02400/28000
Training Step: 609  | total loss: [1m[32m0.24353[0m[0m | time: 128.257s
| Adam | epoch: 002 | loss: 0.24353 - acc: 0.9114 -- iter: 02450/28000
Training Step: 610  | total loss: [1m[32m0.24081[0m[0m | time: 129.920s
| Adam | epoch: 002 | loss: 0.24081 - acc: 0.9103 -- iter: 02500/28000
Training Step: 611  | total loss: [1m[32m0.25515[0m[0m | time: 131.836s
| Adam | epoch: 002 | loss: 0.25515 - acc: 0.9032 -- iter: 02550/28000
Training Step: 612  | total loss: [1m[32m0.24991[0m[0m | time: 133.126s
| Adam | epoch: 002 | loss: 0.24991 - acc: 0.9049 -- iter: 02600/28000
Training Step: 613  | total loss: [1m[32m0.23997[0m[0m | time: 134.723s
| Adam | epoch: 002 | loss: 0.23997 - acc: 0.9084 -- iter: 02650/28000
Training Step: 614  | total loss: [1m[32m0.23916[0m[0m | time: 136.251s
| Adam | epoch: 002 | loss: 0.23916 - acc: 0.9116 -- iter: 02700/28000
Training Step: 615  | total loss: [1m[32m0.23071[0m[0m | time: 137.644s
| Adam | epoch: 002 | loss: 0.23071 - acc: 0.9164 -- iter: 02750/28000
Training Step: 616  | total loss: [1m[32m0.23033[0m[0m | time: 138.922s
| Adam | epoch: 002 | loss: 0.23033 - acc: 0.9148 -- iter: 02800/28000
Training Step: 617  | total loss: [1m[32m0.23538[0m[0m | time: 140.117s
| Adam | epoch: 002 | loss: 0.23538 - acc: 0.9153 -- iter: 02850/28000
Training Step: 618  | total loss: [1m[32m0.23022[0m[0m | time: 141.267s
| Adam | epoch: 002 | loss: 0.23022 - acc: 0.9178 -- iter: 02900/28000
Training Step: 619  | total loss: [1m[32m0.22146[0m[0m | time: 142.535s
| Adam | epoch: 002 | loss: 0.22146 - acc: 0.9200 -- iter: 02950/28000
Training Step: 620  | total loss: [1m[32m0.22448[0m[0m | time: 144.004s
| Adam | epoch: 002 | loss: 0.22448 - acc: 0.9220 -- iter: 03000/28000
Training Step: 621  | total loss: [1m[32m0.23343[0m[0m | time: 145.330s
| Adam | epoch: 002 | loss: 0.23343 - acc: 0.9198 -- iter: 03050/28000
Training Step: 622  | total loss: [1m[32m0.23420[0m[0m | time: 146.713s
| Adam | epoch: 002 | loss: 0.23420 - acc: 0.9178 -- iter: 03100/28000
Training Step: 623  | total loss: [1m[32m0.23110[0m[0m | time: 147.930s
| Adam | epoch: 002 | loss: 0.23110 - acc: 0.9160 -- iter: 03150/28000
Training Step: 624  | total loss: [1m[32m0.23473[0m[0m | time: 149.009s
| Adam | epoch: 002 | loss: 0.23473 - acc: 0.9124 -- iter: 03200/28000
Training Step: 625  | total loss: [1m[32m0.24180[0m[0m | time: 183.929s
| Adam | epoch: 002 | loss: 0.24180 - acc: 0.9092 | val_loss: 0.22061 - val_acc: 0.9173 -- iter: 03250/28000
--
Training Step: 626  | total loss: [1m[32m0.24297[0m[0m | time: 185.245s
| Adam | epoch: 002 | loss: 0.24297 - acc: 0.9063 -- iter: 03300/28000
Training Step: 627  | total loss: [1m[32m0.24346[0m[0m | time: 186.509s
| Adam | epoch: 002 | loss: 0.24346 - acc: 0.9036 -- iter: 03350/28000
Training Step: 628  | total loss: [1m[32m0.24190[0m[0m | time: 187.636s
| Adam | epoch: 002 | loss: 0.24190 - acc: 0.8993 -- iter: 03400/28000
Training Step: 629  | total loss: [1m[32m0.23492[0m[0m | time: 188.785s
| Adam | epoch: 002 | loss: 0.23492 - acc: 0.9034 -- iter: 03450/28000
Training Step: 630  | total loss: [1m[32m0.22862[0m[0m | time: 190.109s
| Adam | epoch: 002 | loss: 0.22862 - acc: 0.9070 -- iter: 03500/28000
Training Step: 631  | total loss: [1m[32m0.22868[0m[0m | time: 191.401s
| Adam | epoch: 002 | loss: 0.22868 - acc: 0.9103 -- iter: 03550/28000
Training Step: 632  | total loss: [1m[32m0.21970[0m[0m | time: 192.726s
| Adam | epoch: 002 | loss: 0.21970 - acc: 0.9153 -- iter: 03600/28000
Training Step: 633  | total loss: [1m[32m0.23367[0m[0m | time: 193.941s
| Adam | epoch: 002 | loss: 0.23367 - acc: 0.9138 -- iter: 03650/28000
Training Step: 634  | total loss: [1m[32m0.23317[0m[0m | time: 195.350s
| Adam | epoch: 002 | loss: 0.23317 - acc: 0.9124 -- iter: 03700/28000
Training Step: 635  | total loss: [1m[32m0.23568[0m[0m | time: 196.492s
| Adam | epoch: 002 | loss: 0.23568 - acc: 0.9111 -- iter: 03750/28000
Training Step: 636  | total loss: [1m[32m0.23650[0m[0m | time: 197.602s
| Adam | epoch: 002 | loss: 0.23650 - acc: 0.9120 -- iter: 03800/28000
Training Step: 637  | total loss: [1m[32m0.24404[0m[0m | time: 198.746s
| Adam | epoch: 002 | loss: 0.24404 - acc: 0.9168 -- iter: 03850/28000
Training Step: 638  | total loss: [1m[32m0.23833[0m[0m | time: 199.856s
| Adam | epoch: 002 | loss: 0.23833 - acc: 0.9151 -- iter: 03900/28000
Training Step: 639  | total loss: [1m[32m0.24878[0m[0m | time: 200.969s
| Adam | epoch: 002 | loss: 0.24878 - acc: 0.9116 -- iter: 03950/28000
Training Step: 640  | total loss: [1m[32m0.25132[0m[0m | time: 202.094s
| Adam | epoch: 002 | loss: 0.25132 - acc: 0.9145 -- iter: 04000/28000
Training Step: 641  | total loss: [1m[32m0.24726[0m[0m | time: 203.222s
| Adam | epoch: 002 | loss: 0.24726 - acc: 0.9150 -- iter: 04050/28000
Training Step: 642  | total loss: [1m[32m0.24771[0m[0m | time: 204.377s
| Adam | epoch: 002 | loss: 0.24771 - acc: 0.9195 -- iter: 04100/28000
Training Step: 643  | total loss: [1m[32m0.24610[0m[0m | time: 205.545s
| Adam | epoch: 002 | loss: 0.24610 - acc: 0.9196 -- iter: 04150/28000
Training Step: 644  | total loss: [1m[32m0.24911[0m[0m | time: 206.722s
| Adam | epoch: 002 | loss: 0.24911 - acc: 0.9136 -- iter: 04200/28000
Training Step: 645  | total loss: [1m[32m0.23642[0m[0m | time: 207.733s
| Adam | epoch: 002 | loss: 0.23642 - acc: 0.9182 -- iter: 04250/28000
Training Step: 646  | total loss: [1m[32m0.23967[0m[0m | time: 208.878s
| Adam | epoch: 002 | loss: 0.23967 - acc: 0.9164 -- iter: 04300/28000
Training Step: 647  | total loss: [1m[32m0.23836[0m[0m | time: 210.145s
| Adam | epoch: 002 | loss: 0.23836 - acc: 0.9168 -- iter: 04350/28000
Training Step: 648  | total loss: [1m[32m0.24220[0m[0m | time: 211.360s
| Adam | epoch: 002 | loss: 0.24220 - acc: 0.9151 -- iter: 04400/28000
Training Step: 649  | total loss: [1m[32m0.24029[0m[0m | time: 212.413s
| Adam | epoch: 002 | loss: 0.24029 - acc: 0.9136 -- iter: 04450/28000
Training Step: 650  | total loss: [1m[32m0.23488[0m[0m | time: 240.455s
| Adam | epoch: 002 | loss: 0.23488 - acc: 0.9142 | val_loss: 0.22606 - val_acc: 0.9161 -- iter: 04500/28000
--
Training Step: 651  | total loss: [1m[32m0.23535[0m[0m | time: 241.690s
| Adam | epoch: 002 | loss: 0.23535 - acc: 0.9148 -- iter: 04550/28000
Training Step: 652  | total loss: [1m[32m0.23452[0m[0m | time: 242.845s
| Adam | epoch: 002 | loss: 0.23452 - acc: 0.9213 -- iter: 04600/28000
Training Step: 653  | total loss: [1m[32m0.23445[0m[0m | time: 243.995s
| Adam | epoch: 002 | loss: 0.23445 - acc: 0.9192 -- iter: 04650/28000
Training Step: 654  | total loss: [1m[32m0.22926[0m[0m | time: 245.153s
| Adam | epoch: 002 | loss: 0.22926 - acc: 0.9193 -- iter: 04700/28000
Training Step: 655  | total loss: [1m[32m0.23032[0m[0m | time: 246.091s
| Adam | epoch: 002 | loss: 0.23032 - acc: 0.9173 -- iter: 04750/28000
Training Step: 656  | total loss: [1m[32m0.22461[0m[0m | time: 247.171s
| Adam | epoch: 002 | loss: 0.22461 - acc: 0.9196 -- iter: 04800/28000
Training Step: 657  | total loss: [1m[32m0.22550[0m[0m | time: 248.336s
| Adam | epoch: 002 | loss: 0.22550 - acc: 0.9177 -- iter: 04850/28000
Training Step: 658  | total loss: [1m[32m0.22208[0m[0m | time: 249.568s
| Adam | epoch: 002 | loss: 0.22208 - acc: 0.9199 -- iter: 04900/28000
Training Step: 659  | total loss: [1m[32m0.21455[0m[0m | time: 250.711s
| Adam | epoch: 002 | loss: 0.21455 - acc: 0.9219 -- iter: 04950/28000
Training Step: 660  | total loss: [1m[32m0.20546[0m[0m | time: 251.834s
| Adam | epoch: 002 | loss: 0.20546 - acc: 0.9277 -- iter: 05000/28000
Training Step: 661  | total loss: [1m[32m0.20896[0m[0m | time: 252.970s
| Adam | epoch: 002 | loss: 0.20896 - acc: 0.9269 -- iter: 05050/28000
Training Step: 662  | total loss: [1m[32m0.21811[0m[0m | time: 254.111s
| Adam | epoch: 002 | loss: 0.21811 - acc: 0.9262 -- iter: 05100/28000
Training Step: 663  | total loss: [1m[32m0.21115[0m[0m | time: 255.391s
| Adam | epoch: 002 | loss: 0.21115 - acc: 0.9276 -- iter: 05150/28000
Training Step: 664  | total loss: [1m[32m0.22901[0m[0m | time: 256.511s
| Adam | epoch: 002 | loss: 0.22901 - acc: 0.9249 -- iter: 05200/28000
Training Step: 665  | total loss: [1m[32m0.21915[0m[0m | time: 257.642s
| Adam | epoch: 002 | loss: 0.21915 - acc: 0.9284 -- iter: 05250/28000
Training Step: 666  | total loss: [1m[32m0.21494[0m[0m | time: 258.792s
| Adam | epoch: 002 | loss: 0.21494 - acc: 0.9275 -- iter: 05300/28000
Training Step: 667  | total loss: [1m[32m0.21392[0m[0m | time: 259.921s
| Adam | epoch: 002 | loss: 0.21392 - acc: 0.9288 -- iter: 05350/28000
Training Step: 668  | total loss: [1m[32m0.20463[0m[0m | time: 261.065s
| Adam | epoch: 002 | loss: 0.20463 - acc: 0.9319 -- iter: 05400/28000
Training Step: 669  | total loss: [1m[32m0.21032[0m[0m | time: 262.215s
| Adam | epoch: 002 | loss: 0.21032 - acc: 0.9347 -- iter: 05450/28000
Training Step: 670  | total loss: [1m[32m0.20164[0m[0m | time: 263.167s
| Adam | epoch: 002 | loss: 0.20164 - acc: 0.9372 -- iter: 05500/28000
Training Step: 671  | total loss: [1m[32m0.19965[0m[0m | time: 264.288s
| Adam | epoch: 002 | loss: 0.19965 - acc: 0.9415 -- iter: 05550/28000
Training Step: 672  | total loss: [1m[32m0.20517[0m[0m | time: 265.352s
| Adam | epoch: 002 | loss: 0.20517 - acc: 0.9374 -- iter: 05600/28000
Training Step: 673  | total loss: [1m[32m0.20418[0m[0m | time: 266.492s
| Adam | epoch: 002 | loss: 0.20418 - acc: 0.9356 -- iter: 05650/28000
Training Step: 674  | total loss: [1m[32m0.21245[0m[0m | time: 267.688s
| Adam | epoch: 002 | loss: 0.21245 - acc: 0.9321 -- iter: 05700/28000
Training Step: 675  | total loss: [1m[32m0.20622[0m[0m | time: 294.825s
| Adam | epoch: 002 | loss: 0.20622 - acc: 0.9349 | val_loss: 0.23068 - val_acc: 0.9122 -- iter: 05750/28000
--
Training Step: 676  | total loss: [1m[32m0.20825[0m[0m | time: 296.060s
| Adam | epoch: 002 | loss: 0.20825 - acc: 0.9354 -- iter: 05800/28000
Training Step: 677  | total loss: [1m[32m0.21750[0m[0m | time: 297.035s
| Adam | epoch: 002 | loss: 0.21750 - acc: 0.9298 -- iter: 05850/28000
Training Step: 678  | total loss: [1m[32m0.23209[0m[0m | time: 298.092s
| Adam | epoch: 002 | loss: 0.23209 - acc: 0.9209 -- iter: 05900/28000
Training Step: 679  | total loss: [1m[32m0.23613[0m[0m | time: 299.189s
| Adam | epoch: 002 | loss: 0.23613 - acc: 0.9168 -- iter: 05950/28000
Training Step: 680  | total loss: [1m[32m0.23752[0m[0m | time: 300.300s
| Adam | epoch: 002 | loss: 0.23752 - acc: 0.9151 -- iter: 06000/28000
Training Step: 681  | total loss: [1m[32m0.22070[0m[0m | time: 301.469s
| Adam | epoch: 002 | loss: 0.22070 - acc: 0.9236 -- iter: 06050/28000
Training Step: 682  | total loss: [1m[32m0.20968[0m[0m | time: 302.586s
| Adam | epoch: 002 | loss: 0.20968 - acc: 0.9292 -- iter: 06100/28000
Training Step: 683  | total loss: [1m[32m0.20100[0m[0m | time: 303.723s
| Adam | epoch: 002 | loss: 0.20100 - acc: 0.9343 -- iter: 06150/28000
Training Step: 684  | total loss: [1m[32m0.19858[0m[0m | time: 304.864s
| Adam | epoch: 002 | loss: 0.19858 - acc: 0.9329 -- iter: 06200/28000
Training Step: 685  | total loss: [1m[32m0.19631[0m[0m | time: 305.980s
| Adam | epoch: 002 | loss: 0.19631 - acc: 0.9356 -- iter: 06250/28000
Training Step: 686  | total loss: [1m[32m0.19619[0m[0m | time: 307.113s
| Adam | epoch: 002 | loss: 0.19619 - acc: 0.9360 -- iter: 06300/28000
Training Step: 687  | total loss: [1m[32m0.19903[0m[0m | time: 308.090s
| Adam | epoch: 002 | loss: 0.19903 - acc: 0.9344 -- iter: 06350/28000
Training Step: 688  | total loss: [1m[32m0.20763[0m[0m | time: 309.217s
| Adam | epoch: 002 | loss: 0.20763 - acc: 0.9290 -- iter: 06400/28000
Training Step: 689  | total loss: [1m[32m0.21653[0m[0m | time: 310.291s
| Adam | epoch: 002 | loss: 0.21653 - acc: 0.9221 -- iter: 06450/28000
Training Step: 690  | total loss: [1m[32m0.21677[0m[0m | time: 311.437s
| Adam | epoch: 002 | loss: 0.21677 - acc: 0.9219 -- iter: 06500/28000
Training Step: 691  | total loss: [1m[32m0.22433[0m[0m | time: 312.587s
| Adam | epoch: 002 | loss: 0.22433 - acc: 0.9177 -- iter: 06550/28000
Training Step: 692  | total loss: [1m[32m0.21309[0m[0m | time: 313.711s
| Adam | epoch: 002 | loss: 0.21309 - acc: 0.9239 -- iter: 06600/28000
Training Step: 693  | total loss: [1m[32m0.19673[0m[0m | time: 314.843s
| Adam | epoch: 002 | loss: 0.19673 - acc: 0.9315 -- iter: 06650/28000
Training Step: 694  | total loss: [1m[32m0.20057[0m[0m | time: 315.983s
| Adam | epoch: 002 | loss: 0.20057 - acc: 0.9264 -- iter: 06700/28000
Training Step: 695  | total loss: [1m[32m0.19630[0m[0m | time: 317.149s
| Adam | epoch: 002 | loss: 0.19630 - acc: 0.9277 -- iter: 06750/28000
Training Step: 696  | total loss: [1m[32m0.20897[0m[0m | time: 318.284s
| Adam | epoch: 002 | loss: 0.20897 - acc: 0.9270 -- iter: 06800/28000
Training Step: 697  | total loss: [1m[32m0.21615[0m[0m | time: 319.411s
| Adam | epoch: 002 | loss: 0.21615 - acc: 0.9223 -- iter: 06850/28000
Training Step: 698  | total loss: [1m[32m0.20983[0m[0m | time: 320.368s
| Adam | epoch: 002 | loss: 0.20983 - acc: 0.9240 -- iter: 06900/28000
Training Step: 699  | total loss: [1m[32m0.23797[0m[0m | time: 321.480s
| Adam | epoch: 002 | loss: 0.23797 - acc: 0.9196 -- iter: 06950/28000
Training Step: 700  | total loss: [1m[32m0.22789[0m[0m | time: 348.574s
| Adam | epoch: 002 | loss: 0.22789 - acc: 0.9257 | val_loss: 0.21807 - val_acc: 0.9186 -- iter: 07000/28000
--
Training Step: 701  | total loss: [1m[32m0.23577[0m[0m | time: 349.583s
| Adam | epoch: 002 | loss: 0.23577 - acc: 0.9171 -- iter: 07050/28000
Training Step: 702  | total loss: [1m[32m0.23391[0m[0m | time: 350.660s
| Adam | epoch: 002 | loss: 0.23391 - acc: 0.9174 -- iter: 07100/28000
Training Step: 703  | total loss: [1m[32m0.24013[0m[0m | time: 351.790s
| Adam | epoch: 002 | loss: 0.24013 - acc: 0.9097 -- iter: 07150/28000
Training Step: 704  | total loss: [1m[32m0.23456[0m[0m | time: 352.951s
| Adam | epoch: 002 | loss: 0.23456 - acc: 0.9107 -- iter: 07200/28000
Training Step: 705  | total loss: [1m[32m0.22954[0m[0m | time: 354.088s
| Adam | epoch: 002 | loss: 0.22954 - acc: 0.9136 -- iter: 07250/28000
Training Step: 706  | total loss: [1m[32m0.22108[0m[0m | time: 355.240s
| Adam | epoch: 002 | loss: 0.22108 - acc: 0.9203 -- iter: 07300/28000
Training Step: 707  | total loss: [1m[32m0.22028[0m[0m | time: 356.382s
| Adam | epoch: 002 | loss: 0.22028 - acc: 0.9182 -- iter: 07350/28000
Training Step: 708  | total loss: [1m[32m0.22468[0m[0m | time: 357.324s
| Adam | epoch: 002 | loss: 0.22468 - acc: 0.9164 -- iter: 07400/28000
Training Step: 709  | total loss: [1m[32m0.21603[0m[0m | time: 358.409s
| Adam | epoch: 002 | loss: 0.21603 - acc: 0.9208 -- iter: 07450/28000
Training Step: 710  | total loss: [1m[32m0.21699[0m[0m | time: 359.533s
| Adam | epoch: 002 | loss: 0.21699 - acc: 0.9187 -- iter: 07500/28000
Training Step: 711  | total loss: [1m[32m0.21385[0m[0m | time: 360.737s
| Adam | epoch: 002 | loss: 0.21385 - acc: 0.9188 -- iter: 07550/28000
Training Step: 712  | total loss: [1m[32m0.20282[0m[0m | time: 361.891s
| Adam | epoch: 002 | loss: 0.20282 - acc: 0.9249 -- iter: 07600/28000
Training Step: 713  | total loss: [1m[32m0.21217[0m[0m | time: 363.027s
| Adam | epoch: 002 | loss: 0.21217 - acc: 0.9164 -- iter: 07650/28000
Training Step: 714  | total loss: [1m[32m0.22007[0m[0m | time: 364.180s
| Adam | epoch: 002 | loss: 0.22007 - acc: 0.9168 -- iter: 07700/28000
Training Step: 715  | total loss: [1m[32m0.22093[0m[0m | time: 365.130s
| Adam | epoch: 002 | loss: 0.22093 - acc: 0.9211 -- iter: 07750/28000
Training Step: 716  | total loss: [1m[32m0.22342[0m[0m | time: 366.213s
| Adam | epoch: 002 | loss: 0.22342 - acc: 0.9230 -- iter: 07800/28000
Training Step: 717  | total loss: [1m[32m0.23636[0m[0m | time: 367.328s
| Adam | epoch: 002 | loss: 0.23636 - acc: 0.9187 -- iter: 07850/28000
Training Step: 718  | total loss: [1m[32m0.23722[0m[0m | time: 368.474s
| Adam | epoch: 002 | loss: 0.23722 - acc: 0.9168 -- iter: 07900/28000
Training Step: 719  | total loss: [1m[32m0.22553[0m[0m | time: 369.608s
| Adam | epoch: 002 | loss: 0.22553 - acc: 0.9192 -- iter: 07950/28000
Training Step: 720  | total loss: [1m[32m0.22072[0m[0m | time: 370.715s
| Adam | epoch: 002 | loss: 0.22072 - acc: 0.9192 -- iter: 08000/28000
Training Step: 721  | total loss: [1m[32m0.21072[0m[0m | time: 371.872s
| Adam | epoch: 002 | loss: 0.21072 - acc: 0.9213 -- iter: 08050/28000
Training Step: 722  | total loss: [1m[32m0.20733[0m[0m | time: 372.792s
| Adam | epoch: 002 | loss: 0.20733 - acc: 0.9232 -- iter: 08100/28000
Training Step: 723  | total loss: [1m[32m0.21320[0m[0m | time: 373.867s
| Adam | epoch: 002 | loss: 0.21320 - acc: 0.9229 -- iter: 08150/28000
Training Step: 724  | total loss: [1m[32m0.22828[0m[0m | time: 374.972s
| Adam | epoch: 002 | loss: 0.22828 - acc: 0.9166 -- iter: 08200/28000
Training Step: 725  | total loss: [1m[32m0.23026[0m[0m | time: 400.821s
| Adam | epoch: 002 | loss: 0.23026 - acc: 0.9189 | val_loss: 0.21197 - val_acc: 0.9206 -- iter: 08250/28000
--
Training Step: 726  | total loss: [1m[32m0.22484[0m[0m | time: 401.985s
| Adam | epoch: 002 | loss: 0.22484 - acc: 0.9190 -- iter: 08300/28000
Training Step: 727  | total loss: [1m[32m0.21675[0m[0m | time: 403.067s
| Adam | epoch: 002 | loss: 0.21675 - acc: 0.9231 -- iter: 08350/28000
Training Step: 728  | total loss: [1m[32m0.22638[0m[0m | time: 404.128s
| Adam | epoch: 002 | loss: 0.22638 - acc: 0.9168 -- iter: 08400/28000
Training Step: 729  | total loss: [1m[32m0.22085[0m[0m | time: 405.190s
| Adam | epoch: 002 | loss: 0.22085 - acc: 0.9171 -- iter: 08450/28000
Training Step: 730  | total loss: [1m[32m0.21496[0m[0m | time: 406.258s
| Adam | epoch: 002 | loss: 0.21496 - acc: 0.9194 -- iter: 08500/28000
Training Step: 731  | total loss: [1m[32m0.21241[0m[0m | time: 407.343s
| Adam | epoch: 002 | loss: 0.21241 - acc: 0.9175 -- iter: 08550/28000
Training Step: 732  | total loss: [1m[32m0.22160[0m[0m | time: 408.417s
| Adam | epoch: 002 | loss: 0.22160 - acc: 0.9197 -- iter: 08600/28000
Training Step: 733  | total loss: [1m[32m0.21745[0m[0m | time: 409.501s
| Adam | epoch: 002 | loss: 0.21745 - acc: 0.9218 -- iter: 08650/28000
Training Step: 734  | total loss: [1m[32m0.21366[0m[0m | time: 410.561s
| Adam | epoch: 002 | loss: 0.21366 - acc: 0.9216 -- iter: 08700/28000
Training Step: 735  | total loss: [1m[32m0.21242[0m[0m | time: 411.632s
| Adam | epoch: 002 | loss: 0.21242 - acc: 0.9194 -- iter: 08750/28000
Training Step: 736  | total loss: [1m[32m0.21782[0m[0m | time: 412.704s
| Adam | epoch: 002 | loss: 0.21782 - acc: 0.9135 -- iter: 08800/28000
Training Step: 737  | total loss: [1m[32m0.21080[0m[0m | time: 413.786s
| Adam | epoch: 002 | loss: 0.21080 - acc: 0.9161 -- iter: 08850/28000
Training Step: 738  | total loss: [1m[32m0.20263[0m[0m | time: 414.849s
| Adam | epoch: 002 | loss: 0.20263 - acc: 0.9165 -- iter: 08900/28000
Training Step: 739  | total loss: [1m[32m0.21232[0m[0m | time: 415.900s
| Adam | epoch: 002 | loss: 0.21232 - acc: 0.9089 -- iter: 08950/28000
Training Step: 740  | total loss: [1m[32m0.21492[0m[0m | time: 416.973s
| Adam | epoch: 002 | loss: 0.21492 - acc: 0.9080 -- iter: 09000/28000
Training Step: 741  | total loss: [1m[32m0.20760[0m[0m | time: 418.044s
| Adam | epoch: 002 | loss: 0.20760 - acc: 0.9132 -- iter: 09050/28000
Training Step: 742  | total loss: [1m[32m0.21216[0m[0m | time: 419.099s
| Adam | epoch: 002 | loss: 0.21216 - acc: 0.9099 -- iter: 09100/28000
Training Step: 743  | total loss: [1m[32m0.20784[0m[0m | time: 420.158s
| Adam | epoch: 002 | loss: 0.20784 - acc: 0.9149 -- iter: 09150/28000
Training Step: 744  | total loss: [1m[32m0.19442[0m[0m | time: 421.251s
| Adam | epoch: 002 | loss: 0.19442 - acc: 0.9214 -- iter: 09200/28000
Training Step: 745  | total loss: [1m[32m0.20407[0m[0m | time: 422.324s
| Adam | epoch: 002 | loss: 0.20407 - acc: 0.9193 -- iter: 09250/28000
Training Step: 746  | total loss: [1m[32m0.20645[0m[0m | time: 423.379s
| Adam | epoch: 002 | loss: 0.20645 - acc: 0.9193 -- iter: 09300/28000
Training Step: 747  | total loss: [1m[32m0.23198[0m[0m | time: 424.458s
| Adam | epoch: 002 | loss: 0.23198 - acc: 0.9154 -- iter: 09350/28000
Training Step: 748  | total loss: [1m[32m0.21963[0m[0m | time: 425.681s
| Adam | epoch: 002 | loss: 0.21963 - acc: 0.9179 -- iter: 09400/28000
Training Step: 749  | total loss: [1m[32m0.21246[0m[0m | time: 426.774s
| Adam | epoch: 002 | loss: 0.21246 - acc: 0.9201 -- iter: 09450/28000
Training Step: 750  | total loss: [1m[32m0.22974[0m[0m | time: 452.042s
| Adam | epoch: 002 | loss: 0.22974 - acc: 0.9141 | val_loss: 0.22283 - val_acc: 0.9154 -- iter: 09500/28000
--
Training Step: 751  | total loss: [1m[32m0.21897[0m[0m | time: 453.145s
| Adam | epoch: 002 | loss: 0.21897 - acc: 0.9187 -- iter: 09550/28000
Training Step: 752  | total loss: [1m[32m0.21792[0m[0m | time: 454.229s
| Adam | epoch: 002 | loss: 0.21792 - acc: 0.9128 -- iter: 09600/28000
Training Step: 753  | total loss: [1m[32m0.21006[0m[0m | time: 455.320s
| Adam | epoch: 002 | loss: 0.21006 - acc: 0.9155 -- iter: 09650/28000
Training Step: 754  | total loss: [1m[32m0.20891[0m[0m | time: 456.382s
| Adam | epoch: 002 | loss: 0.20891 - acc: 0.9140 -- iter: 09700/28000
Training Step: 755  | total loss: [1m[32m0.20045[0m[0m | time: 457.466s
| Adam | epoch: 002 | loss: 0.20045 - acc: 0.9166 -- iter: 09750/28000
Training Step: 756  | total loss: [1m[32m0.19495[0m[0m | time: 458.564s
| Adam | epoch: 002 | loss: 0.19495 - acc: 0.9209 -- iter: 09800/28000
Training Step: 757  | total loss: [1m[32m0.19967[0m[0m | time: 459.638s
| Adam | epoch: 002 | loss: 0.19967 - acc: 0.9208 -- iter: 09850/28000
Training Step: 758  | total loss: [1m[32m0.19104[0m[0m | time: 460.683s
| Adam | epoch: 002 | loss: 0.19104 - acc: 0.9247 -- iter: 09900/28000
Training Step: 759  | total loss: [1m[32m0.19194[0m[0m | time: 461.754s
| Adam | epoch: 002 | loss: 0.19194 - acc: 0.9243 -- iter: 09950/28000
Training Step: 760  | total loss: [1m[32m0.18552[0m[0m | time: 462.819s
| Adam | epoch: 002 | loss: 0.18552 - acc: 0.9278 -- iter: 10000/28000
Training Step: 761  | total loss: [1m[32m0.18803[0m[0m | time: 463.912s
| Adam | epoch: 002 | loss: 0.18803 - acc: 0.9271 -- iter: 10050/28000
Training Step: 762  | total loss: [1m[32m0.17957[0m[0m | time: 464.968s
| Adam | epoch: 002 | loss: 0.17957 - acc: 0.9303 -- iter: 10100/28000
Training Step: 763  | total loss: [1m[32m0.18889[0m[0m | time: 466.038s
| Adam | epoch: 002 | loss: 0.18889 - acc: 0.9273 -- iter: 10150/28000
Training Step: 764  | total loss: [1m[32m0.18983[0m[0m | time: 467.109s
| Adam | epoch: 002 | loss: 0.18983 - acc: 0.9286 -- iter: 10200/28000
Training Step: 765  | total loss: [1m[32m0.19060[0m[0m | time: 468.177s
| Adam | epoch: 002 | loss: 0.19060 - acc: 0.9237 -- iter: 10250/28000
Training Step: 766  | total loss: [1m[32m0.19951[0m[0m | time: 469.258s
| Adam | epoch: 002 | loss: 0.19951 - acc: 0.9174 -- iter: 10300/28000
Training Step: 767  | total loss: [1m[32m0.21384[0m[0m | time: 470.344s
| Adam | epoch: 002 | loss: 0.21384 - acc: 0.9136 -- iter: 10350/28000
Training Step: 768  | total loss: [1m[32m0.21670[0m[0m | time: 471.435s
| Adam | epoch: 002 | loss: 0.21670 - acc: 0.9103 -- iter: 10400/28000
Training Step: 769  | total loss: [1m[32m0.21525[0m[0m | time: 472.512s
| Adam | epoch: 002 | loss: 0.21525 - acc: 0.9112 -- iter: 10450/28000
Training Step: 770  | total loss: [1m[32m0.20445[0m[0m | time: 473.587s
| Adam | epoch: 002 | loss: 0.20445 - acc: 0.9181 -- iter: 10500/28000
Training Step: 771  | total loss: [1m[32m0.19393[0m[0m | time: 474.664s
| Adam | epoch: 002 | loss: 0.19393 - acc: 0.9223 -- iter: 10550/28000
Training Step: 772  | total loss: [1m[32m0.18522[0m[0m | time: 475.738s
| Adam | epoch: 002 | loss: 0.18522 - acc: 0.9261 -- iter: 10600/28000
Training Step: 773  | total loss: [1m[32m0.18543[0m[0m | time: 476.814s
| Adam | epoch: 002 | loss: 0.18543 - acc: 0.9255 -- iter: 10650/28000
Training Step: 774  | total loss: [1m[32m0.18521[0m[0m | time: 477.873s
| Adam | epoch: 002 | loss: 0.18521 - acc: 0.9269 -- iter: 10700/28000
Training Step: 775  | total loss: [1m[32m0.18039[0m[0m | time: 503.176s
| Adam | epoch: 002 | loss: 0.18039 - acc: 0.9282 | val_loss: 0.21585 - val_acc: 0.9186 -- iter: 10750/28000
--
Training Step: 776  | total loss: [1m[32m0.17931[0m[0m | time: 504.305s
| Adam | epoch: 002 | loss: 0.17931 - acc: 0.9314 -- iter: 10800/28000
Training Step: 777  | total loss: [1m[32m0.18504[0m[0m | time: 505.396s
| Adam | epoch: 002 | loss: 0.18504 - acc: 0.9343 -- iter: 10850/28000
Training Step: 778  | total loss: [1m[32m0.17778[0m[0m | time: 506.478s
| Adam | epoch: 002 | loss: 0.17778 - acc: 0.9388 -- iter: 10900/28000
Training Step: 779  | total loss: [1m[32m0.17167[0m[0m | time: 507.536s
| Adam | epoch: 002 | loss: 0.17167 - acc: 0.9430 -- iter: 10950/28000
Training Step: 780  | total loss: [1m[32m0.18481[0m[0m | time: 508.605s
| Adam | epoch: 002 | loss: 0.18481 - acc: 0.9367 -- iter: 11000/28000
Training Step: 781  | total loss: [1m[32m0.18456[0m[0m | time: 509.660s
| Adam | epoch: 002 | loss: 0.18456 - acc: 0.9350 -- iter: 11050/28000
Training Step: 782  | total loss: [1m[32m0.18463[0m[0m | time: 510.714s
| Adam | epoch: 002 | loss: 0.18463 - acc: 0.9335 -- iter: 11100/28000
Training Step: 783  | total loss: [1m[32m0.17891[0m[0m | time: 511.781s
| Adam | epoch: 002 | loss: 0.17891 - acc: 0.9341 -- iter: 11150/28000
Training Step: 784  | total loss: [1m[32m0.18129[0m[0m | time: 512.848s
| Adam | epoch: 002 | loss: 0.18129 - acc: 0.9327 -- iter: 11200/28000
Training Step: 785  | total loss: [1m[32m0.18974[0m[0m | time: 514.027s
| Adam | epoch: 002 | loss: 0.18974 - acc: 0.9315 -- iter: 11250/28000
Training Step: 786  | total loss: [1m[32m0.19800[0m[0m | time: 515.133s
| Adam | epoch: 002 | loss: 0.19800 - acc: 0.9283 -- iter: 11300/28000
Training Step: 787  | total loss: [1m[32m0.19705[0m[0m | time: 516.193s
| Adam | epoch: 002 | loss: 0.19705 - acc: 0.9275 -- iter: 11350/28000
Training Step: 788  | total loss: [1m[32m0.19692[0m[0m | time: 517.241s
| Adam | epoch: 002 | loss: 0.19692 - acc: 0.9247 -- iter: 11400/28000
Training Step: 789  | total loss: [1m[32m0.18521[0m[0m | time: 518.285s
| Adam | epoch: 002 | loss: 0.18521 - acc: 0.9283 -- iter: 11450/28000
Training Step: 790  | total loss: [1m[32m0.18407[0m[0m | time: 519.363s
| Adam | epoch: 002 | loss: 0.18407 - acc: 0.9294 -- iter: 11500/28000
Training Step: 791  | total loss: [1m[32m0.17716[0m[0m | time: 520.416s
| Adam | epoch: 002 | loss: 0.17716 - acc: 0.9325 -- iter: 11550/28000
Training Step: 792  | total loss: [1m[32m0.18157[0m[0m | time: 521.466s
| Adam | epoch: 002 | loss: 0.18157 - acc: 0.9312 -- iter: 11600/28000
Training Step: 793  | total loss: [1m[32m0.17562[0m[0m | time: 522.527s
| Adam | epoch: 002 | loss: 0.17562 - acc: 0.9341 -- iter: 11650/28000
Training Step: 794  | total loss: [1m[32m0.18693[0m[0m | time: 523.588s
| Adam | epoch: 002 | loss: 0.18693 - acc: 0.9327 -- iter: 11700/28000
Training Step: 795  | total loss: [1m[32m0.18523[0m[0m | time: 524.656s
| Adam | epoch: 002 | loss: 0.18523 - acc: 0.9374 -- iter: 11750/28000
Training Step: 796  | total loss: [1m[32m0.18000[0m[0m | time: 525.707s
| Adam | epoch: 002 | loss: 0.18000 - acc: 0.9377 -- iter: 11800/28000
Training Step: 797  | total loss: [1m[32m0.18243[0m[0m | time: 526.757s
| Adam | epoch: 002 | loss: 0.18243 - acc: 0.9379 -- iter: 11850/28000
Training Step: 798  | total loss: [1m[32m0.18836[0m[0m | time: 527.819s
| Adam | epoch: 002 | loss: 0.18836 - acc: 0.9401 -- iter: 11900/28000
Training Step: 799  | total loss: [1m[32m0.18780[0m[0m | time: 528.896s
| Adam | epoch: 002 | loss: 0.18780 - acc: 0.9421 -- iter: 11950/28000
Training Step: 800  | total loss: [1m[32m0.19087[0m[0m | time: 555.142s
| Adam | epoch: 002 | loss: 0.19087 - acc: 0.9359 | val_loss: 0.22085 - val_acc: 0.9197 -- iter: 12000/28000
--
Training Step: 801  | total loss: [1m[32m0.18572[0m[0m | time: 556.275s
| Adam | epoch: 002 | loss: 0.18572 - acc: 0.9343 -- iter: 12050/28000
Training Step: 802  | total loss: [1m[32m0.20242[0m[0m | time: 557.342s
| Adam | epoch: 002 | loss: 0.20242 - acc: 0.9269 -- iter: 12100/28000
Training Step: 803  | total loss: [1m[32m0.21137[0m[0m | time: 558.407s
| Adam | epoch: 002 | loss: 0.21137 - acc: 0.9202 -- iter: 12150/28000
Training Step: 804  | total loss: [1m[32m0.20470[0m[0m | time: 559.488s
| Adam | epoch: 002 | loss: 0.20470 - acc: 0.9262 -- iter: 12200/28000
Training Step: 805  | total loss: [1m[32m0.20438[0m[0m | time: 560.557s
| Adam | epoch: 002 | loss: 0.20438 - acc: 0.9236 -- iter: 12250/28000
Training Step: 806  | total loss: [1m[32m0.19641[0m[0m | time: 561.633s
| Adam | epoch: 002 | loss: 0.19641 - acc: 0.9272 -- iter: 12300/28000
Training Step: 807  | total loss: [1m[32m0.18634[0m[0m | time: 562.689s
| Adam | epoch: 002 | loss: 0.18634 - acc: 0.9305 -- iter: 12350/28000
Training Step: 808  | total loss: [1m[32m0.19989[0m[0m | time: 563.785s
| Adam | epoch: 002 | loss: 0.19989 - acc: 0.9234 -- iter: 12400/28000
Training Step: 809  | total loss: [1m[32m0.19428[0m[0m | time: 564.860s
| Adam | epoch: 002 | loss: 0.19428 - acc: 0.9251 -- iter: 12450/28000
Training Step: 810  | total loss: [1m[32m0.18985[0m[0m | time: 565.930s
| Adam | epoch: 002 | loss: 0.18985 - acc: 0.9286 -- iter: 12500/28000
Training Step: 811  | total loss: [1m[32m0.20266[0m[0m | time: 566.986s
| Adam | epoch: 002 | loss: 0.20266 - acc: 0.9217 -- iter: 12550/28000
Training Step: 812  | total loss: [1m[32m0.20038[0m[0m | time: 568.111s
| Adam | epoch: 002 | loss: 0.20038 - acc: 0.9236 -- iter: 12600/28000
Training Step: 813  | total loss: [1m[32m0.19351[0m[0m | time: 569.197s
| Adam | epoch: 002 | loss: 0.19351 - acc: 0.9252 -- iter: 12650/28000
Training Step: 814  | total loss: [1m[32m0.18606[0m[0m | time: 570.288s
| Adam | epoch: 002 | loss: 0.18606 - acc: 0.9307 -- iter: 12700/28000
Training Step: 815  | total loss: [1m[32m0.19102[0m[0m | time: 571.349s
| Adam | epoch: 002 | loss: 0.19102 - acc: 0.9296 -- iter: 12750/28000
Training Step: 816  | total loss: [1m[32m0.18547[0m[0m | time: 572.419s
| Adam | epoch: 002 | loss: 0.18547 - acc: 0.9346 -- iter: 12800/28000
Training Step: 817  | total loss: [1m[32m0.19021[0m[0m | time: 573.480s
| Adam | epoch: 002 | loss: 0.19021 - acc: 0.9332 -- iter: 12850/28000
Training Step: 818  | total loss: [1m[32m0.17864[0m[0m | time: 574.557s
| Adam | epoch: 002 | loss: 0.17864 - acc: 0.9379 -- iter: 12900/28000
Training Step: 819  | total loss: [1m[32m0.18491[0m[0m | time: 575.637s
| Adam | epoch: 002 | loss: 0.18491 - acc: 0.9361 -- iter: 12950/28000
Training Step: 820  | total loss: [1m[32m0.19070[0m[0m | time: 576.693s
| Adam | epoch: 002 | loss: 0.19070 - acc: 0.9345 -- iter: 13000/28000
Training Step: 821  | total loss: [1m[32m0.19249[0m[0m | time: 577.768s
| Adam | epoch: 002 | loss: 0.19249 - acc: 0.9330 -- iter: 13050/28000
Training Step: 822  | total loss: [1m[32m0.19766[0m[0m | time: 578.831s
| Adam | epoch: 002 | loss: 0.19766 - acc: 0.9317 -- iter: 13100/28000
Training Step: 823  | total loss: [1m[32m0.19747[0m[0m | time: 579.909s
| Adam | epoch: 002 | loss: 0.19747 - acc: 0.9345 -- iter: 13150/28000
Training Step: 824  | total loss: [1m[32m0.20771[0m[0m | time: 580.966s
| Adam | epoch: 002 | loss: 0.20771 - acc: 0.9331 -- iter: 13200/28000
Training Step: 825  | total loss: [1m[32m0.20325[0m[0m | time: 606.137s
| Adam | epoch: 002 | loss: 0.20325 - acc: 0.9298 | val_loss: 0.21142 - val_acc: 0.9199 -- iter: 13250/28000
--
Training Step: 826  | total loss: [1m[32m0.21777[0m[0m | time: 607.269s
| Adam | epoch: 002 | loss: 0.21777 - acc: 0.9168 -- iter: 13300/28000
Training Step: 827  | total loss: [1m[32m0.21560[0m[0m | time: 608.336s
| Adam | epoch: 002 | loss: 0.21560 - acc: 0.9211 -- iter: 13350/28000
Training Step: 828  | total loss: [1m[32m0.23181[0m[0m | time: 609.399s
| Adam | epoch: 002 | loss: 0.23181 - acc: 0.9210 -- iter: 13400/28000
Training Step: 829  | total loss: [1m[32m0.24515[0m[0m | time: 610.490s
| Adam | epoch: 002 | loss: 0.24515 - acc: 0.9149 -- iter: 13450/28000
Training Step: 830  | total loss: [1m[32m0.24396[0m[0m | time: 611.569s
| Adam | epoch: 002 | loss: 0.24396 - acc: 0.9134 -- iter: 13500/28000
Training Step: 831  | total loss: [1m[32m0.23725[0m[0m | time: 612.619s
| Adam | epoch: 002 | loss: 0.23725 - acc: 0.9161 -- iter: 13550/28000
Training Step: 832  | total loss: [1m[32m0.22987[0m[0m | time: 613.674s
| Adam | epoch: 002 | loss: 0.22987 - acc: 0.9205 -- iter: 13600/28000
Training Step: 833  | total loss: [1m[32m0.23103[0m[0m | time: 614.731s
| Adam | epoch: 002 | loss: 0.23103 - acc: 0.9224 -- iter: 13650/28000
Training Step: 834  | total loss: [1m[32m0.23345[0m[0m | time: 615.795s
| Adam | epoch: 002 | loss: 0.23345 - acc: 0.9222 -- iter: 13700/28000
Training Step: 835  | total loss: [1m[32m0.22409[0m[0m | time: 616.836s
| Adam | epoch: 002 | loss: 0.22409 - acc: 0.9280 -- iter: 13750/28000
Training Step: 836  | total loss: [1m[32m0.22943[0m[0m | time: 617.925s
| Adam | epoch: 002 | loss: 0.22943 - acc: 0.9212 -- iter: 13800/28000
Training Step: 837  | total loss: [1m[32m0.23291[0m[0m | time: 618.997s
| Adam | epoch: 002 | loss: 0.23291 - acc: 0.9191 -- iter: 13850/28000
Training Step: 838  | total loss: [1m[32m0.22658[0m[0m | time: 620.038s
| Adam | epoch: 002 | loss: 0.22658 - acc: 0.9231 -- iter: 13900/28000
Training Step: 839  | total loss: [1m[32m0.22409[0m[0m | time: 621.105s
| Adam | epoch: 002 | loss: 0.22409 - acc: 0.9208 -- iter: 13950/28000
Training Step: 840  | total loss: [1m[32m0.21667[0m[0m | time: 622.180s
| Adam | epoch: 002 | loss: 0.21667 - acc: 0.9247 -- iter: 14000/28000
Training Step: 841  | total loss: [1m[32m0.20265[0m[0m | time: 623.230s
| Adam | epoch: 002 | loss: 0.20265 - acc: 0.9323 -- iter: 14050/28000
Training Step: 842  | total loss: [1m[32m0.19972[0m[0m | time: 624.355s
| Adam | epoch: 002 | loss: 0.19972 - acc: 0.9330 -- iter: 14100/28000
Training Step: 843  | total loss: [1m[32m0.21083[0m[0m | time: 625.419s
| Adam | epoch: 002 | loss: 0.21083 - acc: 0.9297 -- iter: 14150/28000
Training Step: 844  | total loss: [1m[32m0.20736[0m[0m | time: 626.494s
| Adam | epoch: 002 | loss: 0.20736 - acc: 0.9308 -- iter: 14200/28000
Training Step: 845  | total loss: [1m[32m0.20941[0m[0m | time: 627.554s
| Adam | epoch: 002 | loss: 0.20941 - acc: 0.9237 -- iter: 14250/28000
Training Step: 846  | total loss: [1m[32m0.21097[0m[0m | time: 628.617s
| Adam | epoch: 002 | loss: 0.21097 - acc: 0.9233 -- iter: 14300/28000
Training Step: 847  | total loss: [1m[32m0.19950[0m[0m | time: 629.702s
| Adam | epoch: 002 | loss: 0.19950 - acc: 0.9290 -- iter: 14350/28000
Training Step: 848  | total loss: [1m[32m0.19575[0m[0m | time: 630.766s
| Adam | epoch: 002 | loss: 0.19575 - acc: 0.9321 -- iter: 14400/28000
Training Step: 849  | total loss: [1m[32m0.19633[0m[0m | time: 631.818s
| Adam | epoch: 002 | loss: 0.19633 - acc: 0.9329 -- iter: 14450/28000
Training Step: 850  | total loss: [1m[32m0.19370[0m[0m | time: 657.172s
| Adam | epoch: 002 | loss: 0.19370 - acc: 0.9316 | val_loss: 0.22156 - val_acc: 0.9197 -- iter: 14500/28000
--
Training Step: 851  | total loss: [1m[32m0.18003[0m[0m | time: 658.292s
| Adam | epoch: 002 | loss: 0.18003 - acc: 0.9384 -- iter: 14550/28000
Training Step: 852  | total loss: [1m[32m0.19339[0m[0m | time: 659.371s
| Adam | epoch: 002 | loss: 0.19339 - acc: 0.9366 -- iter: 14600/28000
Training Step: 853  | total loss: [1m[32m0.20021[0m[0m | time: 660.446s
| Adam | epoch: 002 | loss: 0.20021 - acc: 0.9309 -- iter: 14650/28000
Training Step: 854  | total loss: [1m[32m0.20124[0m[0m | time: 661.537s
| Adam | epoch: 002 | loss: 0.20124 - acc: 0.9278 -- iter: 14700/28000
Training Step: 855  | total loss: [1m[32m0.20388[0m[0m | time: 662.615s
| Adam | epoch: 002 | loss: 0.20388 - acc: 0.9231 -- iter: 14750/28000
Training Step: 856  | total loss: [1m[32m0.20216[0m[0m | time: 663.675s
| Adam | epoch: 002 | loss: 0.20216 - acc: 0.9207 -- iter: 14800/28000
Training Step: 857  | total loss: [1m[32m0.22349[0m[0m | time: 664.734s
| Adam | epoch: 002 | loss: 0.22349 - acc: 0.9187 -- iter: 14850/28000
Training Step: 858  | total loss: [1m[32m0.20862[0m[0m | time: 665.819s
| Adam | epoch: 002 | loss: 0.20862 - acc: 0.9248 -- iter: 14900/28000
Training Step: 859  | total loss: [1m[32m0.19587[0m[0m | time: 666.881s
| Adam | epoch: 002 | loss: 0.19587 - acc: 0.9303 -- iter: 14950/28000
Training Step: 860  | total loss: [1m[32m0.18989[0m[0m | time: 667.928s
| Adam | epoch: 002 | loss: 0.18989 - acc: 0.9333 -- iter: 15000/28000
Training Step: 861  | total loss: [1m[32m0.20096[0m[0m | time: 668.994s
| Adam | epoch: 002 | loss: 0.20096 - acc: 0.9320 -- iter: 15050/28000
Training Step: 862  | total loss: [1m[32m0.21150[0m[0m | time: 670.070s
| Adam | epoch: 002 | loss: 0.21150 - acc: 0.9308 -- iter: 15100/28000
Training Step: 863  | total loss: [1m[32m0.22720[0m[0m | time: 671.143s
| Adam | epoch: 002 | loss: 0.22720 - acc: 0.9277 -- iter: 15150/28000
Training Step: 864  | total loss: [1m[32m0.24147[0m[0m | time: 672.236s
| Adam | epoch: 002 | loss: 0.24147 - acc: 0.9229 -- iter: 15200/28000
Training Step: 865  | total loss: [1m[32m0.23117[0m[0m | time: 673.301s
| Adam | epoch: 002 | loss: 0.23117 - acc: 0.9266 -- iter: 15250/28000
Training Step: 866  | total loss: [1m[32m0.22070[0m[0m | time: 674.382s
| Adam | epoch: 002 | loss: 0.22070 - acc: 0.9300 -- iter: 15300/28000
Training Step: 867  | total loss: [1m[32m0.21896[0m[0m | time: 675.459s
| Adam | epoch: 002 | loss: 0.21896 - acc: 0.9250 -- iter: 15350/28000
Training Step: 868  | total loss: [1m[32m0.22216[0m[0m | time: 676.543s
| Adam | epoch: 002 | loss: 0.22216 - acc: 0.9205 -- iter: 15400/28000
Training Step: 869  | total loss: [1m[32m0.21582[0m[0m | time: 677.603s
| Adam | epoch: 002 | loss: 0.21582 - acc: 0.9264 -- iter: 15450/28000
Training Step: 870  | total loss: [1m[32m0.20764[0m[0m | time: 678.655s
| Adam | epoch: 002 | loss: 0.20764 - acc: 0.9278 -- iter: 15500/28000
Training Step: 871  | total loss: [1m[32m0.19381[0m[0m | time: 679.705s
| Adam | epoch: 002 | loss: 0.19381 - acc: 0.9330 -- iter: 15550/28000
Training Step: 872  | total loss: [1m[32m0.18265[0m[0m | time: 680.781s
| Adam | epoch: 002 | loss: 0.18265 - acc: 0.9397 -- iter: 15600/28000
Training Step: 873  | total loss: [1m[32m0.19854[0m[0m | time: 681.837s
| Adam | epoch: 002 | loss: 0.19854 - acc: 0.9357 -- iter: 15650/28000
Training Step: 874  | total loss: [1m[32m0.19865[0m[0m | time: 682.924s
| Adam | epoch: 002 | loss: 0.19865 - acc: 0.9322 -- iter: 15700/28000
Training Step: 875  | total loss: [1m[32m0.18693[0m[0m | time: 710.795s
| Adam | epoch: 002 | loss: 0.18693 - acc: 0.9369 | val_loss: 0.29245 - val_acc: 0.8905 -- iter: 15750/28000
--
Training Step: 876  | total loss: [1m[32m0.18693[0m[0m | time: 712.587s
| Adam | epoch: 002 | loss: 0.18693 - acc: 0.9372 -- iter: 15800/28000
Training Step: 877  | total loss: [1m[32m0.20004[0m[0m | time: 714.094s
| Adam | epoch: 002 | loss: 0.20004 - acc: 0.9335 -- iter: 15850/28000
Training Step: 878  | total loss: [1m[32m0.20019[0m[0m | time: 715.556s
| Adam | epoch: 002 | loss: 0.20019 - acc: 0.9342 -- iter: 15900/28000
Training Step: 879  | total loss: [1m[32m0.18978[0m[0m | time: 717.298s
| Adam | epoch: 002 | loss: 0.18978 - acc: 0.9388 -- iter: 15950/28000
Training Step: 880  | total loss: [1m[32m0.18421[0m[0m | time: 718.963s
| Adam | epoch: 002 | loss: 0.18421 - acc: 0.9389 -- iter: 16000/28000
Training Step: 881  | total loss: [1m[32m0.17638[0m[0m | time: 720.679s
| Adam | epoch: 002 | loss: 0.17638 - acc: 0.9410 -- iter: 16050/28000
Training Step: 882  | total loss: [1m[32m0.19815[0m[0m | time: 722.037s
| Adam | epoch: 002 | loss: 0.19815 - acc: 0.9369 -- iter: 16100/28000
Training Step: 883  | total loss: [1m[32m0.21845[0m[0m | time: 723.698s
| Adam | epoch: 002 | loss: 0.21845 - acc: 0.9332 -- iter: 16150/28000
Training Step: 884  | total loss: [1m[32m0.22343[0m[0m | time: 725.439s
| Adam | epoch: 002 | loss: 0.22343 - acc: 0.9279 -- iter: 16200/28000
Training Step: 885  | total loss: [1m[32m0.21411[0m[0m | time: 727.033s
| Adam | epoch: 002 | loss: 0.21411 - acc: 0.9291 -- iter: 16250/28000
Training Step: 886  | total loss: [1m[32m0.21946[0m[0m | time: 728.665s
| Adam | epoch: 002 | loss: 0.21946 - acc: 0.9202 -- iter: 16300/28000
Training Step: 887  | total loss: [1m[32m0.21519[0m[0m | time: 730.328s
| Adam | epoch: 002 | loss: 0.21519 - acc: 0.9242 -- iter: 16350/28000
Training Step: 888  | total loss: [1m[32m0.22279[0m[0m | time: 732.251s
| Adam | epoch: 002 | loss: 0.22279 - acc: 0.9237 -- iter: 16400/28000
Training Step: 889  | total loss: [1m[32m0.21732[0m[0m | time: 734.001s
| Adam | epoch: 002 | loss: 0.21732 - acc: 0.9254 -- iter: 16450/28000
Training Step: 890  | total loss: [1m[32m0.21709[0m[0m | time: 735.702s
| Adam | epoch: 002 | loss: 0.21709 - acc: 0.9248 -- iter: 16500/28000
Training Step: 891  | total loss: [1m[32m0.22083[0m[0m | time: 737.465s
| Adam | epoch: 002 | loss: 0.22083 - acc: 0.9224 -- iter: 16550/28000
Training Step: 892  | total loss: [1m[32m0.21484[0m[0m | time: 738.981s
| Adam | epoch: 002 | loss: 0.21484 - acc: 0.9261 -- iter: 16600/28000
Training Step: 893  | total loss: [1m[32m0.21465[0m[0m | time: 740.311s
| Adam | epoch: 002 | loss: 0.21465 - acc: 0.9255 -- iter: 16650/28000
Training Step: 894  | total loss: [1m[32m0.20883[0m[0m | time: 741.598s
| Adam | epoch: 002 | loss: 0.20883 - acc: 0.9290 -- iter: 16700/28000
Training Step: 895  | total loss: [1m[32m0.20462[0m[0m | time: 742.906s
| Adam | epoch: 002 | loss: 0.20462 - acc: 0.9281 -- iter: 16750/28000
Training Step: 896  | total loss: [1m[32m0.21109[0m[0m | time: 744.274s
| Adam | epoch: 002 | loss: 0.21109 - acc: 0.9253 -- iter: 16800/28000
Training Step: 897  | total loss: [1m[32m0.20179[0m[0m | time: 745.571s
| Adam | epoch: 002 | loss: 0.20179 - acc: 0.9287 -- iter: 16850/28000
Training Step: 898  | total loss: [1m[32m0.19211[0m[0m | time: 746.826s
| Adam | epoch: 002 | loss: 0.19211 - acc: 0.9319 -- iter: 16900/28000
Training Step: 899  | total loss: [1m[32m0.19384[0m[0m | time: 748.161s
| Adam | epoch: 002 | loss: 0.19384 - acc: 0.9287 -- iter: 16950/28000
Training Step: 900  | total loss: [1m[32m0.18773[0m[0m | time: 779.698s
| Adam | epoch: 002 | loss: 0.18773 - acc: 0.9338 | val_loss: 0.21248 - val_acc: 0.9222 -- iter: 17000/28000
--
Training Step: 901  | total loss: [1m[32m0.18748[0m[0m | time: 781.025s
| Adam | epoch: 002 | loss: 0.18748 - acc: 0.9304 -- iter: 17050/28000
Training Step: 902  | total loss: [1m[32m0.19360[0m[0m | time: 782.298s
| Adam | epoch: 002 | loss: 0.19360 - acc: 0.9294 -- iter: 17100/28000
Training Step: 903  | total loss: [1m[32m0.18884[0m[0m | time: 783.559s
| Adam | epoch: 002 | loss: 0.18884 - acc: 0.9304 -- iter: 17150/28000
Training Step: 904  | total loss: [1m[32m0.18693[0m[0m | time: 784.879s
| Adam | epoch: 002 | loss: 0.18693 - acc: 0.9334 -- iter: 17200/28000
Training Step: 905  | total loss: [1m[32m0.17604[0m[0m | time: 786.604s
| Adam | epoch: 002 | loss: 0.17604 - acc: 0.9381 -- iter: 17250/28000
Training Step: 906  | total loss: [1m[32m0.17199[0m[0m | time: 788.177s
| Adam | epoch: 002 | loss: 0.17199 - acc: 0.9383 -- iter: 17300/28000
Training Step: 907  | total loss: [1m[32m0.17384[0m[0m | time: 789.962s
| Adam | epoch: 002 | loss: 0.17384 - acc: 0.9384 -- iter: 17350/28000
Training Step: 908  | total loss: [1m[32m0.17177[0m[0m | time: 791.433s
| Adam | epoch: 002 | loss: 0.17177 - acc: 0.9386 -- iter: 17400/28000
Training Step: 909  | total loss: [1m[32m0.19544[0m[0m | time: 792.702s
| Adam | epoch: 002 | loss: 0.19544 - acc: 0.9347 -- iter: 17450/28000
Training Step: 910  | total loss: [1m[32m0.19040[0m[0m | time: 793.978s
| Adam | epoch: 002 | loss: 0.19040 - acc: 0.9333 -- iter: 17500/28000
Training Step: 911  | total loss: [1m[32m0.19081[0m[0m | time: 795.238s
| Adam | epoch: 002 | loss: 0.19081 - acc: 0.9299 -- iter: 17550/28000
Training Step: 912  | total loss: [1m[32m0.18925[0m[0m | time: 796.535s
| Adam | epoch: 002 | loss: 0.18925 - acc: 0.9309 -- iter: 17600/28000
Training Step: 913  | total loss: [1m[32m0.18667[0m[0m | time: 797.831s
| Adam | epoch: 002 | loss: 0.18667 - acc: 0.9318 -- iter: 17650/28000
Training Step: 914  | total loss: [1m[32m0.17876[0m[0m | time: 799.116s
| Adam | epoch: 002 | loss: 0.17876 - acc: 0.9347 -- iter: 17700/28000
Training Step: 915  | total loss: [1m[32m0.17966[0m[0m | time: 800.366s
| Adam | epoch: 002 | loss: 0.17966 - acc: 0.9352 -- iter: 17750/28000
Training Step: 916  | total loss: [1m[32m0.19804[0m[0m | time: 801.650s
| Adam | epoch: 002 | loss: 0.19804 - acc: 0.9257 -- iter: 17800/28000
Training Step: 917  | total loss: [1m[32m0.20014[0m[0m | time: 802.919s
| Adam | epoch: 002 | loss: 0.20014 - acc: 0.9251 -- iter: 17850/28000
Training Step: 918  | total loss: [1m[32m0.19458[0m[0m | time: 804.193s
| Adam | epoch: 002 | loss: 0.19458 - acc: 0.9266 -- iter: 17900/28000
Training Step: 919  | total loss: [1m[32m0.19877[0m[0m | time: 805.459s
| Adam | epoch: 002 | loss: 0.19877 - acc: 0.9259 -- iter: 17950/28000
Training Step: 920  | total loss: [1m[32m0.21533[0m[0m | time: 806.747s
| Adam | epoch: 002 | loss: 0.21533 - acc: 0.9193 -- iter: 18000/28000
Training Step: 921  | total loss: [1m[32m0.22234[0m[0m | time: 808.042s
| Adam | epoch: 002 | loss: 0.22234 - acc: 0.9154 -- iter: 18050/28000
Training Step: 922  | total loss: [1m[32m0.20931[0m[0m | time: 809.319s
| Adam | epoch: 002 | loss: 0.20931 - acc: 0.9199 -- iter: 18100/28000
Training Step: 923  | total loss: [1m[32m0.20706[0m[0m | time: 810.615s
| Adam | epoch: 002 | loss: 0.20706 - acc: 0.9199 -- iter: 18150/28000
Training Step: 924  | total loss: [1m[32m0.20108[0m[0m | time: 811.874s
| Adam | epoch: 002 | loss: 0.20108 - acc: 0.9199 -- iter: 18200/28000
Training Step: 925  | total loss: [1m[32m0.18928[0m[0m | time: 841.818s
| Adam | epoch: 002 | loss: 0.18928 - acc: 0.9279 | val_loss: 0.22628 - val_acc: 0.9136 -- iter: 18250/28000
--
Training Step: 926  | total loss: [1m[32m0.19231[0m[0m | time: 843.140s
| Adam | epoch: 002 | loss: 0.19231 - acc: 0.9271 -- iter: 18300/28000
Training Step: 927  | total loss: [1m[32m0.19436[0m[0m | time: 844.441s
| Adam | epoch: 002 | loss: 0.19436 - acc: 0.9244 -- iter: 18350/28000
Training Step: 928  | total loss: [1m[32m0.20816[0m[0m | time: 845.784s
| Adam | epoch: 002 | loss: 0.20816 - acc: 0.9200 -- iter: 18400/28000
Training Step: 929  | total loss: [1m[32m0.20095[0m[0m | time: 847.149s
| Adam | epoch: 002 | loss: 0.20095 - acc: 0.9200 -- iter: 18450/28000
Training Step: 930  | total loss: [1m[32m0.20485[0m[0m | time: 848.535s
| Adam | epoch: 002 | loss: 0.20485 - acc: 0.9180 -- iter: 18500/28000
Training Step: 931  | total loss: [1m[32m0.20331[0m[0m | time: 850.142s
| Adam | epoch: 002 | loss: 0.20331 - acc: 0.9202 -- iter: 18550/28000
Training Step: 932  | total loss: [1m[32m0.21243[0m[0m | time: 851.538s
| Adam | epoch: 002 | loss: 0.21243 - acc: 0.9202 -- iter: 18600/28000
Training Step: 933  | total loss: [1m[32m0.20174[0m[0m | time: 852.858s
| Adam | epoch: 002 | loss: 0.20174 - acc: 0.9261 -- iter: 18650/28000
Training Step: 934  | total loss: [1m[32m0.19506[0m[0m | time: 854.176s
| Adam | epoch: 002 | loss: 0.19506 - acc: 0.9295 -- iter: 18700/28000
Training Step: 935  | total loss: [1m[32m0.19475[0m[0m | time: 855.529s
| Adam | epoch: 002 | loss: 0.19475 - acc: 0.9306 -- iter: 18750/28000
Training Step: 936  | total loss: [1m[32m0.19884[0m[0m | time: 856.974s
| Adam | epoch: 002 | loss: 0.19884 - acc: 0.9315 -- iter: 18800/28000
Training Step: 937  | total loss: [1m[32m0.18983[0m[0m | time: 858.296s
| Adam | epoch: 002 | loss: 0.18983 - acc: 0.9364 -- iter: 18850/28000
Training Step: 938  | total loss: [1m[32m0.18878[0m[0m | time: 859.627s
| Adam | epoch: 002 | loss: 0.18878 - acc: 0.9347 -- iter: 18900/28000
Training Step: 939  | total loss: [1m[32m0.19465[0m[0m | time: 860.910s
| Adam | epoch: 002 | loss: 0.19465 - acc: 0.9353 -- iter: 18950/28000
Training Step: 940  | total loss: [1m[32m0.19992[0m[0m | time: 862.179s
| Adam | epoch: 002 | loss: 0.19992 - acc: 0.9297 -- iter: 19000/28000
Training Step: 941  | total loss: [1m[32m0.19361[0m[0m | time: 863.489s
| Adam | epoch: 002 | loss: 0.19361 - acc: 0.9308 -- iter: 19050/28000
Training Step: 942  | total loss: [1m[32m0.18075[0m[0m | time: 864.831s
| Adam | epoch: 002 | loss: 0.18075 - acc: 0.9377 -- iter: 19100/28000
Training Step: 943  | total loss: [1m[32m0.18064[0m[0m | time: 866.112s
| Adam | epoch: 002 | loss: 0.18064 - acc: 0.9399 -- iter: 19150/28000
Training Step: 944  | total loss: [1m[32m0.19041[0m[0m | time: 867.400s
| Adam | epoch: 002 | loss: 0.19041 - acc: 0.9359 -- iter: 19200/28000
Training Step: 945  | total loss: [1m[32m0.19573[0m[0m | time: 868.663s
| Adam | epoch: 002 | loss: 0.19573 - acc: 0.9323 -- iter: 19250/28000
Training Step: 946  | total loss: [1m[32m0.20072[0m[0m | time: 869.958s
| Adam | epoch: 002 | loss: 0.20072 - acc: 0.9311 -- iter: 19300/28000
Training Step: 947  | total loss: [1m[32m0.19968[0m[0m | time: 871.238s
| Adam | epoch: 002 | loss: 0.19968 - acc: 0.9320 -- iter: 19350/28000
Training Step: 948  | total loss: [1m[32m0.19167[0m[0m | time: 872.531s
| Adam | epoch: 002 | loss: 0.19167 - acc: 0.9368 -- iter: 19400/28000
Training Step: 949  | total loss: [1m[32m0.18900[0m[0m | time: 873.823s
| Adam | epoch: 002 | loss: 0.18900 - acc: 0.9351 -- iter: 19450/28000
Training Step: 950  | total loss: [1m[32m0.19026[0m[0m | time: 903.880s
| Adam | epoch: 002 | loss: 0.19026 - acc: 0.9316 | val_loss: 0.21178 - val_acc: 0.9230 -- iter: 19500/28000
--
Training Step: 951  | total loss: [1m[32m0.19103[0m[0m | time: 905.218s
| Adam | epoch: 002 | loss: 0.19103 - acc: 0.9304 -- iter: 19550/28000
Training Step: 952  | total loss: [1m[32m0.20316[0m[0m | time: 906.470s
| Adam | epoch: 002 | loss: 0.20316 - acc: 0.9254 -- iter: 19600/28000
Training Step: 953  | total loss: [1m[32m0.19660[0m[0m | time: 907.777s
| Adam | epoch: 002 | loss: 0.19660 - acc: 0.9269 -- iter: 19650/28000
Training Step: 954  | total loss: [1m[32m0.19050[0m[0m | time: 909.050s
| Adam | epoch: 002 | loss: 0.19050 - acc: 0.9282 -- iter: 19700/28000
Training Step: 955  | total loss: [1m[32m0.18876[0m[0m | time: 910.336s
| Adam | epoch: 002 | loss: 0.18876 - acc: 0.9274 -- iter: 19750/28000
Training Step: 956  | total loss: [1m[32m0.19733[0m[0m | time: 911.661s
| Adam | epoch: 002 | loss: 0.19733 - acc: 0.9286 -- iter: 19800/28000
Training Step: 957  | total loss: [1m[32m0.19506[0m[0m | time: 912.915s
| Adam | epoch: 002 | loss: 0.19506 - acc: 0.9278 -- iter: 19850/28000
Training Step: 958  | total loss: [1m[32m0.19454[0m[0m | time: 914.182s
| Adam | epoch: 002 | loss: 0.19454 - acc: 0.9290 -- iter: 19900/28000
Training Step: 959  | total loss: [1m[32m0.19837[0m[0m | time: 915.463s
| Adam | epoch: 002 | loss: 0.19837 - acc: 0.9321 -- iter: 19950/28000
Training Step: 960  | total loss: [1m[32m0.18414[0m[0m | time: 916.756s
| Adam | epoch: 002 | loss: 0.18414 - acc: 0.9369 -- iter: 20000/28000
Training Step: 961  | total loss: [1m[32m0.17477[0m[0m | time: 918.029s
| Adam | epoch: 002 | loss: 0.17477 - acc: 0.9392 -- iter: 20050/28000
Training Step: 962  | total loss: [1m[32m0.17399[0m[0m | time: 919.303s
| Adam | epoch: 002 | loss: 0.17399 - acc: 0.9373 -- iter: 20100/28000
Training Step: 963  | total loss: [1m[32m0.17502[0m[0m | time: 920.611s
| Adam | epoch: 002 | loss: 0.17502 - acc: 0.9355 -- iter: 20150/28000
Training Step: 964  | total loss: [1m[32m0.16542[0m[0m | time: 921.878s
| Adam | epoch: 002 | loss: 0.16542 - acc: 0.9400 -- iter: 20200/28000
Training Step: 965  | total loss: [1m[32m0.16993[0m[0m | time: 923.154s
| Adam | epoch: 002 | loss: 0.16993 - acc: 0.9380 -- iter: 20250/28000
Training Step: 966  | total loss: [1m[32m0.16791[0m[0m | time: 924.430s
| Adam | epoch: 002 | loss: 0.16791 - acc: 0.9382 -- iter: 20300/28000
Training Step: 967  | total loss: [1m[32m0.16647[0m[0m | time: 925.687s
| Adam | epoch: 002 | loss: 0.16647 - acc: 0.9364 -- iter: 20350/28000
Training Step: 968  | total loss: [1m[32m0.15971[0m[0m | time: 926.946s
| Adam | epoch: 002 | loss: 0.15971 - acc: 0.9367 -- iter: 20400/28000
Training Step: 969  | total loss: [1m[32m0.15537[0m[0m | time: 928.212s
| Adam | epoch: 002 | loss: 0.15537 - acc: 0.9391 -- iter: 20450/28000
Training Step: 970  | total loss: [1m[32m0.17066[0m[0m | time: 929.515s
| Adam | epoch: 002 | loss: 0.17066 - acc: 0.9352 -- iter: 20500/28000
Training Step: 971  | total loss: [1m[32m0.17122[0m[0m | time: 930.804s
| Adam | epoch: 002 | loss: 0.17122 - acc: 0.9336 -- iter: 20550/28000
Training Step: 972  | total loss: [1m[32m0.18171[0m[0m | time: 932.066s
| Adam | epoch: 002 | loss: 0.18171 - acc: 0.9283 -- iter: 20600/28000
Training Step: 973  | total loss: [1m[32m0.17636[0m[0m | time: 933.335s
| Adam | epoch: 002 | loss: 0.17636 - acc: 0.9314 -- iter: 20650/28000
Training Step: 974  | total loss: [1m[32m0.17995[0m[0m | time: 934.600s
| Adam | epoch: 002 | loss: 0.17995 - acc: 0.9263 -- iter: 20700/28000
Training Step: 975  | total loss: [1m[32m0.17959[0m[0m | time: 964.825s
| Adam | epoch: 002 | loss: 0.17959 - acc: 0.9257 | val_loss: 0.20409 - val_acc: 0.9258 -- iter: 20750/28000
--
Training Step: 976  | total loss: [1m[32m0.17069[0m[0m | time: 966.133s
| Adam | epoch: 002 | loss: 0.17069 - acc: 0.9331 -- iter: 20800/28000
Training Step: 977  | total loss: [1m[32m0.16732[0m[0m | time: 967.432s
| Adam | epoch: 002 | loss: 0.16732 - acc: 0.9358 -- iter: 20850/28000
Training Step: 978  | total loss: [1m[32m0.17519[0m[0m | time: 968.712s
| Adam | epoch: 002 | loss: 0.17519 - acc: 0.9362 -- iter: 20900/28000
Training Step: 979  | total loss: [1m[32m0.16939[0m[0m | time: 969.972s
| Adam | epoch: 002 | loss: 0.16939 - acc: 0.9366 -- iter: 20950/28000
Training Step: 980  | total loss: [1m[32m0.16739[0m[0m | time: 971.270s
| Adam | epoch: 002 | loss: 0.16739 - acc: 0.9389 -- iter: 21000/28000
Training Step: 981  | total loss: [1m[32m0.18083[0m[0m | time: 972.552s
| Adam | epoch: 002 | loss: 0.18083 - acc: 0.9330 -- iter: 21050/28000
Training Step: 982  | total loss: [1m[32m0.17464[0m[0m | time: 973.821s
| Adam | epoch: 002 | loss: 0.17464 - acc: 0.9337 -- iter: 21100/28000
Training Step: 983  | total loss: [1m[32m0.16617[0m[0m | time: 975.105s
| Adam | epoch: 002 | loss: 0.16617 - acc: 0.9364 -- iter: 21150/28000
Training Step: 984  | total loss: [1m[32m0.16757[0m[0m | time: 976.381s
| Adam | epoch: 002 | loss: 0.16757 - acc: 0.9367 -- iter: 21200/28000
Training Step: 985  | total loss: [1m[32m0.16712[0m[0m | time: 977.646s
| Adam | epoch: 002 | loss: 0.16712 - acc: 0.9391 -- iter: 21250/28000
Training Step: 986  | total loss: [1m[32m0.16194[0m[0m | time: 978.910s
| Adam | epoch: 002 | loss: 0.16194 - acc: 0.9411 -- iter: 21300/28000
Training Step: 987  | total loss: [1m[32m0.16939[0m[0m | time: 980.222s
| Adam | epoch: 002 | loss: 0.16939 - acc: 0.9410 -- iter: 21350/28000
Training Step: 988  | total loss: [1m[32m0.19039[0m[0m | time: 981.510s
| Adam | epoch: 002 | loss: 0.19039 - acc: 0.9329 -- iter: 21400/28000
Training Step: 989  | total loss: [1m[32m0.20043[0m[0m | time: 982.771s
| Adam | epoch: 002 | loss: 0.20043 - acc: 0.9316 -- iter: 21450/28000
Training Step: 990  | total loss: [1m[32m0.20347[0m[0m | time: 984.049s
| Adam | epoch: 002 | loss: 0.20347 - acc: 0.9325 -- iter: 21500/28000
Training Step: 991  | total loss: [1m[32m0.20270[0m[0m | time: 985.317s
| Adam | epoch: 002 | loss: 0.20270 - acc: 0.9292 -- iter: 21550/28000
Training Step: 992  | total loss: [1m[32m0.19777[0m[0m | time: 986.612s
| Adam | epoch: 002 | loss: 0.19777 - acc: 0.9303 -- iter: 21600/28000
Training Step: 993  | total loss: [1m[32m0.20340[0m[0m | time: 987.873s
| Adam | epoch: 002 | loss: 0.20340 - acc: 0.9213 -- iter: 21650/28000
Training Step: 994  | total loss: [1m[32m0.19781[0m[0m | time: 989.183s
| Adam | epoch: 002 | loss: 0.19781 - acc: 0.9251 -- iter: 21700/28000
Training Step: 995  | total loss: [1m[32m0.19899[0m[0m | time: 990.537s
| Adam | epoch: 002 | loss: 0.19899 - acc: 0.9266 -- iter: 21750/28000
Training Step: 996  | total loss: [1m[32m0.21488[0m[0m | time: 991.836s
| Adam | epoch: 002 | loss: 0.21488 - acc: 0.9200 -- iter: 21800/28000
Training Step: 997  | total loss: [1m[32m0.21109[0m[0m | time: 993.113s
| Adam | epoch: 002 | loss: 0.21109 - acc: 0.9220 -- iter: 21850/28000
Training Step: 998  | total loss: [1m[32m0.21552[0m[0m | time: 994.387s
| Adam | epoch: 002 | loss: 0.21552 - acc: 0.9238 -- iter: 21900/28000
Training Step: 999  | total loss: [1m[32m0.21230[0m[0m | time: 995.666s
| Adam | epoch: 002 | loss: 0.21230 - acc: 0.9214 -- iter: 21950/28000
Training Step: 1000  | total loss: [1m[32m0.21123[0m[0m | time: 1025.798s
| Adam | epoch: 002 | loss: 0.21123 - acc: 0.9233 | val_loss: 0.21896 - val_acc: 0.9194 -- iter: 22000/28000
--
Training Step: 1001  | total loss: [1m[32m0.20445[0m[0m | time: 1027.087s
| Adam | epoch: 002 | loss: 0.20445 - acc: 0.9249 -- iter: 22050/28000
Training Step: 1002  | total loss: [1m[32m0.19689[0m[0m | time: 1028.400s
| Adam | epoch: 002 | loss: 0.19689 - acc: 0.9284 -- iter: 22100/28000
Training Step: 1003  | total loss: [1m[32m0.18628[0m[0m | time: 1029.667s
| Adam | epoch: 002 | loss: 0.18628 - acc: 0.9316 -- iter: 22150/28000
Training Step: 1004  | total loss: [1m[32m0.18958[0m[0m | time: 1030.949s
| Adam | epoch: 002 | loss: 0.18958 - acc: 0.9284 -- iter: 22200/28000
Training Step: 1005  | total loss: [1m[32m0.18676[0m[0m | time: 1032.256s
| Adam | epoch: 002 | loss: 0.18676 - acc: 0.9296 -- iter: 22250/28000
Training Step: 1006  | total loss: [1m[32m0.18206[0m[0m | time: 1033.536s
| Adam | epoch: 002 | loss: 0.18206 - acc: 0.9326 -- iter: 22300/28000
Training Step: 1007  | total loss: [1m[32m0.18637[0m[0m | time: 1034.823s
| Adam | epoch: 002 | loss: 0.18637 - acc: 0.9314 -- iter: 22350/28000
Training Step: 1008  | total loss: [1m[32m0.19578[0m[0m | time: 1036.102s
| Adam | epoch: 002 | loss: 0.19578 - acc: 0.9282 -- iter: 22400/28000
Training Step: 1009  | total loss: [1m[32m0.19217[0m[0m | time: 1037.380s
| Adam | epoch: 002 | loss: 0.19217 - acc: 0.9314 -- iter: 22450/28000
Training Step: 1010  | total loss: [1m[32m0.18375[0m[0m | time: 1038.640s
| Adam | epoch: 002 | loss: 0.18375 - acc: 0.9343 -- iter: 22500/28000
Training Step: 1011  | total loss: [1m[32m0.18413[0m[0m | time: 1039.915s
| Adam | epoch: 002 | loss: 0.18413 - acc: 0.9348 -- iter: 22550/28000
Training Step: 1012  | total loss: [1m[32m0.18376[0m[0m | time: 1041.179s
| Adam | epoch: 002 | loss: 0.18376 - acc: 0.9334 -- iter: 22600/28000
Training Step: 1013  | total loss: [1m[32m0.17411[0m[0m | time: 1042.490s
| Adam | epoch: 002 | loss: 0.17411 - acc: 0.9400 -- iter: 22650/28000
Training Step: 1014  | total loss: [1m[32m0.17761[0m[0m | time: 1043.843s
| Adam | epoch: 002 | loss: 0.17761 - acc: 0.9400 -- iter: 22700/28000
Training Step: 1015  | total loss: [1m[32m0.18944[0m[0m | time: 1045.156s
| Adam | epoch: 002 | loss: 0.18944 - acc: 0.9360 -- iter: 22750/28000
Training Step: 1016  | total loss: [1m[32m0.19457[0m[0m | time: 1046.437s
| Adam | epoch: 002 | loss: 0.19457 - acc: 0.9344 -- iter: 22800/28000
Training Step: 1017  | total loss: [1m[32m0.19398[0m[0m | time: 1047.712s
| Adam | epoch: 002 | loss: 0.19398 - acc: 0.9310 -- iter: 22850/28000
Training Step: 1018  | total loss: [1m[32m0.18808[0m[0m | time: 1048.959s
| Adam | epoch: 002 | loss: 0.18808 - acc: 0.9339 -- iter: 22900/28000
Training Step: 1019  | total loss: [1m[32m0.18886[0m[0m | time: 1050.237s
| Adam | epoch: 002 | loss: 0.18886 - acc: 0.9265 -- iter: 22950/28000
Training Step: 1020  | total loss: [1m[32m0.18130[0m[0m | time: 1051.543s
| Adam | epoch: 002 | loss: 0.18130 - acc: 0.9298 -- iter: 23000/28000
Training Step: 1021  | total loss: [1m[32m0.19257[0m[0m | time: 1052.808s
| Adam | epoch: 002 | loss: 0.19257 - acc: 0.9269 -- iter: 23050/28000
Training Step: 1022  | total loss: [1m[32m0.18542[0m[0m | time: 1054.092s
| Adam | epoch: 002 | loss: 0.18542 - acc: 0.9302 -- iter: 23100/28000
Training Step: 1023  | total loss: [1m[32m0.18237[0m[0m | time: 1055.381s
| Adam | epoch: 002 | loss: 0.18237 - acc: 0.9312 -- iter: 23150/28000
Training Step: 1024  | total loss: [1m[32m0.17613[0m[0m | time: 1056.690s
| Adam | epoch: 002 | loss: 0.17613 - acc: 0.9320 -- iter: 23200/28000
Training Step: 1025  | total loss: [1m[32m0.17339[0m[0m | time: 1086.785s
| Adam | epoch: 002 | loss: 0.17339 - acc: 0.9308 | val_loss: 0.21931 - val_acc: 0.9145 -- iter: 23250/28000
--
Training Step: 1026  | total loss: [1m[32m0.16553[0m[0m | time: 1088.154s
| Adam | epoch: 002 | loss: 0.16553 - acc: 0.9338 -- iter: 23300/28000
Training Step: 1027  | total loss: [1m[32m0.17818[0m[0m | time: 1089.463s
| Adam | epoch: 002 | loss: 0.17818 - acc: 0.9324 -- iter: 23350/28000
Training Step: 1028  | total loss: [1m[32m0.17629[0m[0m | time: 1090.752s
| Adam | epoch: 002 | loss: 0.17629 - acc: 0.9311 -- iter: 23400/28000
Training Step: 1029  | total loss: [1m[32m0.16394[0m[0m | time: 1092.019s
| Adam | epoch: 002 | loss: 0.16394 - acc: 0.9380 -- iter: 23450/28000
Training Step: 1030  | total loss: [1m[32m0.16290[0m[0m | time: 1093.285s
| Adam | epoch: 002 | loss: 0.16290 - acc: 0.9402 -- iter: 23500/28000
Training Step: 1031  | total loss: [1m[32m0.17714[0m[0m | time: 1094.579s
| Adam | epoch: 002 | loss: 0.17714 - acc: 0.9362 -- iter: 23550/28000
Training Step: 1032  | total loss: [1m[32m0.17697[0m[0m | time: 1095.830s
| Adam | epoch: 002 | loss: 0.17697 - acc: 0.9346 -- iter: 23600/28000
Training Step: 1033  | total loss: [1m[32m0.16895[0m[0m | time: 1097.091s
| Adam | epoch: 002 | loss: 0.16895 - acc: 0.9391 -- iter: 23650/28000
Training Step: 1034  | total loss: [1m[32m0.16320[0m[0m | time: 1098.332s
| Adam | epoch: 002 | loss: 0.16320 - acc: 0.9392 -- iter: 23700/28000
Training Step: 1035  | total loss: [1m[32m0.16150[0m[0m | time: 1099.574s
| Adam | epoch: 002 | loss: 0.16150 - acc: 0.9373 -- iter: 23750/28000
Training Step: 1036  | total loss: [1m[32m0.16513[0m[0m | time: 1100.869s
| Adam | epoch: 002 | loss: 0.16513 - acc: 0.9356 -- iter: 23800/28000
Training Step: 1037  | total loss: [1m[32m0.17861[0m[0m | time: 1102.132s
| Adam | epoch: 002 | loss: 0.17861 - acc: 0.9340 -- iter: 23850/28000
Training Step: 1038  | total loss: [1m[32m0.16517[0m[0m | time: 1103.363s
| Adam | epoch: 002 | loss: 0.16517 - acc: 0.9406 -- iter: 23900/28000
Training Step: 1039  | total loss: [1m[32m0.17136[0m[0m | time: 1104.650s
| Adam | epoch: 002 | loss: 0.17136 - acc: 0.9425 -- iter: 23950/28000
Training Step: 1040  | total loss: [1m[32m0.16517[0m[0m | time: 1105.897s
| Adam | epoch: 002 | loss: 0.16517 - acc: 0.9443 -- iter: 24000/28000
Training Step: 1041  | total loss: [1m[32m0.16630[0m[0m | time: 1107.164s
| Adam | epoch: 002 | loss: 0.16630 - acc: 0.9419 -- iter: 24050/28000
Training Step: 1042  | total loss: [1m[32m0.16785[0m[0m | time: 1108.447s
| Adam | epoch: 002 | loss: 0.16785 - acc: 0.9357 -- iter: 24100/28000
Training Step: 1043  | total loss: [1m[32m0.17333[0m[0m | time: 1109.701s
| Adam | epoch: 002 | loss: 0.17333 - acc: 0.9361 -- iter: 24150/28000
Training Step: 1044  | total loss: [1m[32m0.16592[0m[0m | time: 1110.988s
| Adam | epoch: 002 | loss: 0.16592 - acc: 0.9405 -- iter: 24200/28000
Training Step: 1045  | total loss: [1m[32m0.17796[0m[0m | time: 1112.282s
| Adam | epoch: 002 | loss: 0.17796 - acc: 0.9384 -- iter: 24250/28000
Training Step: 1046  | total loss: [1m[32m0.19690[0m[0m | time: 1113.551s
| Adam | epoch: 002 | loss: 0.19690 - acc: 0.9366 -- iter: 24300/28000
Training Step: 1047  | total loss: [1m[32m0.20212[0m[0m | time: 1114.827s
| Adam | epoch: 002 | loss: 0.20212 - acc: 0.9349 -- iter: 24350/28000
Training Step: 1048  | total loss: [1m[32m0.21028[0m[0m | time: 1116.094s
| Adam | epoch: 002 | loss: 0.21028 - acc: 0.9334 -- iter: 24400/28000
Training Step: 1049  | total loss: [1m[32m0.22235[0m[0m | time: 1117.361s
| Adam | epoch: 002 | loss: 0.22235 - acc: 0.9301 -- iter: 24450/28000
Training Step: 1050  | total loss: [1m[32m0.21338[0m[0m | time: 1147.370s
| Adam | epoch: 002 | loss: 0.21338 - acc: 0.9351 | val_loss: 0.24972 - val_acc: 0.8996 -- iter: 24500/28000
--
Training Step: 1051  | total loss: [1m[32m0.20939[0m[0m | time: 1148.766s
| Adam | epoch: 002 | loss: 0.20939 - acc: 0.9376 -- iter: 24550/28000
Training Step: 1052  | total loss: [1m[32m0.20728[0m[0m | time: 1150.108s
| Adam | epoch: 002 | loss: 0.20728 - acc: 0.9378 -- iter: 24600/28000
Training Step: 1053  | total loss: [1m[32m0.20654[0m[0m | time: 1151.494s
| Adam | epoch: 002 | loss: 0.20654 - acc: 0.9340 -- iter: 24650/28000
Training Step: 1054  | total loss: [1m[32m0.20734[0m[0m | time: 1152.846s
| Adam | epoch: 002 | loss: 0.20734 - acc: 0.9306 -- iter: 24700/28000
Training Step: 1055  | total loss: [1m[32m0.20096[0m[0m | time: 1154.153s
| Adam | epoch: 002 | loss: 0.20096 - acc: 0.9336 -- iter: 24750/28000
Training Step: 1056  | total loss: [1m[32m0.19719[0m[0m | time: 1155.461s
| Adam | epoch: 002 | loss: 0.19719 - acc: 0.9342 -- iter: 24800/28000
Training Step: 1057  | total loss: [1m[32m0.20048[0m[0m | time: 1156.807s
| Adam | epoch: 002 | loss: 0.20048 - acc: 0.9328 -- iter: 24850/28000
Training Step: 1058  | total loss: [1m[32m0.20045[0m[0m | time: 1158.100s
| Adam | epoch: 002 | loss: 0.20045 - acc: 0.9335 -- iter: 24900/28000
Training Step: 1059  | total loss: [1m[32m0.19622[0m[0m | time: 1159.375s
| Adam | epoch: 002 | loss: 0.19622 - acc: 0.9342 -- iter: 24950/28000
Training Step: 1060  | total loss: [1m[32m0.20400[0m[0m | time: 1160.703s
| Adam | epoch: 002 | loss: 0.20400 - acc: 0.9307 -- iter: 25000/28000
Training Step: 1061  | total loss: [1m[32m0.21439[0m[0m | time: 1161.956s
| Adam | epoch: 002 | loss: 0.21439 - acc: 0.9257 -- iter: 25050/28000
Training Step: 1062  | total loss: [1m[32m0.21696[0m[0m | time: 1163.247s
| Adam | epoch: 002 | loss: 0.21696 - acc: 0.9211 -- iter: 25100/28000
Training Step: 1063  | total loss: [1m[32m0.20948[0m[0m | time: 1164.529s
| Adam | epoch: 002 | loss: 0.20948 - acc: 0.9250 -- iter: 25150/28000
Training Step: 1064  | total loss: [1m[32m0.19931[0m[0m | time: 1165.798s
| Adam | epoch: 002 | loss: 0.19931 - acc: 0.9285 -- iter: 25200/28000
Training Step: 1065  | total loss: [1m[32m0.19464[0m[0m | time: 1167.139s
| Adam | epoch: 002 | loss: 0.19464 - acc: 0.9316 -- iter: 25250/28000
Training Step: 1066  | total loss: [1m[32m0.18927[0m[0m | time: 1168.427s
| Adam | epoch: 002 | loss: 0.18927 - acc: 0.9305 -- iter: 25300/28000
Training Step: 1067  | total loss: [1m[32m0.20626[0m[0m | time: 1169.693s
| Adam | epoch: 002 | loss: 0.20626 - acc: 0.9234 -- iter: 25350/28000
Training Step: 1068  | total loss: [1m[32m0.21396[0m[0m | time: 1170.945s
| Adam | epoch: 002 | loss: 0.21396 - acc: 0.9231 -- iter: 25400/28000
Training Step: 1069  | total loss: [1m[32m0.21212[0m[0m | time: 1172.217s
| Adam | epoch: 002 | loss: 0.21212 - acc: 0.9228 -- iter: 25450/28000
Training Step: 1070  | total loss: [1m[32m0.20728[0m[0m | time: 1173.508s
| Adam | epoch: 002 | loss: 0.20728 - acc: 0.9225 -- iter: 25500/28000
Training Step: 1071  | total loss: [1m[32m0.20368[0m[0m | time: 1174.778s
| Adam | epoch: 002 | loss: 0.20368 - acc: 0.9223 -- iter: 25550/28000
Training Step: 1072  | total loss: [1m[32m0.19145[0m[0m | time: 1176.060s
| Adam | epoch: 002 | loss: 0.19145 - acc: 0.9280 -- iter: 25600/28000
Training Step: 1073  | total loss: [1m[32m0.19686[0m[0m | time: 1177.344s
| Adam | epoch: 002 | loss: 0.19686 - acc: 0.9272 -- iter: 25650/28000
Training Step: 1074  | total loss: [1m[32m0.20389[0m[0m | time: 1178.624s
| Adam | epoch: 002 | loss: 0.20389 - acc: 0.9245 -- iter: 25700/28000
Training Step: 1075  | total loss: [1m[32m0.19191[0m[0m | time: 1208.709s
| Adam | epoch: 002 | loss: 0.19191 - acc: 0.9301 | val_loss: 0.20744 - val_acc: 0.9254 -- iter: 25750/28000
--
Training Step: 1076  | total loss: [1m[32m0.19000[0m[0m | time: 1210.005s
| Adam | epoch: 002 | loss: 0.19000 - acc: 0.9270 -- iter: 25800/28000
Training Step: 1077  | total loss: [1m[32m0.18411[0m[0m | time: 1211.282s
| Adam | epoch: 002 | loss: 0.18411 - acc: 0.9323 -- iter: 25850/28000
Training Step: 1078  | total loss: [1m[32m0.17939[0m[0m | time: 1212.546s
| Adam | epoch: 002 | loss: 0.17939 - acc: 0.9351 -- iter: 25900/28000
Training Step: 1079  | total loss: [1m[32m0.18449[0m[0m | time: 1213.806s
| Adam | epoch: 002 | loss: 0.18449 - acc: 0.9356 -- iter: 25950/28000
Training Step: 1080  | total loss: [1m[32m0.18035[0m[0m | time: 1215.052s
| Adam | epoch: 002 | loss: 0.18035 - acc: 0.9380 -- iter: 26000/28000
Training Step: 1081  | total loss: [1m[32m0.19231[0m[0m | time: 1216.335s
| Adam | epoch: 002 | loss: 0.19231 - acc: 0.9342 -- iter: 26050/28000
Training Step: 1082  | total loss: [1m[32m0.18541[0m[0m | time: 1217.609s
| Adam | epoch: 002 | loss: 0.18541 - acc: 0.9368 -- iter: 26100/28000
Training Step: 1083  | total loss: [1m[32m0.17498[0m[0m | time: 1218.892s
| Adam | epoch: 002 | loss: 0.17498 - acc: 0.9411 -- iter: 26150/28000
Training Step: 1084  | total loss: [1m[32m0.17023[0m[0m | time: 1220.208s
| Adam | epoch: 002 | loss: 0.17023 - acc: 0.9430 -- iter: 26200/28000
Training Step: 1085  | total loss: [1m[32m0.16067[0m[0m | time: 1221.514s
| Adam | epoch: 002 | loss: 0.16067 - acc: 0.9447 -- iter: 26250/28000
Training Step: 1086  | total loss: [1m[32m0.14864[0m[0m | time: 1222.769s
| Adam | epoch: 002 | loss: 0.14864 - acc: 0.9502 -- iter: 26300/28000
Training Step: 1087  | total loss: [1m[32m0.14753[0m[0m | time: 1224.046s
| Adam | epoch: 002 | loss: 0.14753 - acc: 0.9512 -- iter: 26350/28000
Training Step: 1088  | total loss: [1m[32m0.14930[0m[0m | time: 1225.396s
| Adam | epoch: 002 | loss: 0.14930 - acc: 0.9481 -- iter: 26400/28000
Training Step: 1089  | total loss: [1m[32m0.16959[0m[0m | time: 1226.740s
| Adam | epoch: 002 | loss: 0.16959 - acc: 0.9373 -- iter: 26450/28000
Training Step: 1090  | total loss: [1m[32m0.17994[0m[0m | time: 1228.055s
| Adam | epoch: 002 | loss: 0.17994 - acc: 0.9336 -- iter: 26500/28000
Training Step: 1091  | total loss: [1m[32m0.17921[0m[0m | time: 1229.351s
| Adam | epoch: 002 | loss: 0.17921 - acc: 0.9342 -- iter: 26550/28000
Training Step: 1092  | total loss: [1m[32m0.17428[0m[0m | time: 1230.723s
| Adam | epoch: 002 | loss: 0.17428 - acc: 0.9308 -- iter: 26600/28000
Training Step: 1093  | total loss: [1m[32m0.17923[0m[0m | time: 1232.061s
| Adam | epoch: 002 | loss: 0.17923 - acc: 0.9337 -- iter: 26650/28000
Training Step: 1094  | total loss: [1m[32m0.17241[0m[0m | time: 1233.402s
| Adam | epoch: 002 | loss: 0.17241 - acc: 0.9343 -- iter: 26700/28000
Training Step: 1095  | total loss: [1m[32m0.16525[0m[0m | time: 1234.667s
| Adam | epoch: 002 | loss: 0.16525 - acc: 0.9389 -- iter: 26750/28000
Training Step: 1096  | total loss: [1m[32m0.15450[0m[0m | time: 1235.968s
| Adam | epoch: 002 | loss: 0.15450 - acc: 0.9450 -- iter: 26800/28000
Training Step: 1097  | total loss: [1m[32m0.16288[0m[0m | time: 1237.244s
| Adam | epoch: 002 | loss: 0.16288 - acc: 0.9465 -- iter: 26850/28000
Training Step: 1098  | total loss: [1m[32m0.16969[0m[0m | time: 1238.521s
| Adam | epoch: 002 | loss: 0.16969 - acc: 0.9459 -- iter: 26900/28000
Training Step: 1099  | total loss: [1m[32m0.17686[0m[0m | time: 1239.902s
| Adam | epoch: 002 | loss: 0.17686 - acc: 0.9413 -- iter: 26950/28000
Training Step: 1100  | total loss: [1m[32m0.17613[0m[0m | time: 1270.396s
| Adam | epoch: 002 | loss: 0.17613 - acc: 0.9411 | val_loss: 0.21138 - val_acc: 0.9218 -- iter: 27000/28000
--
Training Step: 1101  | total loss: [1m[32m0.17981[0m[0m | time: 1271.741s
| Adam | epoch: 002 | loss: 0.17981 - acc: 0.9390 -- iter: 27050/28000
Training Step: 1102  | total loss: [1m[32m0.17752[0m[0m | time: 1273.034s
| Adam | epoch: 002 | loss: 0.17752 - acc: 0.9391 -- iter: 27100/28000
Training Step: 1103  | total loss: [1m[32m0.19727[0m[0m | time: 1274.298s
| Adam | epoch: 002 | loss: 0.19727 - acc: 0.9332 -- iter: 27150/28000
Training Step: 1104  | total loss: [1m[32m0.18579[0m[0m | time: 1275.509s
| Adam | epoch: 002 | loss: 0.18579 - acc: 0.9379 -- iter: 27200/28000
Training Step: 1105  | total loss: [1m[32m0.19228[0m[0m | time: 1276.773s
| Adam | epoch: 002 | loss: 0.19228 - acc: 0.9301 -- iter: 27250/28000
Training Step: 1106  | total loss: [1m[32m0.18654[0m[0m | time: 1278.092s
| Adam | epoch: 002 | loss: 0.18654 - acc: 0.9291 -- iter: 27300/28000
Training Step: 1107  | total loss: [1m[32m0.20252[0m[0m | time: 1279.453s
| Adam | epoch: 002 | loss: 0.20252 - acc: 0.9262 -- iter: 27350/28000
Training Step: 1108  | total loss: [1m[32m0.20241[0m[0m | time: 1280.758s
| Adam | epoch: 002 | loss: 0.20241 - acc: 0.9296 -- iter: 27400/28000
Training Step: 1109  | total loss: [1m[32m0.20062[0m[0m | time: 1282.023s
| Adam | epoch: 002 | loss: 0.20062 - acc: 0.9266 -- iter: 27450/28000
Training Step: 1110  | total loss: [1m[32m0.22167[0m[0m | time: 1283.301s
| Adam | epoch: 002 | loss: 0.22167 - acc: 0.9159 -- iter: 27500/28000
Training Step: 1111  | total loss: [1m[32m0.20919[0m[0m | time: 1284.603s
| Adam | epoch: 002 | loss: 0.20919 - acc: 0.9204 -- iter: 27550/28000
Training Step: 1112  | total loss: [1m[32m0.20034[0m[0m | time: 1285.861s
| Adam | epoch: 002 | loss: 0.20034 - acc: 0.9263 -- iter: 27600/28000
Training Step: 1113  | total loss: [1m[32m0.18873[0m[0m | time: 1287.123s
| Adam | epoch: 002 | loss: 0.18873 - acc: 0.9337 -- iter: 27650/28000
Training Step: 1114  | total loss: [1m[32m0.22455[0m[0m | time: 1288.690s
| Adam | epoch: 002 | loss: 0.22455 - acc: 0.9223 -- iter: 27700/28000
Training Step: 1115  | total loss: [1m[32m0.24371[0m[0m | time: 1290.383s
| Adam | epoch: 002 | loss: 0.24371 - acc: 0.9161 -- iter: 27750/28000
Training Step: 1116  | total loss: [1m[32m0.23570[0m[0m | time: 1291.983s
| Adam | epoch: 002 | loss: 0.23570 - acc: 0.9185 -- iter: 27800/28000
Training Step: 1117  | total loss: [1m[32m0.23212[0m[0m | time: 1293.328s
| Adam | epoch: 002 | loss: 0.23212 - acc: 0.9206 -- iter: 27850/28000
Training Step: 1118  | total loss: [1m[32m0.22927[0m[0m | time: 1294.609s
| Adam | epoch: 002 | loss: 0.22927 - acc: 0.9186 -- iter: 27900/28000
Training Step: 1119  | total loss: [1m[32m0.22735[0m[0m | time: 1295.892s
| Adam | epoch: 002 | loss: 0.22735 - acc: 0.9187 -- iter: 27950/28000
Training Step: 1120  | total loss: [1m[32m0.23344[0m[0m | time: 1297.188s
| Adam | epoch: 002 | loss: 0.23344 - acc: 0.9108 -- iter: 28000/28000
Training Step: 1121  | total loss: [1m[32m0.23013[0m[0m | time: 1.341s
| Adam | epoch: 003 | loss: 0.23013 - acc: 0.9158 -- iter: 00050/28000
Training Step: 1122  | total loss: [1m[32m0.22708[0m[0m | time: 2.743s
| Adam | epoch: 003 | loss: 0.22708 - acc: 0.9182 -- iter: 00100/28000
Training Step: 1123  | total loss: [1m[32m0.22279[0m[0m | time: 4.062s
| Adam | epoch: 003 | loss: 0.22279 - acc: 0.9184 -- iter: 00150/28000
Training Step: 1124  | total loss: [1m[32m0.21858[0m[0m | time: 5.326s
| Adam | epoch: 003 | loss: 0.21858 - acc: 0.9205 -- iter: 00200/28000
Training Step: 1125  | total loss: [1m[32m0.21624[0m[0m | time: 35.211s
| Adam | epoch: 003 | loss: 0.21624 - acc: 0.9225 | val_loss: 0.22234 - val_acc: 0.9152 -- iter: 00250/28000
--
Training Step: 1126  | total loss: [1m[32m0.21219[0m[0m | time: 36.519s
| Adam | epoch: 003 | loss: 0.21219 - acc: 0.9242 -- iter: 00300/28000
Training Step: 1127  | total loss: [1m[32m0.21295[0m[0m | time: 37.790s
| Adam | epoch: 003 | loss: 0.21295 - acc: 0.9218 -- iter: 00350/28000
Training Step: 1128  | total loss: [1m[32m0.21523[0m[0m | time: 39.043s
| Adam | epoch: 003 | loss: 0.21523 - acc: 0.9196 -- iter: 00400/28000
Training Step: 1129  | total loss: [1m[32m0.21801[0m[0m | time: 40.316s
| Adam | epoch: 003 | loss: 0.21801 - acc: 0.9197 -- iter: 00450/28000
Training Step: 1130  | total loss: [1m[32m0.21643[0m[0m | time: 41.585s
| Adam | epoch: 003 | loss: 0.21643 - acc: 0.9217 -- iter: 00500/28000
Training Step: 1131  | total loss: [1m[32m0.20155[0m[0m | time: 42.882s
| Adam | epoch: 003 | loss: 0.20155 - acc: 0.9295 -- iter: 00550/28000
Training Step: 1132  | total loss: [1m[32m0.19373[0m[0m | time: 44.127s
| Adam | epoch: 003 | loss: 0.19373 - acc: 0.9326 -- iter: 00600/28000
Training Step: 1133  | total loss: [1m[32m0.19294[0m[0m | time: 45.393s
| Adam | epoch: 003 | loss: 0.19294 - acc: 0.9333 -- iter: 00650/28000
Training Step: 1134  | total loss: [1m[32m0.18414[0m[0m | time: 46.672s
| Adam | epoch: 003 | loss: 0.18414 - acc: 0.9380 -- iter: 00700/28000
Training Step: 1135  | total loss: [1m[32m0.19493[0m[0m | time: 47.944s
| Adam | epoch: 003 | loss: 0.19493 - acc: 0.9322 -- iter: 00750/28000
Training Step: 1136  | total loss: [1m[32m0.19942[0m[0m | time: 49.208s
| Adam | epoch: 003 | loss: 0.19942 - acc: 0.9330 -- iter: 00800/28000
Training Step: 1137  | total loss: [1m[32m0.19114[0m[0m | time: 50.469s
| Adam | epoch: 003 | loss: 0.19114 - acc: 0.9377 -- iter: 00850/28000
Training Step: 1138  | total loss: [1m[32m0.19976[0m[0m | time: 51.744s
| Adam | epoch: 003 | loss: 0.19976 - acc: 0.9359 -- iter: 00900/28000
Training Step: 1139  | total loss: [1m[32m0.19699[0m[0m | time: 53.027s
| Adam | epoch: 003 | loss: 0.19699 - acc: 0.9363 -- iter: 00950/28000
Training Step: 1140  | total loss: [1m[32m0.18234[0m[0m | time: 54.308s
| Adam | epoch: 003 | loss: 0.18234 - acc: 0.9407 -- iter: 01000/28000
Training Step: 1141  | total loss: [1m[32m0.18195[0m[0m | time: 55.554s
| Adam | epoch: 003 | loss: 0.18195 - acc: 0.9426 -- iter: 01050/28000
Training Step: 1142  | total loss: [1m[32m0.17655[0m[0m | time: 56.820s
| Adam | epoch: 003 | loss: 0.17655 - acc: 0.9424 -- iter: 01100/28000
Training Step: 1143  | total loss: [1m[32m0.17972[0m[0m | time: 58.111s
| Adam | epoch: 003 | loss: 0.17972 - acc: 0.9401 -- iter: 01150/28000
Training Step: 1144  | total loss: [1m[32m0.18411[0m[0m | time: 59.415s
| Adam | epoch: 003 | loss: 0.18411 - acc: 0.9341 -- iter: 01200/28000
Training Step: 1145  | total loss: [1m[32m0.18574[0m[0m | time: 60.702s
| Adam | epoch: 003 | loss: 0.18574 - acc: 0.9347 -- iter: 01250/28000
Training Step: 1146  | total loss: [1m[32m0.18577[0m[0m | time: 61.984s
| Adam | epoch: 003 | loss: 0.18577 - acc: 0.9312 -- iter: 01300/28000
Training Step: 1147  | total loss: [1m[32m0.18237[0m[0m | time: 63.251s
| Adam | epoch: 003 | loss: 0.18237 - acc: 0.9321 -- iter: 01350/28000
Training Step: 1148  | total loss: [1m[32m0.20198[0m[0m | time: 64.517s
| Adam | epoch: 003 | loss: 0.20198 - acc: 0.9289 -- iter: 01400/28000
Training Step: 1149  | total loss: [1m[32m0.19446[0m[0m | time: 65.771s
| Adam | epoch: 003 | loss: 0.19446 - acc: 0.9300 -- iter: 01450/28000
Training Step: 1150  | total loss: [1m[32m0.19652[0m[0m | time: 95.786s
| Adam | epoch: 003 | loss: 0.19652 - acc: 0.9310 | val_loss: 0.19896 - val_acc: 0.9262 -- iter: 01500/28000
--
Training Step: 1151  | total loss: [1m[32m0.18549[0m[0m | time: 97.115s
| Adam | epoch: 003 | loss: 0.18549 - acc: 0.9359 -- iter: 01550/28000
Training Step: 1152  | total loss: [1m[32m0.18543[0m[0m | time: 98.406s
| Adam | epoch: 003 | loss: 0.18543 - acc: 0.9343 -- iter: 01600/28000
Training Step: 1153  | total loss: [1m[32m0.17683[0m[0m | time: 99.653s
| Adam | epoch: 003 | loss: 0.17683 - acc: 0.9389 -- iter: 01650/28000
Training Step: 1154  | total loss: [1m[32m0.17749[0m[0m | time: 100.905s
| Adam | epoch: 003 | loss: 0.17749 - acc: 0.9390 -- iter: 01700/28000
Training Step: 1155  | total loss: [1m[32m0.18511[0m[0m | time: 102.178s
| Adam | epoch: 003 | loss: 0.18511 - acc: 0.9371 -- iter: 01750/28000
Training Step: 1156  | total loss: [1m[32m0.17665[0m[0m | time: 103.448s
| Adam | epoch: 003 | loss: 0.17665 - acc: 0.9414 -- iter: 01800/28000
Training Step: 1157  | total loss: [1m[32m0.19573[0m[0m | time: 104.700s
| Adam | epoch: 003 | loss: 0.19573 - acc: 0.9392 -- iter: 01850/28000
Training Step: 1158  | total loss: [1m[32m0.19367[0m[0m | time: 105.976s
| Adam | epoch: 003 | loss: 0.19367 - acc: 0.9393 -- iter: 01900/28000
Training Step: 1159  | total loss: [1m[32m0.18634[0m[0m | time: 107.284s
| Adam | epoch: 003 | loss: 0.18634 - acc: 0.9394 -- iter: 01950/28000
Training Step: 1160  | total loss: [1m[32m0.18653[0m[0m | time: 108.578s
| Adam | epoch: 003 | loss: 0.18653 - acc: 0.9395 -- iter: 02000/28000
Training Step: 1161  | total loss: [1m[32m0.18048[0m[0m | time: 109.843s
| Adam | epoch: 003 | loss: 0.18048 - acc: 0.9395 -- iter: 02050/28000
Training Step: 1162  | total loss: [1m[32m0.19325[0m[0m | time: 111.099s
| Adam | epoch: 003 | loss: 0.19325 - acc: 0.9336 -- iter: 02100/28000
Training Step: 1163  | total loss: [1m[32m0.19216[0m[0m | time: 112.371s
| Adam | epoch: 003 | loss: 0.19216 - acc: 0.9322 -- iter: 02150/28000
Training Step: 1164  | total loss: [1m[32m0.18558[0m[0m | time: 113.659s
| Adam | epoch: 003 | loss: 0.18558 - acc: 0.9350 -- iter: 02200/28000
Training Step: 1165  | total loss: [1m[32m0.18942[0m[0m | time: 114.933s
| Adam | epoch: 003 | loss: 0.18942 - acc: 0.9335 -- iter: 02250/28000
Training Step: 1166  | total loss: [1m[32m0.20407[0m[0m | time: 116.203s
| Adam | epoch: 003 | loss: 0.20407 - acc: 0.9301 -- iter: 02300/28000
Training Step: 1167  | total loss: [1m[32m0.20738[0m[0m | time: 117.541s
| Adam | epoch: 003 | loss: 0.20738 - acc: 0.9231 -- iter: 02350/28000
Training Step: 1168  | total loss: [1m[32m0.20401[0m[0m | time: 118.862s
| Adam | epoch: 003 | loss: 0.20401 - acc: 0.9228 -- iter: 02400/28000
Training Step: 1169  | total loss: [1m[32m0.20727[0m[0m | time: 120.128s
| Adam | epoch: 003 | loss: 0.20727 - acc: 0.9165 -- iter: 02450/28000
Training Step: 1170  | total loss: [1m[32m0.20881[0m[0m | time: 121.405s
| Adam | epoch: 003 | loss: 0.20881 - acc: 0.9189 -- iter: 02500/28000
Training Step: 1171  | total loss: [1m[32m0.20713[0m[0m | time: 122.680s
| Adam | epoch: 003 | loss: 0.20713 - acc: 0.9210 -- iter: 02550/28000
Training Step: 1172  | total loss: [1m[32m0.21492[0m[0m | time: 123.964s
| Adam | epoch: 003 | loss: 0.21492 - acc: 0.9169 -- iter: 02600/28000
Training Step: 1173  | total loss: [1m[32m0.20735[0m[0m | time: 125.225s
| Adam | epoch: 003 | loss: 0.20735 - acc: 0.9192 -- iter: 02650/28000
Training Step: 1174  | total loss: [1m[32m0.19972[0m[0m | time: 126.514s
| Adam | epoch: 003 | loss: 0.19972 - acc: 0.9233 -- iter: 02700/28000
Training Step: 1175  | total loss: [1m[32m0.19640[0m[0m | time: 156.643s
| Adam | epoch: 003 | loss: 0.19640 - acc: 0.9250 | val_loss: 0.20205 - val_acc: 0.9236 -- iter: 02750/28000
--
Training Step: 1176  | total loss: [1m[32m0.19002[0m[0m | time: 157.987s
| Adam | epoch: 003 | loss: 0.19002 - acc: 0.9285 -- iter: 02800/28000
Training Step: 1177  | total loss: [1m[32m0.18663[0m[0m | time: 159.280s
| Adam | epoch: 003 | loss: 0.18663 - acc: 0.9296 -- iter: 02850/28000
Training Step: 1178  | total loss: [1m[32m0.18907[0m[0m | time: 160.579s
| Adam | epoch: 003 | loss: 0.18907 - acc: 0.9286 -- iter: 02900/28000
Training Step: 1179  | total loss: [1m[32m0.18970[0m[0m | time: 161.884s
| Adam | epoch: 003 | loss: 0.18970 - acc: 0.9278 -- iter: 02950/28000
Training Step: 1180  | total loss: [1m[32m0.18171[0m[0m | time: 163.247s
| Adam | epoch: 003 | loss: 0.18171 - acc: 0.9310 -- iter: 03000/28000
Training Step: 1181  | total loss: [1m[32m0.18500[0m[0m | time: 164.522s
| Adam | epoch: 003 | loss: 0.18500 - acc: 0.9319 -- iter: 03050/28000
Training Step: 1182  | total loss: [1m[32m0.18983[0m[0m | time: 165.857s
| Adam | epoch: 003 | loss: 0.18983 - acc: 0.9307 -- iter: 03100/28000
Training Step: 1183  | total loss: [1m[32m0.19385[0m[0m | time: 167.156s
| Adam | epoch: 003 | loss: 0.19385 - acc: 0.9256 -- iter: 03150/28000
Training Step: 1184  | total loss: [1m[32m0.18910[0m[0m | time: 168.458s
| Adam | epoch: 003 | loss: 0.18910 - acc: 0.9271 -- iter: 03200/28000
Training Step: 1185  | total loss: [1m[32m0.18843[0m[0m | time: 169.784s
| Adam | epoch: 003 | loss: 0.18843 - acc: 0.9284 -- iter: 03250/28000
Training Step: 1186  | total loss: [1m[32m0.19943[0m[0m | time: 171.149s
| Adam | epoch: 003 | loss: 0.19943 - acc: 0.9255 -- iter: 03300/28000
Training Step: 1187  | total loss: [1m[32m0.20213[0m[0m | time: 172.435s
| Adam | epoch: 003 | loss: 0.20213 - acc: 0.9250 -- iter: 03350/28000
Training Step: 1188  | total loss: [1m[32m0.20386[0m[0m | time: 173.716s
| Adam | epoch: 003 | loss: 0.20386 - acc: 0.9225 -- iter: 03400/28000
Training Step: 1189  | total loss: [1m[32m0.20367[0m[0m | time: 175.011s
| Adam | epoch: 003 | loss: 0.20367 - acc: 0.9222 -- iter: 03450/28000
Training Step: 1190  | total loss: [1m[32m0.19606[0m[0m | time: 176.257s
| Adam | epoch: 003 | loss: 0.19606 - acc: 0.9260 -- iter: 03500/28000
Training Step: 1191  | total loss: [1m[32m0.19366[0m[0m | time: 177.552s
| Adam | epoch: 003 | loss: 0.19366 - acc: 0.9274 -- iter: 03550/28000
Training Step: 1192  | total loss: [1m[32m0.19817[0m[0m | time: 178.848s
| Adam | epoch: 003 | loss: 0.19817 - acc: 0.9287 -- iter: 03600/28000
Training Step: 1193  | total loss: [1m[32m0.18854[0m[0m | time: 180.109s
| Adam | epoch: 003 | loss: 0.18854 - acc: 0.9318 -- iter: 03650/28000
Training Step: 1194  | total loss: [1m[32m0.20300[0m[0m | time: 181.363s
| Adam | epoch: 003 | loss: 0.20300 - acc: 0.9286 -- iter: 03700/28000
Training Step: 1195  | total loss: [1m[32m0.20459[0m[0m | time: 182.642s
| Adam | epoch: 003 | loss: 0.20459 - acc: 0.9278 -- iter: 03750/28000
Training Step: 1196  | total loss: [1m[32m0.20871[0m[0m | time: 183.921s
| Adam | epoch: 003 | loss: 0.20871 - acc: 0.9270 -- iter: 03800/28000
Training Step: 1197  | total loss: [1m[32m0.20924[0m[0m | time: 185.179s
| Adam | epoch: 003 | loss: 0.20924 - acc: 0.9263 -- iter: 03850/28000
Training Step: 1198  | total loss: [1m[32m0.21581[0m[0m | time: 186.523s
| Adam | epoch: 003 | loss: 0.21581 - acc: 0.9297 -- iter: 03900/28000
Training Step: 1199  | total loss: [1m[32m0.20929[0m[0m | time: 187.972s
| Adam | epoch: 003 | loss: 0.20929 - acc: 0.9287 -- iter: 03950/28000
Training Step: 1200  | total loss: [1m[32m0.22388[0m[0m | time: 220.443s
| Adam | epoch: 003 | loss: 0.22388 - acc: 0.9238 | val_loss: 0.21306 - val_acc: 0.9201 -- iter: 04000/28000
--
Training Step: 1201  | total loss: [1m[32m0.22508[0m[0m | time: 221.746s
| Adam | epoch: 003 | loss: 0.22508 - acc: 0.9234 -- iter: 04050/28000
Training Step: 1202  | total loss: [1m[32m0.22350[0m[0m | time: 223.008s
| Adam | epoch: 003 | loss: 0.22350 - acc: 0.9251 -- iter: 04100/28000
Training Step: 1203  | total loss: [1m[32m0.22293[0m[0m | time: 224.259s
| Adam | epoch: 003 | loss: 0.22293 - acc: 0.9286 -- iter: 04150/28000
Training Step: 1204  | total loss: [1m[32m0.22051[0m[0m | time: 225.532s
| Adam | epoch: 003 | loss: 0.22051 - acc: 0.9277 -- iter: 04200/28000
Training Step: 1205  | total loss: [1m[32m0.21828[0m[0m | time: 226.775s
| Adam | epoch: 003 | loss: 0.21828 - acc: 0.9230 -- iter: 04250/28000
Training Step: 1206  | total loss: [1m[32m0.20923[0m[0m | time: 228.015s
| Adam | epoch: 003 | loss: 0.20923 - acc: 0.9287 -- iter: 04300/28000
Training Step: 1207  | total loss: [1m[32m0.20734[0m[0m | time: 229.284s
| Adam | epoch: 003 | loss: 0.20734 - acc: 0.9318 -- iter: 04350/28000
Training Step: 1208  | total loss: [1m[32m0.20950[0m[0m | time: 230.542s
| Adam | epoch: 003 | loss: 0.20950 - acc: 0.9326 -- iter: 04400/28000
Training Step: 1209  | total loss: [1m[32m0.21152[0m[0m | time: 231.792s
| Adam | epoch: 003 | loss: 0.21152 - acc: 0.9334 -- iter: 04450/28000
Training Step: 1210  | total loss: [1m[32m0.21273[0m[0m | time: 233.056s
| Adam | epoch: 003 | loss: 0.21273 - acc: 0.9300 -- iter: 04500/28000
Training Step: 1211  | total loss: [1m[32m0.20448[0m[0m | time: 234.307s
| Adam | epoch: 003 | loss: 0.20448 - acc: 0.9330 -- iter: 04550/28000
Training Step: 1212  | total loss: [1m[32m0.20706[0m[0m | time: 235.584s
| Adam | epoch: 003 | loss: 0.20706 - acc: 0.9317 -- iter: 04600/28000
Training Step: 1213  | total loss: [1m[32m0.20641[0m[0m | time: 236.820s
| Adam | epoch: 003 | loss: 0.20641 - acc: 0.9365 -- iter: 04650/28000
Training Step: 1214  | total loss: [1m[32m0.20631[0m[0m | time: 238.071s
| Adam | epoch: 003 | loss: 0.20631 - acc: 0.9349 -- iter: 04700/28000
Training Step: 1215  | total loss: [1m[32m0.20507[0m[0m | time: 239.395s
| Adam | epoch: 003 | loss: 0.20507 - acc: 0.9334 -- iter: 04750/28000
Training Step: 1216  | total loss: [1m[32m0.20436[0m[0m | time: 240.651s
| Adam | epoch: 003 | loss: 0.20436 - acc: 0.9301 -- iter: 04800/28000
Training Step: 1217  | total loss: [1m[32m0.19693[0m[0m | time: 241.899s
| Adam | epoch: 003 | loss: 0.19693 - acc: 0.9311 -- iter: 04850/28000
Training Step: 1218  | total loss: [1m[32m0.19660[0m[0m | time: 243.175s
| Adam | epoch: 003 | loss: 0.19660 - acc: 0.9299 -- iter: 04900/28000
Training Step: 1219  | total loss: [1m[32m0.19191[0m[0m | time: 244.421s
| Adam | epoch: 003 | loss: 0.19191 - acc: 0.9330 -- iter: 04950/28000
Training Step: 1220  | total loss: [1m[32m0.18782[0m[0m | time: 245.672s
| Adam | epoch: 003 | loss: 0.18782 - acc: 0.9337 -- iter: 05000/28000
Training Step: 1221  | total loss: [1m[32m0.18054[0m[0m | time: 246.947s
| Adam | epoch: 003 | loss: 0.18054 - acc: 0.9343 -- iter: 05050/28000
Training Step: 1222  | total loss: [1m[32m0.17816[0m[0m | time: 248.213s
| Adam | epoch: 003 | loss: 0.17816 - acc: 0.9329 -- iter: 05100/28000
Training Step: 1223  | total loss: [1m[32m0.18987[0m[0m | time: 249.482s
| Adam | epoch: 003 | loss: 0.18987 - acc: 0.9296 -- iter: 05150/28000
Training Step: 1224  | total loss: [1m[32m0.18361[0m[0m | time: 250.773s
| Adam | epoch: 003 | loss: 0.18361 - acc: 0.9326 -- iter: 05200/28000
Training Step: 1225  | total loss: [1m[32m0.20589[0m[0m | time: 281.006s
| Adam | epoch: 003 | loss: 0.20589 - acc: 0.9274 | val_loss: 0.19738 - val_acc: 0.9310 -- iter: 05250/28000
--
Training Step: 1226  | total loss: [1m[32m0.19593[0m[0m | time: 282.303s
| Adam | epoch: 003 | loss: 0.19593 - acc: 0.9326 -- iter: 05300/28000
Training Step: 1227  | total loss: [1m[32m0.18673[0m[0m | time: 283.590s
| Adam | epoch: 003 | loss: 0.18673 - acc: 0.9374 -- iter: 05350/28000
Training Step: 1228  | total loss: [1m[32m0.18441[0m[0m | time: 284.846s
| Adam | epoch: 003 | loss: 0.18441 - acc: 0.9376 -- iter: 05400/28000
Training Step: 1229  | total loss: [1m[32m0.17616[0m[0m | time: 286.115s
| Adam | epoch: 003 | loss: 0.17616 - acc: 0.9399 -- iter: 05450/28000
Training Step: 1230  | total loss: [1m[32m0.18920[0m[0m | time: 287.380s
| Adam | epoch: 003 | loss: 0.18920 - acc: 0.9399 -- iter: 05500/28000
Training Step: 1231  | total loss: [1m[32m0.17821[0m[0m | time: 288.668s
| Adam | epoch: 003 | loss: 0.17821 - acc: 0.9459 -- iter: 05550/28000
Training Step: 1232  | total loss: [1m[32m0.17370[0m[0m | time: 289.924s
| Adam | epoch: 003 | loss: 0.17370 - acc: 0.9493 -- iter: 05600/28000
Training Step: 1233  | total loss: [1m[32m0.17908[0m[0m | time: 291.273s
| Adam | epoch: 003 | loss: 0.17908 - acc: 0.9464 -- iter: 05650/28000
Training Step: 1234  | total loss: [1m[32m0.18012[0m[0m | time: 292.547s
| Adam | epoch: 003 | loss: 0.18012 - acc: 0.9437 -- iter: 05700/28000
Training Step: 1235  | total loss: [1m[32m0.19300[0m[0m | time: 293.813s
| Adam | epoch: 003 | loss: 0.19300 - acc: 0.9374 -- iter: 05750/28000
Training Step: 1236  | total loss: [1m[32m0.18615[0m[0m | time: 295.105s
| Adam | epoch: 003 | loss: 0.18615 - acc: 0.9416 -- iter: 05800/28000
Training Step: 1237  | total loss: [1m[32m0.18829[0m[0m | time: 296.362s
| Adam | epoch: 003 | loss: 0.18829 - acc: 0.9395 -- iter: 05850/28000
Training Step: 1238  | total loss: [1m[32m0.19655[0m[0m | time: 297.652s
| Adam | epoch: 003 | loss: 0.19655 - acc: 0.9375 -- iter: 05900/28000
Training Step: 1239  | total loss: [1m[32m0.21390[0m[0m | time: 298.949s
| Adam | epoch: 003 | loss: 0.21390 - acc: 0.9318 -- iter: 05950/28000
Training Step: 1240  | total loss: [1m[32m0.22461[0m[0m | time: 300.252s
| Adam | epoch: 003 | loss: 0.22461 - acc: 0.9226 -- iter: 06000/28000
Training Step: 1241  | total loss: [1m[32m0.22655[0m[0m | time: 301.512s
| Adam | epoch: 003 | loss: 0.22655 - acc: 0.9183 -- iter: 06050/28000
Training Step: 1242  | total loss: [1m[32m0.20918[0m[0m | time: 302.785s
| Adam | epoch: 003 | loss: 0.20918 - acc: 0.9265 -- iter: 06100/28000
Training Step: 1243  | total loss: [1m[32m0.19528[0m[0m | time: 304.052s
| Adam | epoch: 003 | loss: 0.19528 - acc: 0.9318 -- iter: 06150/28000
Training Step: 1244  | total loss: [1m[32m0.18519[0m[0m | time: 305.309s
| Adam | epoch: 003 | loss: 0.18519 - acc: 0.9387 -- iter: 06200/28000
Training Step: 1245  | total loss: [1m[32m0.18209[0m[0m | time: 306.620s
| Adam | epoch: 003 | loss: 0.18209 - acc: 0.9388 -- iter: 06250/28000
Training Step: 1246  | total loss: [1m[32m0.18212[0m[0m | time: 307.907s
| Adam | epoch: 003 | loss: 0.18212 - acc: 0.9389 -- iter: 06300/28000
Training Step: 1247  | total loss: [1m[32m0.18329[0m[0m | time: 309.189s
| Adam | epoch: 003 | loss: 0.18329 - acc: 0.9410 -- iter: 06350/28000
Training Step: 1248  | total loss: [1m[32m0.18563[0m[0m | time: 310.449s
| Adam | epoch: 003 | loss: 0.18563 - acc: 0.9409 -- iter: 06400/28000
Training Step: 1249  | total loss: [1m[32m0.18694[0m[0m | time: 311.717s
| Adam | epoch: 003 | loss: 0.18694 - acc: 0.9388 -- iter: 06450/28000
Training Step: 1250  | total loss: [1m[32m0.19472[0m[0m | time: 341.692s
| Adam | epoch: 003 | loss: 0.19472 - acc: 0.9329 | val_loss: 0.25316 - val_acc: 0.9034 -- iter: 06500/28000
--
Training Step: 1251  | total loss: [1m[32m0.19684[0m[0m | time: 343.015s
| Adam | epoch: 003 | loss: 0.19684 - acc: 0.9337 -- iter: 06550/28000
Training Step: 1252  | total loss: [1m[32m0.20631[0m[0m | time: 344.273s
| Adam | epoch: 003 | loss: 0.20631 - acc: 0.9283 -- iter: 06600/28000
Training Step: 1253  | total loss: [1m[32m0.20374[0m[0m | time: 345.546s
| Adam | epoch: 003 | loss: 0.20374 - acc: 0.9235 -- iter: 06650/28000
Training Step: 1254  | total loss: [1m[32m0.19124[0m[0m | time: 346.840s
| Adam | epoch: 003 | loss: 0.19124 - acc: 0.9311 -- iter: 06700/28000
Training Step: 1255  | total loss: [1m[32m0.19328[0m[0m | time: 348.123s
| Adam | epoch: 003 | loss: 0.19328 - acc: 0.9300 -- iter: 06750/28000
Training Step: 1256  | total loss: [1m[32m0.19425[0m[0m | time: 349.385s
| Adam | epoch: 003 | loss: 0.19425 - acc: 0.9270 -- iter: 06800/28000
Training Step: 1257  | total loss: [1m[32m0.20950[0m[0m | time: 350.699s
| Adam | epoch: 003 | loss: 0.20950 - acc: 0.9243 -- iter: 06850/28000
Training Step: 1258  | total loss: [1m[32m0.21301[0m[0m | time: 351.981s
| Adam | epoch: 003 | loss: 0.21301 - acc: 0.9199 -- iter: 06900/28000
Training Step: 1259  | total loss: [1m[32m0.20480[0m[0m | time: 353.264s
| Adam | epoch: 003 | loss: 0.20480 - acc: 0.9239 -- iter: 06950/28000
Training Step: 1260  | total loss: [1m[32m0.23787[0m[0m | time: 354.548s
| Adam | epoch: 003 | loss: 0.23787 - acc: 0.9155 -- iter: 07000/28000
Training Step: 1261  | total loss: [1m[32m0.23646[0m[0m | time: 355.886s
| Adam | epoch: 003 | loss: 0.23646 - acc: 0.9139 -- iter: 07050/28000
Training Step: 1262  | total loss: [1m[32m0.23037[0m[0m | time: 357.319s
| Adam | epoch: 003 | loss: 0.23037 - acc: 0.9126 -- iter: 07100/28000
Training Step: 1263  | total loss: [1m[32m0.22751[0m[0m | time: 358.792s
| Adam | epoch: 003 | loss: 0.22751 - acc: 0.9073 -- iter: 07150/28000
Training Step: 1264  | total loss: [1m[32m0.23178[0m[0m | time: 360.153s
| Adam | epoch: 003 | loss: 0.23178 - acc: 0.9006 -- iter: 07200/28000
Training Step: 1265  | total loss: [1m[32m0.22205[0m[0m | time: 361.410s
| Adam | epoch: 003 | loss: 0.22205 - acc: 0.9025 -- iter: 07250/28000
Training Step: 1266  | total loss: [1m[32m0.21531[0m[0m | time: 362.693s
| Adam | epoch: 003 | loss: 0.21531 - acc: 0.9063 -- iter: 07300/28000
Training Step: 1267  | total loss: [1m[32m0.20743[0m[0m | time: 363.967s
| Adam | epoch: 003 | loss: 0.20743 - acc: 0.9116 -- iter: 07350/28000
Training Step: 1268  | total loss: [1m[32m0.20603[0m[0m | time: 365.223s
| Adam | epoch: 003 | loss: 0.20603 - acc: 0.9145 -- iter: 07400/28000
Training Step: 1269  | total loss: [1m[32m0.20794[0m[0m | time: 366.496s
| Adam | epoch: 003 | loss: 0.20794 - acc: 0.9150 -- iter: 07450/28000
Training Step: 1270  | total loss: [1m[32m0.20343[0m[0m | time: 367.767s
| Adam | epoch: 003 | loss: 0.20343 - acc: 0.9195 -- iter: 07500/28000
Training Step: 1271  | total loss: [1m[32m0.20436[0m[0m | time: 369.044s
| Adam | epoch: 003 | loss: 0.20436 - acc: 0.9156 -- iter: 07550/28000
Training Step: 1272  | total loss: [1m[32m0.20195[0m[0m | time: 370.301s
| Adam | epoch: 003 | loss: 0.20195 - acc: 0.9140 -- iter: 07600/28000
Training Step: 1273  | total loss: [1m[32m0.19357[0m[0m | time: 371.592s
| Adam | epoch: 003 | loss: 0.19357 - acc: 0.9186 -- iter: 07650/28000
Training Step: 1274  | total loss: [1m[32m0.20218[0m[0m | time: 372.863s
| Adam | epoch: 003 | loss: 0.20218 - acc: 0.9147 -- iter: 07700/28000
Training Step: 1275  | total loss: [1m[32m0.21124[0m[0m | time: 402.816s
| Adam | epoch: 003 | loss: 0.21124 - acc: 0.9173 | val_loss: 0.19496 - val_acc: 0.9280 -- iter: 07750/28000
--
Training Step: 1276  | total loss: [1m[32m0.21104[0m[0m | time: 404.123s
| Adam | epoch: 003 | loss: 0.21104 - acc: 0.9235 -- iter: 07800/28000
Training Step: 1277  | total loss: [1m[32m0.21300[0m[0m | time: 405.392s
| Adam | epoch: 003 | loss: 0.21300 - acc: 0.9252 -- iter: 07850/28000
Training Step: 1278  | total loss: [1m[32m0.22570[0m[0m | time: 406.673s
| Adam | epoch: 003 | loss: 0.22570 - acc: 0.9207 -- iter: 07900/28000
Training Step: 1279  | total loss: [1m[32m0.22206[0m[0m | time: 407.975s
| Adam | epoch: 003 | loss: 0.22206 - acc: 0.9226 -- iter: 07950/28000
Training Step: 1280  | total loss: [1m[32m0.21106[0m[0m | time: 409.235s
| Adam | epoch: 003 | loss: 0.21106 - acc: 0.9243 -- iter: 08000/28000
Training Step: 1281  | total loss: [1m[32m0.20653[0m[0m | time: 410.513s
| Adam | epoch: 003 | loss: 0.20653 - acc: 0.9219 -- iter: 08050/28000
Training Step: 1282  | total loss: [1m[32m0.19702[0m[0m | time: 411.832s
| Adam | epoch: 003 | loss: 0.19702 - acc: 0.9257 -- iter: 08100/28000
Training Step: 1283  | total loss: [1m[32m0.19385[0m[0m | time: 413.115s
| Adam | epoch: 003 | loss: 0.19385 - acc: 0.9291 -- iter: 08150/28000
Training Step: 1284  | total loss: [1m[32m0.19792[0m[0m | time: 414.390s
| Adam | epoch: 003 | loss: 0.19792 - acc: 0.9242 -- iter: 08200/28000
Training Step: 1285  | total loss: [1m[32m0.21142[0m[0m | time: 415.677s
| Adam | epoch: 003 | loss: 0.21142 - acc: 0.9178 -- iter: 08250/28000
Training Step: 1286  | total loss: [1m[32m0.21278[0m[0m | time: 416.945s
| Adam | epoch: 003 | loss: 0.21278 - acc: 0.9220 -- iter: 08300/28000
Training Step: 1287  | total loss: [1m[32m0.20740[0m[0m | time: 418.176s
| Adam | epoch: 003 | loss: 0.20740 - acc: 0.9238 -- iter: 08350/28000
Training Step: 1288  | total loss: [1m[32m0.20069[0m[0m | time: 419.449s
| Adam | epoch: 003 | loss: 0.20069 - acc: 0.9294 -- iter: 08400/28000
Training Step: 1289  | total loss: [1m[32m0.20836[0m[0m | time: 420.729s
| Adam | epoch: 003 | loss: 0.20836 - acc: 0.9285 -- iter: 08450/28000
Training Step: 1290  | total loss: [1m[32m0.20304[0m[0m | time: 422.003s
| Adam | epoch: 003 | loss: 0.20304 - acc: 0.9296 -- iter: 08500/28000
Training Step: 1291  | total loss: [1m[32m0.19714[0m[0m | time: 423.301s
| Adam | epoch: 003 | loss: 0.19714 - acc: 0.9307 -- iter: 08550/28000
Training Step: 1292  | total loss: [1m[32m0.19397[0m[0m | time: 424.565s
| Adam | epoch: 003 | loss: 0.19397 - acc: 0.9316 -- iter: 08600/28000
Training Step: 1293  | total loss: [1m[32m0.20262[0m[0m | time: 425.852s
| Adam | epoch: 003 | loss: 0.20262 - acc: 0.9325 -- iter: 08650/28000
Training Step: 1294  | total loss: [1m[32m0.20046[0m[0m | time: 427.152s
| Adam | epoch: 003 | loss: 0.20046 - acc: 0.9312 -- iter: 08700/28000
Training Step: 1295  | total loss: [1m[32m0.19073[0m[0m | time: 428.435s
| Adam | epoch: 003 | loss: 0.19073 - acc: 0.9341 -- iter: 08750/28000
Training Step: 1296  | total loss: [1m[32m0.18829[0m[0m | time: 429.721s
| Adam | epoch: 003 | loss: 0.18829 - acc: 0.9327 -- iter: 08800/28000
Training Step: 1297  | total loss: [1m[32m0.18750[0m[0m | time: 431.015s
| Adam | epoch: 003 | loss: 0.18750 - acc: 0.9334 -- iter: 08850/28000
Training Step: 1298  | total loss: [1m[32m0.18250[0m[0m | time: 432.305s
| Adam | epoch: 003 | loss: 0.18250 - acc: 0.9341 -- iter: 08900/28000
Training Step: 1299  | total loss: [1m[32m0.17569[0m[0m | time: 433.563s
| Adam | epoch: 003 | loss: 0.17569 - acc: 0.9347 -- iter: 08950/28000
Training Step: 1300  | total loss: [1m[32m0.18313[0m[0m | time: 464.198s
| Adam | epoch: 003 | loss: 0.18313 - acc: 0.9332 | val_loss: 0.19200 - val_acc: 0.9303 -- iter: 09000/28000
--
Training Step: 1301  | total loss: [1m[32m0.18521[0m[0m | time: 466.016s
| Adam | epoch: 003 | loss: 0.18521 - acc: 0.9279 -- iter: 09050/28000
Training Step: 1302  | total loss: [1m[32m0.17594[0m[0m | time: 467.604s
| Adam | epoch: 003 | loss: 0.17594 - acc: 0.9331 -- iter: 09100/28000
Training Step: 1303  | total loss: [1m[32m0.18187[0m[0m | time: 468.976s
| Adam | epoch: 003 | loss: 0.18187 - acc: 0.9278 -- iter: 09150/28000
Training Step: 1304  | total loss: [1m[32m0.18056[0m[0m | time: 470.357s
| Adam | epoch: 003 | loss: 0.18056 - acc: 0.9290 -- iter: 09200/28000
Training Step: 1305  | total loss: [1m[32m0.16833[0m[0m | time: 471.629s
| Adam | epoch: 003 | loss: 0.16833 - acc: 0.9341 -- iter: 09250/28000
Training Step: 1306  | total loss: [1m[32m0.17457[0m[0m | time: 472.906s
| Adam | epoch: 003 | loss: 0.17457 - acc: 0.9287 -- iter: 09300/28000
Training Step: 1307  | total loss: [1m[32m0.18071[0m[0m | time: 474.155s
| Adam | epoch: 003 | loss: 0.18071 - acc: 0.9298 -- iter: 09350/28000
Training Step: 1308  | total loss: [1m[32m0.20601[0m[0m | time: 475.418s
| Adam | epoch: 003 | loss: 0.20601 - acc: 0.9248 -- iter: 09400/28000
Training Step: 1309  | total loss: [1m[32m0.19540[0m[0m | time: 476.698s
| Adam | epoch: 003 | loss: 0.19540 - acc: 0.9284 -- iter: 09450/28000
Training Step: 1310  | total loss: [1m[32m0.18799[0m[0m | time: 478.008s
| Adam | epoch: 003 | loss: 0.18799 - acc: 0.9295 -- iter: 09500/28000
Training Step: 1311  | total loss: [1m[32m0.20702[0m[0m | time: 479.324s
| Adam | epoch: 003 | loss: 0.20702 - acc: 0.9246 -- iter: 09550/28000
Training Step: 1312  | total loss: [1m[32m0.19471[0m[0m | time: 480.612s
| Adam | epoch: 003 | loss: 0.19471 - acc: 0.9301 -- iter: 09600/28000
Training Step: 1313  | total loss: [1m[32m0.19459[0m[0m | time: 481.925s
| Adam | epoch: 003 | loss: 0.19459 - acc: 0.9271 -- iter: 09650/28000
Training Step: 1314  | total loss: [1m[32m0.18434[0m[0m | time: 483.232s
| Adam | epoch: 003 | loss: 0.18434 - acc: 0.9304 -- iter: 09700/28000
Training Step: 1315  | total loss: [1m[32m0.18640[0m[0m | time: 484.519s
| Adam | epoch: 003 | loss: 0.18640 - acc: 0.9294 -- iter: 09750/28000
Training Step: 1316  | total loss: [1m[32m0.18447[0m[0m | time: 485.822s
| Adam | epoch: 003 | loss: 0.18447 - acc: 0.9304 -- iter: 09800/28000
Training Step: 1317  | total loss: [1m[32m0.17821[0m[0m | time: 487.211s
| Adam | epoch: 003 | loss: 0.17821 - acc: 0.9294 -- iter: 09850/28000
Training Step: 1318  | total loss: [1m[32m0.17715[0m[0m | time: 488.491s
| Adam | epoch: 003 | loss: 0.17715 - acc: 0.9304 -- iter: 09900/28000
Training Step: 1319  | total loss: [1m[32m0.16750[0m[0m | time: 489.765s
| Adam | epoch: 003 | loss: 0.16750 - acc: 0.9334 -- iter: 09950/28000
Training Step: 1320  | total loss: [1m[32m0.17230[0m[0m | time: 491.033s
| Adam | epoch: 003 | loss: 0.17230 - acc: 0.9341 -- iter: 10000/28000
Training Step: 1321  | total loss: [1m[32m0.16522[0m[0m | time: 492.307s
| Adam | epoch: 003 | loss: 0.16522 - acc: 0.9366 -- iter: 10050/28000
Training Step: 1322  | total loss: [1m[32m0.16904[0m[0m | time: 493.561s
| Adam | epoch: 003 | loss: 0.16904 - acc: 0.9350 -- iter: 10100/28000
Training Step: 1323  | total loss: [1m[32m0.15939[0m[0m | time: 494.831s
| Adam | epoch: 003 | loss: 0.15939 - acc: 0.9415 -- iter: 10150/28000
Training Step: 1324  | total loss: [1m[32m0.16858[0m[0m | time: 496.084s
| Adam | epoch: 003 | loss: 0.16858 - acc: 0.9413 -- iter: 10200/28000
Training Step: 1325  | total loss: [1m[32m0.17181[0m[0m | time: 526.127s
| Adam | epoch: 003 | loss: 0.17181 - acc: 0.9412 | val_loss: 0.21379 - val_acc: 0.9205 -- iter: 10250/28000
--
Training Step: 1326  | total loss: [1m[32m0.17427[0m[0m | time: 527.498s
| Adam | epoch: 003 | loss: 0.17427 - acc: 0.9351 -- iter: 10300/28000
Training Step: 1327  | total loss: [1m[32m0.18287[0m[0m | time: 528.764s
| Adam | epoch: 003 | loss: 0.18287 - acc: 0.9316 -- iter: 10350/28000
Training Step: 1328  | total loss: [1m[32m0.19749[0m[0m | time: 530.083s
| Adam | epoch: 003 | loss: 0.19749 - acc: 0.9224 -- iter: 10400/28000
Training Step: 1329  | total loss: [1m[32m0.20025[0m[0m | time: 531.370s
| Adam | epoch: 003 | loss: 0.20025 - acc: 0.9202 -- iter: 10450/28000
Training Step: 1330  | total loss: [1m[32m0.20046[0m[0m | time: 532.705s
| Adam | epoch: 003 | loss: 0.20046 - acc: 0.9182 -- iter: 10500/28000
Training Step: 1331  | total loss: [1m[32m0.19308[0m[0m | time: 533.991s
| Adam | epoch: 003 | loss: 0.19308 - acc: 0.9223 -- iter: 10550/28000
Training Step: 1332  | total loss: [1m[32m0.18414[0m[0m | time: 535.269s
| Adam | epoch: 003 | loss: 0.18414 - acc: 0.9241 -- iter: 10600/28000
Training Step: 1333  | total loss: [1m[32m0.17615[0m[0m | time: 536.557s
| Adam | epoch: 003 | loss: 0.17615 - acc: 0.9297 -- iter: 10650/28000
Training Step: 1334  | total loss: [1m[32m0.17464[0m[0m | time: 537.899s
| Adam | epoch: 003 | loss: 0.17464 - acc: 0.9307 -- iter: 10700/28000
Training Step: 1335  | total loss: [1m[32m0.16859[0m[0m | time: 539.225s
| Adam | epoch: 003 | loss: 0.16859 - acc: 0.9337 -- iter: 10750/28000
Training Step: 1336  | total loss: [1m[32m0.16532[0m[0m | time: 540.499s
| Adam | epoch: 003 | loss: 0.16532 - acc: 0.9343 -- iter: 10800/28000
Training Step: 1337  | total loss: [1m[32m0.16121[0m[0m | time: 541.751s
| Adam | epoch: 003 | loss: 0.16121 - acc: 0.9369 -- iter: 10850/28000
Training Step: 1338  | total loss: [1m[32m0.16872[0m[0m | time: 543.073s
| Adam | epoch: 003 | loss: 0.16872 - acc: 0.9372 -- iter: 10900/28000
Training Step: 1339  | total loss: [1m[32m0.16348[0m[0m | time: 544.346s
| Adam | epoch: 003 | loss: 0.16348 - acc: 0.9395 -- iter: 10950/28000
Training Step: 1340  | total loss: [1m[32m0.15997[0m[0m | time: 545.628s
| Adam | epoch: 003 | loss: 0.15997 - acc: 0.9375 -- iter: 11000/28000
Training Step: 1341  | total loss: [1m[32m0.17311[0m[0m | time: 546.889s
| Adam | epoch: 003 | loss: 0.17311 - acc: 0.9358 -- iter: 11050/28000
Training Step: 1342  | total loss: [1m[32m0.17416[0m[0m | time: 548.163s
| Adam | epoch: 003 | loss: 0.17416 - acc: 0.9362 -- iter: 11100/28000
Training Step: 1343  | total loss: [1m[32m0.17391[0m[0m | time: 549.434s
| Adam | epoch: 003 | loss: 0.17391 - acc: 0.9346 -- iter: 11150/28000
Training Step: 1344  | total loss: [1m[32m0.17158[0m[0m | time: 550.717s
| Adam | epoch: 003 | loss: 0.17158 - acc: 0.9331 -- iter: 11200/28000
Training Step: 1345  | total loss: [1m[32m0.17142[0m[0m | time: 552.004s
| Adam | epoch: 003 | loss: 0.17142 - acc: 0.9338 -- iter: 11250/28000
Training Step: 1346  | total loss: [1m[32m0.17522[0m[0m | time: 553.263s
| Adam | epoch: 003 | loss: 0.17522 - acc: 0.9344 -- iter: 11300/28000
Training Step: 1347  | total loss: [1m[32m0.18117[0m[0m | time: 554.539s
| Adam | epoch: 003 | loss: 0.18117 - acc: 0.9330 -- iter: 11350/28000
Training Step: 1348  | total loss: [1m[32m0.18064[0m[0m | time: 555.788s
| Adam | epoch: 003 | loss: 0.18064 - acc: 0.9297 -- iter: 11400/28000
Training Step: 1349  | total loss: [1m[32m0.17861[0m[0m | time: 557.061s
| Adam | epoch: 003 | loss: 0.17861 - acc: 0.9287 -- iter: 11450/28000
Training Step: 1350  | total loss: [1m[32m0.16785[0m[0m | time: 587.248s
| Adam | epoch: 003 | loss: 0.16785 - acc: 0.9338 | val_loss: 0.20890 - val_acc: 0.9237 -- iter: 11500/28000
--
Training Step: 1351  | total loss: [1m[32m0.17141[0m[0m | time: 588.593s
| Adam | epoch: 003 | loss: 0.17141 - acc: 0.9365 -- iter: 11550/28000
Training Step: 1352  | total loss: [1m[32m0.16606[0m[0m | time: 589.870s
| Adam | epoch: 003 | loss: 0.16606 - acc: 0.9368 -- iter: 11600/28000
Training Step: 1353  | total loss: [1m[32m0.16888[0m[0m | time: 591.187s
| Adam | epoch: 003 | loss: 0.16888 - acc: 0.9331 -- iter: 11650/28000
Training Step: 1354  | total loss: [1m[32m0.16280[0m[0m | time: 592.492s
| Adam | epoch: 003 | loss: 0.16280 - acc: 0.9358 -- iter: 11700/28000
Training Step: 1355  | total loss: [1m[32m0.17039[0m[0m | time: 593.765s
| Adam | epoch: 003 | loss: 0.17039 - acc: 0.9382 -- iter: 11750/28000
Training Step: 1356  | total loss: [1m[32m0.16828[0m[0m | time: 595.021s
| Adam | epoch: 003 | loss: 0.16828 - acc: 0.9424 -- iter: 11800/28000
Training Step: 1357  | total loss: [1m[32m0.16419[0m[0m | time: 596.286s
| Adam | epoch: 003 | loss: 0.16419 - acc: 0.9462 -- iter: 11850/28000
Training Step: 1358  | total loss: [1m[32m0.16676[0m[0m | time: 597.630s
| Adam | epoch: 003 | loss: 0.16676 - acc: 0.9456 -- iter: 11900/28000
Training Step: 1359  | total loss: [1m[32m0.17237[0m[0m | time: 598.901s
| Adam | epoch: 003 | loss: 0.17237 - acc: 0.9470 -- iter: 11950/28000
Training Step: 1360  | total loss: [1m[32m0.16991[0m[0m | time: 600.167s
| Adam | epoch: 003 | loss: 0.16991 - acc: 0.9483 -- iter: 12000/28000
Training Step: 1361  | total loss: [1m[32m0.17340[0m[0m | time: 601.446s
| Adam | epoch: 003 | loss: 0.17340 - acc: 0.9455 -- iter: 12050/28000
Training Step: 1362  | total loss: [1m[32m0.16709[0m[0m | time: 602.739s
| Adam | epoch: 003 | loss: 0.16709 - acc: 0.9469 -- iter: 12100/28000
Training Step: 1363  | total loss: [1m[32m0.18573[0m[0m | time: 604.024s
| Adam | epoch: 003 | loss: 0.18573 - acc: 0.9402 -- iter: 12150/28000
Training Step: 1364  | total loss: [1m[32m0.18595[0m[0m | time: 605.290s
| Adam | epoch: 003 | loss: 0.18595 - acc: 0.9362 -- iter: 12200/28000
Training Step: 1365  | total loss: [1m[32m0.17842[0m[0m | time: 606.561s
| Adam | epoch: 003 | loss: 0.17842 - acc: 0.9406 -- iter: 12250/28000
Training Step: 1366  | total loss: [1m[32m0.17381[0m[0m | time: 607.832s
| Adam | epoch: 003 | loss: 0.17381 - acc: 0.9445 -- iter: 12300/28000
Training Step: 1367  | total loss: [1m[32m0.16568[0m[0m | time: 609.116s
| Adam | epoch: 003 | loss: 0.16568 - acc: 0.9461 -- iter: 12350/28000
Training Step: 1368  | total loss: [1m[32m0.15699[0m[0m | time: 610.365s
| Adam | epoch: 003 | loss: 0.15699 - acc: 0.9475 -- iter: 12400/28000
Training Step: 1369  | total loss: [1m[32m0.16675[0m[0m | time: 611.630s
| Adam | epoch: 003 | loss: 0.16675 - acc: 0.9447 -- iter: 12450/28000
Training Step: 1370  | total loss: [1m[32m0.16379[0m[0m | time: 612.924s
| Adam | epoch: 003 | loss: 0.16379 - acc: 0.9462 -- iter: 12500/28000
Training Step: 1371  | total loss: [1m[32m0.15907[0m[0m | time: 614.183s
| Adam | epoch: 003 | loss: 0.15907 - acc: 0.9476 -- iter: 12550/28000
Training Step: 1372  | total loss: [1m[32m0.17283[0m[0m | time: 615.453s
| Adam | epoch: 003 | loss: 0.17283 - acc: 0.9389 -- iter: 12600/28000
Training Step: 1373  | total loss: [1m[32m0.17199[0m[0m | time: 616.743s
| Adam | epoch: 003 | loss: 0.17199 - acc: 0.9390 -- iter: 12650/28000
Training Step: 1374  | total loss: [1m[32m0.16487[0m[0m | time: 618.033s
| Adam | epoch: 003 | loss: 0.16487 - acc: 0.9411 -- iter: 12700/28000
Training Step: 1375  | total loss: [1m[32m0.15853[0m[0m | time: 648.538s
| Adam | epoch: 003 | loss: 0.15853 - acc: 0.9450 | val_loss: 0.18790 - val_acc: 0.9332 -- iter: 12750/28000
--
Training Step: 1376  | total loss: [1m[32m0.16077[0m[0m | time: 649.807s
| Adam | epoch: 003 | loss: 0.16077 - acc: 0.9465 -- iter: 12800/28000
Training Step: 1377  | total loss: [1m[32m0.15687[0m[0m | time: 651.020s
| Adam | epoch: 003 | loss: 0.15687 - acc: 0.9458 -- iter: 12850/28000
Training Step: 1378  | total loss: [1m[32m0.16274[0m[0m | time: 652.238s
| Adam | epoch: 003 | loss: 0.16274 - acc: 0.9452 -- iter: 12900/28000
Training Step: 1379  | total loss: [1m[32m0.15263[0m[0m | time: 653.449s
| Adam | epoch: 003 | loss: 0.15263 - acc: 0.9487 -- iter: 12950/28000
Training Step: 1380  | total loss: [1m[32m0.15803[0m[0m | time: 654.742s
| Adam | epoch: 003 | loss: 0.15803 - acc: 0.9498 -- iter: 13000/28000
Training Step: 1381  | total loss: [1m[32m0.16088[0m[0m | time: 656.045s
| Adam | epoch: 003 | loss: 0.16088 - acc: 0.9469 -- iter: 13050/28000
Training Step: 1382  | total loss: [1m[32m0.16163[0m[0m | time: 657.342s
| Adam | epoch: 003 | loss: 0.16163 - acc: 0.9442 -- iter: 13100/28000
Training Step: 1383  | total loss: [1m[32m0.16212[0m[0m | time: 658.652s
| Adam | epoch: 003 | loss: 0.16212 - acc: 0.9438 -- iter: 13150/28000
Training Step: 1384  | total loss: [1m[32m0.16315[0m[0m | time: 659.945s
| Adam | epoch: 003 | loss: 0.16315 - acc: 0.9474 -- iter: 13200/28000
Training Step: 1385  | total loss: [1m[32m0.17178[0m[0m | time: 661.219s
| Adam | epoch: 003 | loss: 0.17178 - acc: 0.9446 -- iter: 13250/28000
Training Step: 1386  | total loss: [1m[32m0.16733[0m[0m | time: 662.521s
| Adam | epoch: 003 | loss: 0.16733 - acc: 0.9402 -- iter: 13300/28000
Training Step: 1387  | total loss: [1m[32m0.18132[0m[0m | time: 663.808s
| Adam | epoch: 003 | loss: 0.18132 - acc: 0.9342 -- iter: 13350/28000
Training Step: 1388  | total loss: [1m[32m0.18040[0m[0m | time: 665.110s
| Adam | epoch: 003 | loss: 0.18040 - acc: 0.9367 -- iter: 13400/28000
Training Step: 1389  | total loss: [1m[32m0.19791[0m[0m | time: 666.394s
| Adam | epoch: 003 | loss: 0.19791 - acc: 0.9331 -- iter: 13450/28000
Training Step: 1390  | total loss: [1m[32m0.20642[0m[0m | time: 667.679s
| Adam | epoch: 003 | loss: 0.20642 - acc: 0.9318 -- iter: 13500/28000
Training Step: 1391  | total loss: [1m[32m0.20509[0m[0m | time: 668.981s
| Adam | epoch: 003 | loss: 0.20509 - acc: 0.9326 -- iter: 13550/28000
Training Step: 1392  | total loss: [1m[32m0.19851[0m[0m | time: 670.260s
| Adam | epoch: 003 | loss: 0.19851 - acc: 0.9333 -- iter: 13600/28000
Training Step: 1393  | total loss: [1m[32m0.19240[0m[0m | time: 671.643s
| Adam | epoch: 003 | loss: 0.19240 - acc: 0.9340 -- iter: 13650/28000
Training Step: 1394  | total loss: [1m[32m0.19172[0m[0m | time: 672.934s
| Adam | epoch: 003 | loss: 0.19172 - acc: 0.9326 -- iter: 13700/28000
Training Step: 1395  | total loss: [1m[32m0.18865[0m[0m | time: 674.197s
| Adam | epoch: 003 | loss: 0.18865 - acc: 0.9353 -- iter: 13750/28000
Training Step: 1396  | total loss: [1m[32m0.17835[0m[0m | time: 675.458s
| Adam | epoch: 003 | loss: 0.17835 - acc: 0.9378 -- iter: 13800/28000
Training Step: 1397  | total loss: [1m[32m0.18623[0m[0m | time: 676.751s
| Adam | epoch: 003 | loss: 0.18623 - acc: 0.9320 -- iter: 13850/28000
Training Step: 1398  | total loss: [1m[32m0.19045[0m[0m | time: 678.035s
| Adam | epoch: 003 | loss: 0.19045 - acc: 0.9288 -- iter: 13900/28000
Training Step: 1399  | total loss: [1m[32m0.18131[0m[0m | time: 679.312s
| Adam | epoch: 003 | loss: 0.18131 - acc: 0.9339 -- iter: 13950/28000
Training Step: 1400  | total loss: [1m[32m0.17490[0m[0m | time: 710.299s
| Adam | epoch: 003 | loss: 0.17490 - acc: 0.9365 | val_loss: 0.19605 - val_acc: 0.9311 -- iter: 14000/28000
--
Training Step: 1401  | total loss: [1m[32m0.16954[0m[0m | time: 711.584s
| Adam | epoch: 003 | loss: 0.16954 - acc: 0.9389 -- iter: 14050/28000
Training Step: 1402  | total loss: [1m[32m0.15855[0m[0m | time: 712.812s
| Adam | epoch: 003 | loss: 0.15855 - acc: 0.9430 -- iter: 14100/28000
Training Step: 1403  | total loss: [1m[32m0.15879[0m[0m | time: 714.088s
| Adam | epoch: 003 | loss: 0.15879 - acc: 0.9427 -- iter: 14150/28000
Training Step: 1404  | total loss: [1m[32m0.17712[0m[0m | time: 715.350s
| Adam | epoch: 003 | loss: 0.17712 - acc: 0.9364 -- iter: 14200/28000
Training Step: 1405  | total loss: [1m[32m0.16748[0m[0m | time: 716.594s
| Adam | epoch: 003 | loss: 0.16748 - acc: 0.9428 -- iter: 14250/28000
Training Step: 1406  | total loss: [1m[32m0.17555[0m[0m | time: 717.846s
| Adam | epoch: 003 | loss: 0.17555 - acc: 0.9345 -- iter: 14300/28000
Training Step: 1407  | total loss: [1m[32m0.17776[0m[0m | time: 719.111s
| Adam | epoch: 003 | loss: 0.17776 - acc: 0.9331 -- iter: 14350/28000
Training Step: 1408  | total loss: [1m[32m0.16972[0m[0m | time: 720.360s
| Adam | epoch: 003 | loss: 0.16972 - acc: 0.9378 -- iter: 14400/28000
Training Step: 1409  | total loss: [1m[32m0.16676[0m[0m | time: 721.606s
| Adam | epoch: 003 | loss: 0.16676 - acc: 0.9400 -- iter: 14450/28000
Training Step: 1410  | total loss: [1m[32m0.16402[0m[0m | time: 722.884s
| Adam | epoch: 003 | loss: 0.16402 - acc: 0.9440 -- iter: 14500/28000
Training Step: 1411  | total loss: [1m[32m0.15980[0m[0m | time: 724.155s
| Adam | epoch: 003 | loss: 0.15980 - acc: 0.9436 -- iter: 14550/28000
Training Step: 1412  | total loss: [1m[32m0.14732[0m[0m | time: 725.422s
| Adam | epoch: 003 | loss: 0.14732 - acc: 0.9492 -- iter: 14600/28000
Training Step: 1413  | total loss: [1m[32m0.16525[0m[0m | time: 726.668s
| Adam | epoch: 003 | loss: 0.16525 - acc: 0.9443 -- iter: 14650/28000
Training Step: 1414  | total loss: [1m[32m0.16782[0m[0m | time: 727.924s
| Adam | epoch: 003 | loss: 0.16782 - acc: 0.9419 -- iter: 14700/28000
Training Step: 1415  | total loss: [1m[32m0.16346[0m[0m | time: 729.280s
| Adam | epoch: 003 | loss: 0.16346 - acc: 0.9397 -- iter: 14750/28000
Training Step: 1416  | total loss: [1m[32m0.16635[0m[0m | time: 730.564s
| Adam | epoch: 003 | loss: 0.16635 - acc: 0.9357 -- iter: 14800/28000
Training Step: 1417  | total loss: [1m[32m0.16423[0m[0m | time: 731.818s
| Adam | epoch: 003 | loss: 0.16423 - acc: 0.9321 -- iter: 14850/28000
Training Step: 1418  | total loss: [1m[32m0.18185[0m[0m | time: 733.079s
| Adam | epoch: 003 | loss: 0.18185 - acc: 0.9269 -- iter: 14900/28000
Training Step: 1419  | total loss: [1m[32m0.16873[0m[0m | time: 734.349s
| Adam | epoch: 003 | loss: 0.16873 - acc: 0.9342 -- iter: 14950/28000
Training Step: 1420  | total loss: [1m[32m0.15783[0m[0m | time: 735.657s
| Adam | epoch: 003 | loss: 0.15783 - acc: 0.9408 -- iter: 15000/28000
Training Step: 1421  | total loss: [1m[32m0.15347[0m[0m | time: 736.916s
| Adam | epoch: 003 | loss: 0.15347 - acc: 0.9407 -- iter: 15050/28000
Training Step: 1422  | total loss: [1m[32m0.16940[0m[0m | time: 738.189s
| Adam | epoch: 003 | loss: 0.16940 - acc: 0.9387 -- iter: 15100/28000
Training Step: 1423  | total loss: [1m[32m0.17967[0m[0m | time: 739.475s
| Adam | epoch: 003 | loss: 0.17967 - acc: 0.9408 -- iter: 15150/28000
Training Step: 1424  | total loss: [1m[32m0.19606[0m[0m | time: 740.758s
| Adam | epoch: 003 | loss: 0.19606 - acc: 0.9387 -- iter: 15200/28000
Training Step: 1425  | total loss: [1m[32m0.20866[0m[0m | time: 770.892s
| Adam | epoch: 003 | loss: 0.20866 - acc: 0.9348 | val_loss: 0.20014 - val_acc: 0.9287 -- iter: 15250/28000
--
Training Step: 1426  | total loss: [1m[32m0.19583[0m[0m | time: 772.386s
| Adam | epoch: 003 | loss: 0.19583 - acc: 0.9374 -- iter: 15300/28000
Training Step: 1427  | total loss: [1m[32m0.18700[0m[0m | time: 773.772s
| Adam | epoch: 003 | loss: 0.18700 - acc: 0.9396 -- iter: 15350/28000
Training Step: 1428  | total loss: [1m[32m0.18588[0m[0m | time: 775.054s
| Adam | epoch: 003 | loss: 0.18588 - acc: 0.9357 -- iter: 15400/28000
Training Step: 1429  | total loss: [1m[32m0.18808[0m[0m | time: 776.300s
| Adam | epoch: 003 | loss: 0.18808 - acc: 0.9361 -- iter: 15450/28000
Training Step: 1430  | total loss: [1m[32m0.18407[0m[0m | time: 777.554s
| Adam | epoch: 003 | loss: 0.18407 - acc: 0.9385 -- iter: 15500/28000
Training Step: 1431  | total loss: [1m[32m0.17796[0m[0m | time: 778.806s
| Adam | epoch: 003 | loss: 0.17796 - acc: 0.9406 -- iter: 15550/28000
Training Step: 1432  | total loss: [1m[32m0.16745[0m[0m | time: 780.064s
| Adam | epoch: 003 | loss: 0.16745 - acc: 0.9426 -- iter: 15600/28000
Training Step: 1433  | total loss: [1m[32m0.15914[0m[0m | time: 781.324s
| Adam | epoch: 003 | loss: 0.15914 - acc: 0.9463 -- iter: 15650/28000
Training Step: 1434  | total loss: [1m[32m0.17739[0m[0m | time: 782.567s
| Adam | epoch: 003 | loss: 0.17739 - acc: 0.9377 -- iter: 15700/28000
Training Step: 1435  | total loss: [1m[32m0.17839[0m[0m | time: 783.835s
| Adam | epoch: 003 | loss: 0.17839 - acc: 0.9359 -- iter: 15750/28000
Training Step: 1436  | total loss: [1m[32m0.16763[0m[0m | time: 785.168s
| Adam | epoch: 003 | loss: 0.16763 - acc: 0.9423 -- iter: 15800/28000
Training Step: 1437  | total loss: [1m[32m0.16648[0m[0m | time: 786.430s
| Adam | epoch: 003 | loss: 0.16648 - acc: 0.9421 -- iter: 15850/28000
Training Step: 1438  | total loss: [1m[32m0.17455[0m[0m | time: 787.696s
| Adam | epoch: 003 | loss: 0.17455 - acc: 0.9379 -- iter: 15900/28000
Training Step: 1439  | total loss: [1m[32m0.17692[0m[0m | time: 788.923s
| Adam | epoch: 003 | loss: 0.17692 - acc: 0.9381 -- iter: 15950/28000
Training Step: 1440  | total loss: [1m[32m0.16867[0m[0m | time: 790.185s
| Adam | epoch: 003 | loss: 0.16867 - acc: 0.9403 -- iter: 16000/28000
Training Step: 1441  | total loss: [1m[32m0.16357[0m[0m | time: 791.439s
| Adam | epoch: 003 | loss: 0.16357 - acc: 0.9423 -- iter: 16050/28000
Training Step: 1442  | total loss: [1m[32m0.15527[0m[0m | time: 792.681s
| Adam | epoch: 003 | loss: 0.15527 - acc: 0.9460 -- iter: 16100/28000
Training Step: 1443  | total loss: [1m[32m0.17699[0m[0m | time: 793.947s
| Adam | epoch: 003 | loss: 0.17699 - acc: 0.9414 -- iter: 16150/28000
Training Step: 1444  | total loss: [1m[32m0.19711[0m[0m | time: 795.200s
| Adam | epoch: 003 | loss: 0.19711 - acc: 0.9353 -- iter: 16200/28000
Training Step: 1445  | total loss: [1m[32m0.20050[0m[0m | time: 796.447s
| Adam | epoch: 003 | loss: 0.20050 - acc: 0.9338 -- iter: 16250/28000
Training Step: 1446  | total loss: [1m[32m0.19135[0m[0m | time: 797.671s
| Adam | epoch: 003 | loss: 0.19135 - acc: 0.9344 -- iter: 16300/28000
Training Step: 1447  | total loss: [1m[32m0.18980[0m[0m | time: 798.918s
| Adam | epoch: 003 | loss: 0.18980 - acc: 0.9309 -- iter: 16350/28000
Training Step: 1448  | total loss: [1m[32m0.18546[0m[0m | time: 800.166s
| Adam | epoch: 003 | loss: 0.18546 - acc: 0.9318 -- iter: 16400/28000
Training Step: 1449  | total loss: [1m[32m0.19113[0m[0m | time: 801.418s
| Adam | epoch: 003 | loss: 0.19113 - acc: 0.9287 -- iter: 16450/28000
Training Step: 1450  | total loss: [1m[32m0.18616[0m[0m | time: 831.297s
| Adam | epoch: 003 | loss: 0.18616 - acc: 0.9318 | val_loss: 0.21867 - val_acc: 0.9175 -- iter: 16500/28000
--
Training Step: 1451  | total loss: [1m[32m0.18584[0m[0m | time: 832.601s
| Adam | epoch: 003 | loss: 0.18584 - acc: 0.9346 -- iter: 16550/28000
Training Step: 1452  | total loss: [1m[32m0.19180[0m[0m | time: 833.876s
| Adam | epoch: 003 | loss: 0.19180 - acc: 0.9312 -- iter: 16600/28000
Training Step: 1453  | total loss: [1m[32m0.18503[0m[0m | time: 835.137s
| Adam | epoch: 003 | loss: 0.18503 - acc: 0.9340 -- iter: 16650/28000
Training Step: 1454  | total loss: [1m[32m0.18696[0m[0m | time: 836.384s
| Adam | epoch: 003 | loss: 0.18696 - acc: 0.9306 -- iter: 16700/28000
Training Step: 1455  | total loss: [1m[32m0.17996[0m[0m | time: 837.661s
| Adam | epoch: 003 | loss: 0.17996 - acc: 0.9316 -- iter: 16750/28000
Training Step: 1456  | total loss: [1m[32m0.17680[0m[0m | time: 838.921s
| Adam | epoch: 003 | loss: 0.17680 - acc: 0.9304 -- iter: 16800/28000
Training Step: 1457  | total loss: [1m[32m0.18431[0m[0m | time: 840.165s
| Adam | epoch: 003 | loss: 0.18431 - acc: 0.9274 -- iter: 16850/28000
Training Step: 1458  | total loss: [1m[32m0.17830[0m[0m | time: 841.419s
| Adam | epoch: 003 | loss: 0.17830 - acc: 0.9306 -- iter: 16900/28000
Training Step: 1459  | total loss: [1m[32m0.16754[0m[0m | time: 842.667s
| Adam | epoch: 003 | loss: 0.16754 - acc: 0.9356 -- iter: 16950/28000
Training Step: 1460  | total loss: [1m[32m0.16689[0m[0m | time: 843.914s
| Adam | epoch: 003 | loss: 0.16689 - acc: 0.9340 -- iter: 17000/28000
Training Step: 1461  | total loss: [1m[32m0.16328[0m[0m | time: 845.208s
| Adam | epoch: 003 | loss: 0.16328 - acc: 0.9386 -- iter: 17050/28000
Training Step: 1462  | total loss: [1m[32m0.16256[0m[0m | time: 846.506s
| Adam | epoch: 003 | loss: 0.16256 - acc: 0.9348 -- iter: 17100/28000
Training Step: 1463  | total loss: [1m[32m0.16980[0m[0m | time: 847.768s
| Adam | epoch: 003 | loss: 0.16980 - acc: 0.9313 -- iter: 17150/28000
Training Step: 1464  | total loss: [1m[32m0.16904[0m[0m | time: 849.058s
| Adam | epoch: 003 | loss: 0.16904 - acc: 0.9301 -- iter: 17200/28000
Training Step: 1465  | total loss: [1m[32m0.16680[0m[0m | time: 850.372s
| Adam | epoch: 003 | loss: 0.16680 - acc: 0.9291 -- iter: 17250/28000
Training Step: 1466  | total loss: [1m[32m0.15554[0m[0m | time: 851.651s
| Adam | epoch: 003 | loss: 0.15554 - acc: 0.9342 -- iter: 17300/28000
Training Step: 1467  | total loss: [1m[32m0.15244[0m[0m | time: 852.932s
| Adam | epoch: 003 | loss: 0.15244 - acc: 0.9328 -- iter: 17350/28000
Training Step: 1468  | total loss: [1m[32m0.15079[0m[0m | time: 854.209s
| Adam | epoch: 003 | loss: 0.15079 - acc: 0.9355 -- iter: 17400/28000
Training Step: 1469  | total loss: [1m[32m0.14761[0m[0m | time: 855.501s
| Adam | epoch: 003 | loss: 0.14761 - acc: 0.9360 -- iter: 17450/28000
Training Step: 1470  | total loss: [1m[32m0.17040[0m[0m | time: 856.797s
| Adam | epoch: 003 | loss: 0.17040 - acc: 0.9324 -- iter: 17500/28000
Training Step: 1471  | total loss: [1m[32m0.16825[0m[0m | time: 858.069s
| Adam | epoch: 003 | loss: 0.16825 - acc: 0.9311 -- iter: 17550/28000
Training Step: 1472  | total loss: [1m[32m0.16906[0m[0m | time: 859.343s
| Adam | epoch: 003 | loss: 0.16906 - acc: 0.9300 -- iter: 17600/28000
Training Step: 1473  | total loss: [1m[32m0.16882[0m[0m | time: 860.600s
| Adam | epoch: 003 | loss: 0.16882 - acc: 0.9290 -- iter: 17650/28000
Training Step: 1474  | total loss: [1m[32m0.16648[0m[0m | time: 861.862s
| Adam | epoch: 003 | loss: 0.16648 - acc: 0.9321 -- iter: 17700/28000
Training Step: 1475  | total loss: [1m[32m0.16138[0m[0m | time: 891.982s
| Adam | epoch: 003 | loss: 0.16138 - acc: 0.9329 | val_loss: 0.20116 - val_acc: 0.9264 -- iter: 17750/28000
--
Training Step: 1476  | total loss: [1m[32m0.16037[0m[0m | time: 893.283s
| Adam | epoch: 003 | loss: 0.16037 - acc: 0.9316 -- iter: 17800/28000
Training Step: 1477  | total loss: [1m[32m0.18218[0m[0m | time: 894.529s
| Adam | epoch: 003 | loss: 0.18218 - acc: 0.9245 -- iter: 17850/28000
Training Step: 1478  | total loss: [1m[32m0.18189[0m[0m | time: 895.757s
| Adam | epoch: 003 | loss: 0.18189 - acc: 0.9240 -- iter: 17900/28000
Training Step: 1479  | total loss: [1m[32m0.17521[0m[0m | time: 897.001s
| Adam | epoch: 003 | loss: 0.17521 - acc: 0.9276 -- iter: 17950/28000
Training Step: 1480  | total loss: [1m[32m0.17953[0m[0m | time: 898.280s
| Adam | epoch: 003 | loss: 0.17953 - acc: 0.9268 -- iter: 18000/28000
Training Step: 1481  | total loss: [1m[32m0.19783[0m[0m | time: 899.540s
| Adam | epoch: 003 | loss: 0.19783 - acc: 0.9202 -- iter: 18050/28000
Training Step: 1482  | total loss: [1m[32m0.20639[0m[0m | time: 900.799s
| Adam | epoch: 003 | loss: 0.20639 - acc: 0.9181 -- iter: 18100/28000
Training Step: 1483  | total loss: [1m[32m0.19537[0m[0m | time: 902.082s
| Adam | epoch: 003 | loss: 0.19537 - acc: 0.9223 -- iter: 18150/28000
Training Step: 1484  | total loss: [1m[32m0.19400[0m[0m | time: 903.340s
| Adam | epoch: 003 | loss: 0.19400 - acc: 0.9221 -- iter: 18200/28000
Training Step: 1485  | total loss: [1m[32m0.19077[0m[0m | time: 904.614s
| Adam | epoch: 003 | loss: 0.19077 - acc: 0.9239 -- iter: 18250/28000
Training Step: 1486  | total loss: [1m[32m0.17889[0m[0m | time: 905.862s
| Adam | epoch: 003 | loss: 0.17889 - acc: 0.9315 -- iter: 18300/28000
Training Step: 1487  | total loss: [1m[32m0.17545[0m[0m | time: 907.130s
| Adam | epoch: 003 | loss: 0.17545 - acc: 0.9323 -- iter: 18350/28000
Training Step: 1488  | total loss: [1m[32m0.17818[0m[0m | time: 908.390s
| Adam | epoch: 003 | loss: 0.17818 - acc: 0.9271 -- iter: 18400/28000
Training Step: 1489  | total loss: [1m[32m0.19038[0m[0m | time: 909.653s
| Adam | epoch: 003 | loss: 0.19038 - acc: 0.9224 -- iter: 18450/28000
Training Step: 1490  | total loss: [1m[32m0.18348[0m[0m | time: 910.934s
| Adam | epoch: 003 | loss: 0.18348 - acc: 0.9262 -- iter: 18500/28000
Training Step: 1491  | total loss: [1m[32m0.18495[0m[0m | time: 912.212s
| Adam | epoch: 003 | loss: 0.18495 - acc: 0.9235 -- iter: 18550/28000
Training Step: 1492  | total loss: [1m[32m0.18293[0m[0m | time: 913.479s
| Adam | epoch: 003 | loss: 0.18293 - acc: 0.9292 -- iter: 18600/28000
Training Step: 1493  | total loss: [1m[32m0.18554[0m[0m | time: 914.737s
| Adam | epoch: 003 | loss: 0.18554 - acc: 0.9323 -- iter: 18650/28000
Training Step: 1494  | total loss: [1m[32m0.17601[0m[0m | time: 916.000s
| Adam | epoch: 003 | loss: 0.17601 - acc: 0.9370 -- iter: 18700/28000
Training Step: 1495  | total loss: [1m[32m0.16967[0m[0m | time: 917.277s
| Adam | epoch: 003 | loss: 0.16967 - acc: 0.9393 -- iter: 18750/28000
Training Step: 1496  | total loss: [1m[32m0.16645[0m[0m | time: 918.534s
| Adam | epoch: 003 | loss: 0.16645 - acc: 0.9414 -- iter: 18800/28000
Training Step: 1497  | total loss: [1m[32m0.17834[0m[0m | time: 919.800s
| Adam | epoch: 003 | loss: 0.17834 - acc: 0.9373 -- iter: 18850/28000
Training Step: 1498  | total loss: [1m[32m0.16700[0m[0m | time: 921.061s
| Adam | epoch: 003 | loss: 0.16700 - acc: 0.9435 -- iter: 18900/28000
Training Step: 1499  | total loss: [1m[32m0.16649[0m[0m | time: 922.363s
| Adam | epoch: 003 | loss: 0.16649 - acc: 0.9412 -- iter: 18950/28000
Training Step: 1500  | total loss: [1m[32m0.17497[0m[0m | time: 952.603s
| Adam | epoch: 003 | loss: 0.17497 - acc: 0.9411 | val_loss: 0.19290 - val_acc: 0.9321 -- iter: 19000/28000
--
Training Step: 1501  | total loss: [1m[32m0.17917[0m[0m | time: 953.888s
| Adam | epoch: 003 | loss: 0.17917 - acc: 0.9390 -- iter: 19050/28000
Training Step: 1502  | total loss: [1m[32m0.17348[0m[0m | time: 955.167s
| Adam | epoch: 003 | loss: 0.17348 - acc: 0.9411 -- iter: 19100/28000
Training Step: 1503  | total loss: [1m[32m0.16076[0m[0m | time: 956.414s
| Adam | epoch: 003 | loss: 0.16076 - acc: 0.9470 -- iter: 19150/28000
Training Step: 1504  | total loss: [1m[32m0.16179[0m[0m | time: 957.683s
| Adam | epoch: 003 | loss: 0.16179 - acc: 0.9483 -- iter: 19200/28000
Training Step: 1505  | total loss: [1m[32m0.17352[0m[0m | time: 958.969s
| Adam | epoch: 003 | loss: 0.17352 - acc: 0.9454 -- iter: 19250/28000
Training Step: 1506  | total loss: [1m[32m0.17454[0m[0m | time: 960.237s
| Adam | epoch: 003 | loss: 0.17454 - acc: 0.9429 -- iter: 19300/28000
Training Step: 1507  | total loss: [1m[32m0.17961[0m[0m | time: 961.548s
| Adam | epoch: 003 | loss: 0.17961 - acc: 0.9446 -- iter: 19350/28000
Training Step: 1508  | total loss: [1m[32m0.17367[0m[0m | time: 962.817s
| Adam | epoch: 003 | loss: 0.17367 - acc: 0.9461 -- iter: 19400/28000
Training Step: 1509  | total loss: [1m[32m0.16872[0m[0m | time: 964.077s
| Adam | epoch: 003 | loss: 0.16872 - acc: 0.9495 -- iter: 19450/28000
Training Step: 1510  | total loss: [1m[32m0.16434[0m[0m | time: 965.355s
| Adam | epoch: 003 | loss: 0.16434 - acc: 0.9486 -- iter: 19500/28000
Training Step: 1511  | total loss: [1m[32m0.16720[0m[0m | time: 966.635s
| Adam | epoch: 003 | loss: 0.16720 - acc: 0.9457 -- iter: 19550/28000
Training Step: 1512  | total loss: [1m[32m0.16738[0m[0m | time: 967.913s
| Adam | epoch: 003 | loss: 0.16738 - acc: 0.9411 -- iter: 19600/28000
Training Step: 1513  | total loss: [1m[32m0.18047[0m[0m | time: 969.188s
| Adam | epoch: 003 | loss: 0.18047 - acc: 0.9370 -- iter: 19650/28000
Training Step: 1514  | total loss: [1m[32m0.17363[0m[0m | time: 970.472s
| Adam | epoch: 003 | loss: 0.17363 - acc: 0.9373 -- iter: 19700/28000
Training Step: 1515  | total loss: [1m[32m0.16787[0m[0m | time: 971.772s
| Adam | epoch: 003 | loss: 0.16787 - acc: 0.9436 -- iter: 19750/28000
Training Step: 1516  | total loss: [1m[32m0.16883[0m[0m | time: 973.066s
| Adam | epoch: 003 | loss: 0.16883 - acc: 0.9412 -- iter: 19800/28000
Training Step: 1517  | total loss: [1m[32m0.17658[0m[0m | time: 974.388s
| Adam | epoch: 003 | loss: 0.17658 - acc: 0.9391 -- iter: 19850/28000
Training Step: 1518  | total loss: [1m[32m0.17463[0m[0m | time: 975.669s
| Adam | epoch: 003 | loss: 0.17463 - acc: 0.9392 -- iter: 19900/28000
Training Step: 1519  | total loss: [1m[32m0.17138[0m[0m | time: 976.925s
| Adam | epoch: 003 | loss: 0.17138 - acc: 0.9413 -- iter: 19950/28000
Training Step: 1520  | total loss: [1m[32m0.17187[0m[0m | time: 978.219s
| Adam | epoch: 003 | loss: 0.17187 - acc: 0.9452 -- iter: 20000/28000
Training Step: 1521  | total loss: [1m[32m0.15906[0m[0m | time: 979.488s
| Adam | epoch: 003 | loss: 0.15906 - acc: 0.9506 -- iter: 20050/28000
Training Step: 1522  | total loss: [1m[32m0.15194[0m[0m | time: 980.777s
| Adam | epoch: 003 | loss: 0.15194 - acc: 0.9556 -- iter: 20100/28000
Training Step: 1523  | total loss: [1m[32m0.14925[0m[0m | time: 982.058s
| Adam | epoch: 003 | loss: 0.14925 - acc: 0.9560 -- iter: 20150/28000
Training Step: 1524  | total loss: [1m[32m0.14748[0m[0m | time: 983.423s
| Adam | epoch: 003 | loss: 0.14748 - acc: 0.9564 -- iter: 20200/28000
Training Step: 1525  | total loss: [1m[32m0.14015[0m[0m | time: 1013.634s
| Adam | epoch: 003 | loss: 0.14015 - acc: 0.9588 | val_loss: 0.19521 - val_acc: 0.9303 -- iter: 20250/28000
--
Training Step: 1526  | total loss: [1m[32m0.14716[0m[0m | time: 1014.936s
| Adam | epoch: 003 | loss: 0.14716 - acc: 0.9549 -- iter: 20300/28000
Training Step: 1527  | total loss: [1m[32m0.14590[0m[0m | time: 1016.221s
| Adam | epoch: 003 | loss: 0.14590 - acc: 0.9534 -- iter: 20350/28000
Training Step: 1528  | total loss: [1m[32m0.14274[0m[0m | time: 1017.492s
| Adam | epoch: 003 | loss: 0.14274 - acc: 0.9541 -- iter: 20400/28000
Training Step: 1529  | total loss: [1m[32m0.13654[0m[0m | time: 1018.853s
| Adam | epoch: 003 | loss: 0.13654 - acc: 0.9567 -- iter: 20450/28000
Training Step: 1530  | total loss: [1m[32m0.13123[0m[0m | time: 1020.147s
| Adam | epoch: 003 | loss: 0.13123 - acc: 0.9570 -- iter: 20500/28000
Training Step: 1531  | total loss: [1m[32m0.14691[0m[0m | time: 1021.388s
| Adam | epoch: 003 | loss: 0.14691 - acc: 0.9513 -- iter: 20550/28000
Training Step: 1532  | total loss: [1m[32m0.15207[0m[0m | time: 1022.670s
| Adam | epoch: 003 | loss: 0.15207 - acc: 0.9462 -- iter: 20600/28000
Training Step: 1533  | total loss: [1m[32m0.15621[0m[0m | time: 1023.913s
| Adam | epoch: 003 | loss: 0.15621 - acc: 0.9415 -- iter: 20650/28000
Training Step: 1534  | total loss: [1m[32m0.15159[0m[0m | time: 1025.176s
| Adam | epoch: 003 | loss: 0.15159 - acc: 0.9454 -- iter: 20700/28000
Training Step: 1535  | total loss: [1m[32m0.16054[0m[0m | time: 1026.449s
| Adam | epoch: 003 | loss: 0.16054 - acc: 0.9369 -- iter: 20750/28000
Training Step: 1536  | total loss: [1m[32m0.16196[0m[0m | time: 1027.703s
| Adam | epoch: 003 | loss: 0.16196 - acc: 0.9372 -- iter: 20800/28000
Training Step: 1537  | total loss: [1m[32m0.15250[0m[0m | time: 1028.950s
| Adam | epoch: 003 | loss: 0.15250 - acc: 0.9435 -- iter: 20850/28000
Training Step: 1538  | total loss: [1m[32m0.14800[0m[0m | time: 1030.214s
| Adam | epoch: 003 | loss: 0.14800 - acc: 0.9451 -- iter: 20900/28000
Training Step: 1539  | total loss: [1m[32m0.15753[0m[0m | time: 1031.478s
| Adam | epoch: 003 | loss: 0.15753 - acc: 0.9426 -- iter: 20950/28000
Training Step: 1540  | total loss: [1m[32m0.15068[0m[0m | time: 1032.740s
| Adam | epoch: 003 | loss: 0.15068 - acc: 0.9423 -- iter: 21000/28000
Training Step: 1541  | total loss: [1m[32m0.14753[0m[0m | time: 1034.015s
| Adam | epoch: 003 | loss: 0.14753 - acc: 0.9461 -- iter: 21050/28000
Training Step: 1542  | total loss: [1m[32m0.15717[0m[0m | time: 1035.295s
| Adam | epoch: 003 | loss: 0.15717 - acc: 0.9435 -- iter: 21100/28000
Training Step: 1543  | total loss: [1m[32m0.14901[0m[0m | time: 1036.557s
| Adam | epoch: 003 | loss: 0.14901 - acc: 0.9451 -- iter: 21150/28000
Training Step: 1544  | total loss: [1m[32m0.13996[0m[0m | time: 1037.805s
| Adam | epoch: 003 | loss: 0.13996 - acc: 0.9506 -- iter: 21200/28000
Training Step: 1545  | total loss: [1m[32m0.14174[0m[0m | time: 1039.065s
| Adam | epoch: 003 | loss: 0.14174 - acc: 0.9476 -- iter: 21250/28000
Training Step: 1546  | total loss: [1m[32m0.14191[0m[0m | time: 1040.310s
| Adam | epoch: 003 | loss: 0.14191 - acc: 0.9488 -- iter: 21300/28000
Training Step: 1547  | total loss: [1m[32m0.13598[0m[0m | time: 1041.565s
| Adam | epoch: 003 | loss: 0.13598 - acc: 0.9519 -- iter: 21350/28000
Training Step: 1548  | total loss: [1m[32m0.14494[0m[0m | time: 1042.881s
| Adam | epoch: 003 | loss: 0.14494 - acc: 0.9487 -- iter: 21400/28000
Training Step: 1549  | total loss: [1m[32m0.16604[0m[0m | time: 1044.182s
| Adam | epoch: 003 | loss: 0.16604 - acc: 0.9399 -- iter: 21450/28000
Training Step: 1550  | total loss: [1m[32m0.17395[0m[0m | time: 1074.305s
| Adam | epoch: 003 | loss: 0.17395 - acc: 0.9399 | val_loss: 0.20112 - val_acc: 0.9293 -- iter: 21500/28000
--
Training Step: 1551  | total loss: [1m[32m0.17592[0m[0m | time: 1075.686s
| Adam | epoch: 003 | loss: 0.17592 - acc: 0.9339 -- iter: 21550/28000
Training Step: 1552  | total loss: [1m[32m0.17150[0m[0m | time: 1077.003s
| Adam | epoch: 003 | loss: 0.17150 - acc: 0.9325 -- iter: 21600/28000
Training Step: 1553  | total loss: [1m[32m0.16974[0m[0m | time: 1078.312s
| Adam | epoch: 003 | loss: 0.16974 - acc: 0.9332 -- iter: 21650/28000
Training Step: 1554  | total loss: [1m[32m0.17447[0m[0m | time: 1079.642s
| Adam | epoch: 003 | loss: 0.17447 - acc: 0.9319 -- iter: 21700/28000
Training Step: 1555  | total loss: [1m[32m0.16775[0m[0m | time: 1080.984s
| Adam | epoch: 003 | loss: 0.16775 - acc: 0.9347 -- iter: 21750/28000
Training Step: 1556  | total loss: [1m[32m0.16609[0m[0m | time: 1082.322s
| Adam | epoch: 003 | loss: 0.16609 - acc: 0.9393 -- iter: 21800/28000
Training Step: 1557  | total loss: [1m[32m0.17772[0m[0m | time: 1083.633s
| Adam | epoch: 003 | loss: 0.17772 - acc: 0.9333 -- iter: 21850/28000
Training Step: 1558  | total loss: [1m[32m0.17765[0m[0m | time: 1084.921s
| Adam | epoch: 003 | loss: 0.17765 - acc: 0.9340 -- iter: 21900/28000
Training Step: 1559  | total loss: [1m[32m0.18583[0m[0m | time: 1086.181s
| Adam | epoch: 003 | loss: 0.18583 - acc: 0.9366 -- iter: 21950/28000
Training Step: 1560  | total loss: [1m[32m0.18215[0m[0m | time: 1087.436s
| Adam | epoch: 003 | loss: 0.18215 - acc: 0.9349 -- iter: 22000/28000
Training Step: 1561  | total loss: [1m[32m0.18367[0m[0m | time: 1088.670s
| Adam | epoch: 003 | loss: 0.18367 - acc: 0.9374 -- iter: 22050/28000
Training Step: 1562  | total loss: [1m[32m0.17643[0m[0m | time: 1089.923s
| Adam | epoch: 003 | loss: 0.17643 - acc: 0.9397 -- iter: 22100/28000
Training Step: 1563  | total loss: [1m[32m0.17015[0m[0m | time: 1091.181s
| Adam | epoch: 003 | loss: 0.17015 - acc: 0.9397 -- iter: 22150/28000
Training Step: 1564  | total loss: [1m[32m0.16037[0m[0m | time: 1092.438s
| Adam | epoch: 003 | loss: 0.16037 - acc: 0.9458 -- iter: 22200/28000
Training Step: 1565  | total loss: [1m[32m0.16716[0m[0m | time: 1093.703s
| Adam | epoch: 003 | loss: 0.16716 - acc: 0.9432 -- iter: 22250/28000
Training Step: 1566  | total loss: [1m[32m0.16398[0m[0m | time: 1094.971s
| Adam | epoch: 003 | loss: 0.16398 - acc: 0.9409 -- iter: 22300/28000
Training Step: 1567  | total loss: [1m[32m0.16018[0m[0m | time: 1096.220s
| Adam | epoch: 003 | loss: 0.16018 - acc: 0.9448 -- iter: 22350/28000
Training Step: 1568  | total loss: [1m[32m0.16607[0m[0m | time: 1097.491s
| Adam | epoch: 003 | loss: 0.16607 - acc: 0.9403 -- iter: 22400/28000
Training Step: 1569  | total loss: [1m[32m0.17399[0m[0m | time: 1098.748s
| Adam | epoch: 003 | loss: 0.17399 - acc: 0.9383 -- iter: 22450/28000
Training Step: 1570  | total loss: [1m[32m0.16996[0m[0m | time: 1100.005s
| Adam | epoch: 003 | loss: 0.16996 - acc: 0.9424 -- iter: 22500/28000
Training Step: 1571  | total loss: [1m[32m0.15990[0m[0m | time: 1101.244s
| Adam | epoch: 003 | loss: 0.15990 - acc: 0.9442 -- iter: 22550/28000
Training Step: 1572  | total loss: [1m[32m0.16208[0m[0m | time: 1102.540s
| Adam | epoch: 003 | loss: 0.16208 - acc: 0.9458 -- iter: 22600/28000
Training Step: 1573  | total loss: [1m[32m0.15833[0m[0m | time: 1103.802s
| Adam | epoch: 003 | loss: 0.15833 - acc: 0.9472 -- iter: 22650/28000
Training Step: 1574  | total loss: [1m[32m0.14877[0m[0m | time: 1105.031s
| Adam | epoch: 003 | loss: 0.14877 - acc: 0.9525 -- iter: 22700/28000
Training Step: 1575  | total loss: [1m[32m0.15193[0m[0m | time: 1134.993s
| Adam | epoch: 003 | loss: 0.15193 - acc: 0.9512 | val_loss: 0.19553 - val_acc: 0.9297 -- iter: 22750/28000
--
Training Step: 1576  | total loss: [1m[32m0.16351[0m[0m | time: 1136.283s
| Adam | epoch: 003 | loss: 0.16351 - acc: 0.9461 -- iter: 22800/28000
Training Step: 1577  | total loss: [1m[32m0.16739[0m[0m | time: 1137.572s
| Adam | epoch: 003 | loss: 0.16739 - acc: 0.9475 -- iter: 22850/28000
Training Step: 1578  | total loss: [1m[32m0.16919[0m[0m | time: 1138.903s
| Adam | epoch: 003 | loss: 0.16919 - acc: 0.9407 -- iter: 22900/28000
Training Step: 1579  | total loss: [1m[32m0.16491[0m[0m | time: 1140.151s
| Adam | epoch: 003 | loss: 0.16491 - acc: 0.9427 -- iter: 22950/28000
Training Step: 1580  | total loss: [1m[32m0.16288[0m[0m | time: 1141.435s
| Adam | epoch: 003 | loss: 0.16288 - acc: 0.9384 -- iter: 23000/28000
Training Step: 1581  | total loss: [1m[32m0.15648[0m[0m | time: 1142.724s
| Adam | epoch: 003 | loss: 0.15648 - acc: 0.9406 -- iter: 23050/28000
Training Step: 1582  | total loss: [1m[32m0.16403[0m[0m | time: 1144.015s
| Adam | epoch: 003 | loss: 0.16403 - acc: 0.9365 -- iter: 23100/28000
Training Step: 1583  | total loss: [1m[32m0.15849[0m[0m | time: 1145.299s
| Adam | epoch: 003 | loss: 0.15849 - acc: 0.9389 -- iter: 23150/28000
Training Step: 1584  | total loss: [1m[32m0.15358[0m[0m | time: 1146.574s
| Adam | epoch: 003 | loss: 0.15358 - acc: 0.9370 -- iter: 23200/28000
Training Step: 1585  | total loss: [1m[32m0.14700[0m[0m | time: 1147.837s
| Adam | epoch: 003 | loss: 0.14700 - acc: 0.9393 -- iter: 23250/28000
Training Step: 1586  | total loss: [1m[32m0.14368[0m[0m | time: 1149.112s
| Adam | epoch: 003 | loss: 0.14368 - acc: 0.9413 -- iter: 23300/28000
Training Step: 1587  | total loss: [1m[32m0.13579[0m[0m | time: 1150.411s
| Adam | epoch: 003 | loss: 0.13579 - acc: 0.9452 -- iter: 23350/28000
Training Step: 1588  | total loss: [1m[32m0.14702[0m[0m | time: 1151.671s
| Adam | epoch: 003 | loss: 0.14702 - acc: 0.9427 -- iter: 23400/28000
Training Step: 1589  | total loss: [1m[32m0.14491[0m[0m | time: 1152.938s
| Adam | epoch: 003 | loss: 0.14491 - acc: 0.9444 -- iter: 23450/28000
Training Step: 1590  | total loss: [1m[32m0.13365[0m[0m | time: 1154.213s
| Adam | epoch: 003 | loss: 0.13365 - acc: 0.9500 -- iter: 23500/28000
Training Step: 1591  | total loss: [1m[32m0.13017[0m[0m | time: 1155.480s
| Adam | epoch: 003 | loss: 0.13017 - acc: 0.9510 -- iter: 23550/28000
Training Step: 1592  | total loss: [1m[32m0.14610[0m[0m | time: 1156.732s
| Adam | epoch: 003 | loss: 0.14610 - acc: 0.9479 -- iter: 23600/28000
Training Step: 1593  | total loss: [1m[32m0.14771[0m[0m | time: 1158.001s
| Adam | epoch: 003 | loss: 0.14771 - acc: 0.9451 -- iter: 23650/28000
Training Step: 1594  | total loss: [1m[32m0.14072[0m[0m | time: 1159.285s
| Adam | epoch: 003 | loss: 0.14072 - acc: 0.9486 -- iter: 23700/28000
Training Step: 1595  | total loss: [1m[32m0.13611[0m[0m | time: 1160.557s
| Adam | epoch: 003 | loss: 0.13611 - acc: 0.9497 -- iter: 23750/28000
Training Step: 1596  | total loss: [1m[32m0.13317[0m[0m | time: 1161.835s
| Adam | epoch: 003 | loss: 0.13317 - acc: 0.9488 -- iter: 23800/28000
Training Step: 1597  | total loss: [1m[32m0.13621[0m[0m | time: 1163.135s
| Adam | epoch: 003 | loss: 0.13621 - acc: 0.9459 -- iter: 23850/28000
Training Step: 1598  | total loss: [1m[32m0.15418[0m[0m | time: 1164.397s
| Adam | epoch: 003 | loss: 0.15418 - acc: 0.9413 -- iter: 23900/28000
Training Step: 1599  | total loss: [1m[32m0.14296[0m[0m | time: 1165.650s
| Adam | epoch: 003 | loss: 0.14296 - acc: 0.9432 -- iter: 23950/28000
Training Step: 1600  | total loss: [1m[32m0.14674[0m[0m | time: 1195.853s
| Adam | epoch: 003 | loss: 0.14674 - acc: 0.9428 | val_loss: 0.21142 - val_acc: 0.9267 -- iter: 24000/28000
--
Training Step: 1601  | total loss: [1m[32m0.14050[0m[0m | time: 1197.130s
| Adam | epoch: 003 | loss: 0.14050 - acc: 0.9446 -- iter: 24050/28000
Training Step: 1602  | total loss: [1m[32m0.13942[0m[0m | time: 1198.411s
| Adam | epoch: 003 | loss: 0.13942 - acc: 0.9461 -- iter: 24100/28000
Training Step: 1603  | total loss: [1m[32m0.13792[0m[0m | time: 1199.646s
| Adam | epoch: 003 | loss: 0.13792 - acc: 0.9435 -- iter: 24150/28000
Training Step: 1604  | total loss: [1m[32m0.14272[0m[0m | time: 1200.910s
| Adam | epoch: 003 | loss: 0.14272 - acc: 0.9431 -- iter: 24200/28000
Training Step: 1605  | total loss: [1m[32m0.13508[0m[0m | time: 1202.166s
| Adam | epoch: 003 | loss: 0.13508 - acc: 0.9488 -- iter: 24250/28000
Training Step: 1606  | total loss: [1m[32m0.14984[0m[0m | time: 1203.423s
| Adam | epoch: 003 | loss: 0.14984 - acc: 0.9439 -- iter: 24300/28000
Training Step: 1607  | total loss: [1m[32m0.17478[0m[0m | time: 1204.662s
| Adam | epoch: 003 | loss: 0.17478 - acc: 0.9416 -- iter: 24350/28000
Training Step: 1608  | total loss: [1m[32m0.18055[0m[0m | time: 1205.905s
| Adam | epoch: 003 | loss: 0.18055 - acc: 0.9374 -- iter: 24400/28000
Training Step: 1609  | total loss: [1m[32m0.18879[0m[0m | time: 1207.164s
| Adam | epoch: 003 | loss: 0.18879 - acc: 0.9357 -- iter: 24450/28000
Training Step: 1610  | total loss: [1m[32m0.20487[0m[0m | time: 1208.405s
| Adam | epoch: 003 | loss: 0.20487 - acc: 0.9301 -- iter: 24500/28000
Training Step: 1611  | total loss: [1m[32m0.19565[0m[0m | time: 1209.647s
| Adam | epoch: 003 | loss: 0.19565 - acc: 0.9331 -- iter: 24550/28000
Training Step: 1612  | total loss: [1m[32m0.19262[0m[0m | time: 1210.910s
| Adam | epoch: 003 | loss: 0.19262 - acc: 0.9338 -- iter: 24600/28000
Training Step: 1613  | total loss: [1m[32m0.19266[0m[0m | time: 1212.161s
| Adam | epoch: 003 | loss: 0.19266 - acc: 0.9344 -- iter: 24650/28000
Training Step: 1614  | total loss: [1m[32m0.19356[0m[0m | time: 1213.432s
| Adam | epoch: 003 | loss: 0.19356 - acc: 0.9330 -- iter: 24700/28000
Training Step: 1615  | total loss: [1m[32m0.19405[0m[0m | time: 1214.711s
| Adam | epoch: 003 | loss: 0.19405 - acc: 0.9297 -- iter: 24750/28000
Training Step: 1616  | total loss: [1m[32m0.18975[0m[0m | time: 1215.940s
| Adam | epoch: 003 | loss: 0.18975 - acc: 0.9307 -- iter: 24800/28000
Training Step: 1617  | total loss: [1m[32m0.18600[0m[0m | time: 1217.186s
| Adam | epoch: 003 | loss: 0.18600 - acc: 0.9336 -- iter: 24850/28000
Training Step: 1618  | total loss: [1m[32m0.19353[0m[0m | time: 1218.444s
| Adam | epoch: 003 | loss: 0.19353 - acc: 0.9283 -- iter: 24900/28000
Training Step: 1619  | total loss: [1m[32m0.19240[0m[0m | time: 1219.709s
| Adam | epoch: 003 | loss: 0.19240 - acc: 0.9294 -- iter: 24950/28000
Training Step: 1620  | total loss: [1m[32m0.18752[0m[0m | time: 1220.961s
| Adam | epoch: 003 | loss: 0.18752 - acc: 0.9305 -- iter: 25000/28000
Training Step: 1621  | total loss: [1m[32m0.19112[0m[0m | time: 1222.236s
| Adam | epoch: 003 | loss: 0.19112 - acc: 0.9274 -- iter: 25050/28000
Training Step: 1622  | total loss: [1m[32m0.20095[0m[0m | time: 1223.522s
| Adam | epoch: 003 | loss: 0.20095 - acc: 0.9207 -- iter: 25100/28000
Training Step: 1623  | total loss: [1m[32m0.20419[0m[0m | time: 1224.790s
| Adam | epoch: 003 | loss: 0.20419 - acc: 0.9166 -- iter: 25150/28000
Training Step: 1624  | total loss: [1m[32m0.19536[0m[0m | time: 1226.022s
| Adam | epoch: 003 | loss: 0.19536 - acc: 0.9210 -- iter: 25200/28000
Training Step: 1625  | total loss: [1m[32m0.18619[0m[0m | time: 1255.813s
| Adam | epoch: 003 | loss: 0.18619 - acc: 0.9229 | val_loss: 0.19540 - val_acc: 0.9306 -- iter: 25250/28000
--
Training Step: 1626  | total loss: [1m[32m0.17614[0m[0m | time: 1257.104s
| Adam | epoch: 003 | loss: 0.17614 - acc: 0.9286 -- iter: 25300/28000
Training Step: 1627  | total loss: [1m[32m0.16960[0m[0m | time: 1258.355s
| Adam | epoch: 003 | loss: 0.16960 - acc: 0.9317 -- iter: 25350/28000
Training Step: 1628  | total loss: [1m[32m0.18928[0m[0m | time: 1259.615s
| Adam | epoch: 003 | loss: 0.18928 - acc: 0.9226 -- iter: 25400/28000
Training Step: 1629  | total loss: [1m[32m0.19695[0m[0m | time: 1260.854s
| Adam | epoch: 003 | loss: 0.19695 - acc: 0.9223 -- iter: 25450/28000
Training Step: 1630  | total loss: [1m[32m0.19250[0m[0m | time: 1262.111s
| Adam | epoch: 003 | loss: 0.19250 - acc: 0.9241 -- iter: 25500/28000
Training Step: 1631  | total loss: [1m[32m0.18570[0m[0m | time: 1263.410s
| Adam | epoch: 003 | loss: 0.18570 - acc: 0.9277 -- iter: 25550/28000
Training Step: 1632  | total loss: [1m[32m0.17977[0m[0m | time: 1264.643s
| Adam | epoch: 003 | loss: 0.17977 - acc: 0.9289 -- iter: 25600/28000
Training Step: 1633  | total loss: [1m[32m0.16984[0m[0m | time: 1265.904s
| Adam | epoch: 003 | loss: 0.16984 - acc: 0.9340 -- iter: 25650/28000
Training Step: 1634  | total loss: [1m[32m0.17622[0m[0m | time: 1267.175s
| Adam | epoch: 003 | loss: 0.17622 - acc: 0.9326 -- iter: 25700/28000
Training Step: 1635  | total loss: [1m[32m0.18977[0m[0m | time: 1268.433s
| Adam | epoch: 003 | loss: 0.18977 - acc: 0.9273 -- iter: 25750/28000
Training Step: 1636  | total loss: [1m[32m0.17733[0m[0m | time: 1269.697s
| Adam | epoch: 003 | loss: 0.17733 - acc: 0.9326 -- iter: 25800/28000
Training Step: 1637  | total loss: [1m[32m0.17337[0m[0m | time: 1270.947s
| Adam | epoch: 003 | loss: 0.17337 - acc: 0.9353 -- iter: 25850/28000
Training Step: 1638  | total loss: [1m[32m0.16732[0m[0m | time: 1272.206s
| Adam | epoch: 003 | loss: 0.16732 - acc: 0.9398 -- iter: 25900/28000
Training Step: 1639  | total loss: [1m[32m0.16321[0m[0m | time: 1273.495s
| Adam | epoch: 003 | loss: 0.16321 - acc: 0.9398 -- iter: 25950/28000
Training Step: 1640  | total loss: [1m[32m0.17103[0m[0m | time: 1274.755s
| Adam | epoch: 003 | loss: 0.17103 - acc: 0.9398 -- iter: 26000/28000
Training Step: 1641  | total loss: [1m[32m0.16412[0m[0m | time: 1276.002s
| Adam | epoch: 003 | loss: 0.16412 - acc: 0.9419 -- iter: 26050/28000
Training Step: 1642  | total loss: [1m[32m0.17338[0m[0m | time: 1277.262s
| Adam | epoch: 003 | loss: 0.17338 - acc: 0.9417 -- iter: 26100/28000
Training Step: 1643  | total loss: [1m[32m0.16937[0m[0m | time: 1278.548s
| Adam | epoch: 003 | loss: 0.16937 - acc: 0.9435 -- iter: 26150/28000
Training Step: 1644  | total loss: [1m[32m0.15873[0m[0m | time: 1279.805s
| Adam | epoch: 003 | loss: 0.15873 - acc: 0.9472 -- iter: 26200/28000
Training Step: 1645  | total loss: [1m[32m0.15431[0m[0m | time: 1281.068s
| Adam | epoch: 003 | loss: 0.15431 - acc: 0.9484 -- iter: 26250/28000
Training Step: 1646  | total loss: [1m[32m0.14484[0m[0m | time: 1282.369s
| Adam | epoch: 003 | loss: 0.14484 - acc: 0.9516 -- iter: 26300/28000
Training Step: 1647  | total loss: [1m[32m0.13295[0m[0m | time: 1283.639s
| Adam | epoch: 003 | loss: 0.13295 - acc: 0.9564 -- iter: 26350/28000
Training Step: 1648  | total loss: [1m[32m0.13377[0m[0m | time: 1284.910s
| Adam | epoch: 003 | loss: 0.13377 - acc: 0.9568 -- iter: 26400/28000
Training Step: 1649  | total loss: [1m[32m0.13662[0m[0m | time: 1286.174s
| Adam | epoch: 003 | loss: 0.13662 - acc: 0.9511 -- iter: 26450/28000
Training Step: 1650  | total loss: [1m[32m0.15617[0m[0m | time: 1316.174s
| Adam | epoch: 003 | loss: 0.15617 - acc: 0.9440 | val_loss: 0.19957 - val_acc: 0.9252 -- iter: 26500/28000
--
Training Step: 1651  | total loss: [1m[32m0.16578[0m[0m | time: 1317.477s
| Adam | epoch: 003 | loss: 0.16578 - acc: 0.9396 -- iter: 26550/28000
Training Step: 1652  | total loss: [1m[32m0.16109[0m[0m | time: 1318.749s
| Adam | epoch: 003 | loss: 0.16109 - acc: 0.9416 -- iter: 26600/28000
Training Step: 1653  | total loss: [1m[32m0.15627[0m[0m | time: 1320.006s
| Adam | epoch: 003 | loss: 0.15627 - acc: 0.9435 -- iter: 26650/28000
Training Step: 1654  | total loss: [1m[32m0.15756[0m[0m | time: 1321.271s
| Adam | epoch: 003 | loss: 0.15756 - acc: 0.9451 -- iter: 26700/28000
Training Step: 1655  | total loss: [1m[32m0.14922[0m[0m | time: 1322.539s
| Adam | epoch: 003 | loss: 0.14922 - acc: 0.9486 -- iter: 26750/28000
Training Step: 1656  | total loss: [1m[32m0.14278[0m[0m | time: 1323.810s
| Adam | epoch: 003 | loss: 0.14278 - acc: 0.9498 -- iter: 26800/28000
Training Step: 1657  | total loss: [1m[32m0.13257[0m[0m | time: 1325.149s
| Adam | epoch: 003 | loss: 0.13257 - acc: 0.9548 -- iter: 26850/28000
Training Step: 1658  | total loss: [1m[32m0.13944[0m[0m | time: 1326.420s
| Adam | epoch: 003 | loss: 0.13944 - acc: 0.9533 -- iter: 26900/28000
Training Step: 1659  | total loss: [1m[32m0.14574[0m[0m | time: 1327.713s
| Adam | epoch: 003 | loss: 0.14574 - acc: 0.9520 -- iter: 26950/28000
Training Step: 1660  | total loss: [1m[32m0.15393[0m[0m | time: 1328.970s
| Adam | epoch: 003 | loss: 0.15393 - acc: 0.9488 -- iter: 27000/28000
Training Step: 1661  | total loss: [1m[32m0.15250[0m[0m | time: 1330.262s
| Adam | epoch: 003 | loss: 0.15250 - acc: 0.9459 -- iter: 27050/28000
Training Step: 1662  | total loss: [1m[32m0.15292[0m[0m | time: 1331.546s
| Adam | epoch: 003 | loss: 0.15292 - acc: 0.9433 -- iter: 27100/28000
Training Step: 1663  | total loss: [1m[32m0.15243[0m[0m | time: 1332.806s
| Adam | epoch: 003 | loss: 0.15243 - acc: 0.9430 -- iter: 27150/28000
Training Step: 1664  | total loss: [1m[32m0.16677[0m[0m | time: 1334.067s
| Adam | epoch: 003 | loss: 0.16677 - acc: 0.9347 -- iter: 27200/28000
Training Step: 1665  | total loss: [1m[32m0.15823[0m[0m | time: 1335.351s
| Adam | epoch: 003 | loss: 0.15823 - acc: 0.9392 -- iter: 27250/28000
Training Step: 1666  | total loss: [1m[32m0.16329[0m[0m | time: 1336.598s
| Adam | epoch: 003 | loss: 0.16329 - acc: 0.9373 -- iter: 27300/28000
Training Step: 1667  | total loss: [1m[32m0.15871[0m[0m | time: 1337.849s
| Adam | epoch: 003 | loss: 0.15871 - acc: 0.9396 -- iter: 27350/28000
Training Step: 1668  | total loss: [1m[32m0.17537[0m[0m | time: 1339.157s
| Adam | epoch: 003 | loss: 0.17537 - acc: 0.9356 -- iter: 27400/28000
Training Step: 1669  | total loss: [1m[32m0.17869[0m[0m | time: 1340.407s
| Adam | epoch: 003 | loss: 0.17869 - acc: 0.9380 -- iter: 27450/28000
Training Step: 1670  | total loss: [1m[32m0.17178[0m[0m | time: 1341.654s
| Adam | epoch: 003 | loss: 0.17178 - acc: 0.9402 -- iter: 27500/28000
Training Step: 1671  | total loss: [1m[32m0.19493[0m[0m | time: 1342.939s
| Adam | epoch: 003 | loss: 0.19493 - acc: 0.9302 -- iter: 27550/28000
Training Step: 1672  | total loss: [1m[32m0.18533[0m[0m | time: 1344.181s
| Adam | epoch: 003 | loss: 0.18533 - acc: 0.9332 -- iter: 27600/28000
Training Step: 1673  | total loss: [1m[32m0.17673[0m[0m | time: 1345.443s
| Adam | epoch: 003 | loss: 0.17673 - acc: 0.9339 -- iter: 27650/28000
Training Step: 1674  | total loss: [1m[32m0.16490[0m[0m | time: 1346.675s
| Adam | epoch: 003 | loss: 0.16490 - acc: 0.9405 -- iter: 27700/28000
Training Step: 1675  | total loss: [1m[32m0.19106[0m[0m | time: 1376.531s
| Adam | epoch: 003 | loss: 0.19106 - acc: 0.9284 | val_loss: 0.22293 - val_acc: 0.9141 -- iter: 27750/28000
--
Training Step: 1676  | total loss: [1m[32m0.21383[0m[0m | time: 1377.827s
| Adam | epoch: 003 | loss: 0.21383 - acc: 0.9236 -- iter: 27800/28000
Training Step: 1677  | total loss: [1m[32m0.21165[0m[0m | time: 1379.066s
| Adam | epoch: 003 | loss: 0.21165 - acc: 0.9252 -- iter: 27850/28000
Training Step: 1678  | total loss: [1m[32m0.21044[0m[0m | time: 1380.344s
| Adam | epoch: 003 | loss: 0.21044 - acc: 0.9227 -- iter: 27900/28000
Training Step: 1679  | total loss: [1m[32m0.20474[0m[0m | time: 1381.587s
| Adam | epoch: 003 | loss: 0.20474 - acc: 0.9224 -- iter: 27950/28000
Training Step: 1680  | total loss: [1m[32m0.20228[0m[0m | time: 1382.843s
| Adam | epoch: 003 | loss: 0.20228 - acc: 0.9242 -- iter: 28000/28000
Training Step: 1681  | total loss: [1m[32m0.20308[0m[0m | time: 1.261s
| Adam | epoch: 004 | loss: 0.20308 - acc: 0.9238 -- iter: 00050/28000
Training Step: 1682  | total loss: [1m[32m0.20104[0m[0m | time: 2.502s
| Adam | epoch: 004 | loss: 0.20104 - acc: 0.9274 -- iter: 00100/28000
Training Step: 1683  | total loss: [1m[32m0.19884[0m[0m | time: 3.753s
| Adam | epoch: 004 | loss: 0.19884 - acc: 0.9287 -- iter: 00150/28000
Training Step: 1684  | total loss: [1m[32m0.19594[0m[0m | time: 4.998s
| Adam | epoch: 004 | loss: 0.19594 - acc: 0.9298 -- iter: 00200/28000
Training Step: 1685  | total loss: [1m[32m0.19709[0m[0m | time: 6.246s
| Adam | epoch: 004 | loss: 0.19709 - acc: 0.9308 -- iter: 00250/28000
Training Step: 1686  | total loss: [1m[32m0.19708[0m[0m | time: 7.524s
| Adam | epoch: 004 | loss: 0.19708 - acc: 0.9317 -- iter: 00300/28000
Training Step: 1687  | total loss: [1m[32m0.19731[0m[0m | time: 8.780s
| Adam | epoch: 004 | loss: 0.19731 - acc: 0.9306 -- iter: 00350/28000
Training Step: 1688  | total loss: [1m[32m0.19429[0m[0m | time: 10.015s
| Adam | epoch: 004 | loss: 0.19429 - acc: 0.9315 -- iter: 00400/28000
Training Step: 1689  | total loss: [1m[32m0.19653[0m[0m | time: 11.260s
| Adam | epoch: 004 | loss: 0.19653 - acc: 0.9304 -- iter: 00450/28000
Training Step: 1690  | total loss: [1m[32m0.19735[0m[0m | time: 12.518s
| Adam | epoch: 004 | loss: 0.19735 - acc: 0.9293 -- iter: 00500/28000
Training Step: 1691  | total loss: [1m[32m0.19643[0m[0m | time: 13.765s
| Adam | epoch: 004 | loss: 0.19643 - acc: 0.9304 -- iter: 00550/28000
Training Step: 1692  | total loss: [1m[32m0.18509[0m[0m | time: 15.028s
| Adam | epoch: 004 | loss: 0.18509 - acc: 0.9353 -- iter: 00600/28000
Training Step: 1693  | total loss: [1m[32m0.17511[0m[0m | time: 16.328s
| Adam | epoch: 004 | loss: 0.17511 - acc: 0.9378 -- iter: 00650/28000
Training Step: 1694  | total loss: [1m[32m0.17172[0m[0m | time: 17.632s
| Adam | epoch: 004 | loss: 0.17172 - acc: 0.9360 -- iter: 00700/28000
Training Step: 1695  | total loss: [1m[32m0.16457[0m[0m | time: 18.924s
| Adam | epoch: 004 | loss: 0.16457 - acc: 0.9404 -- iter: 00750/28000
Training Step: 1696  | total loss: [1m[32m0.17561[0m[0m | time: 20.229s
| Adam | epoch: 004 | loss: 0.17561 - acc: 0.9344 -- iter: 00800/28000
Training Step: 1697  | total loss: [1m[32m0.17886[0m[0m | time: 21.506s
| Adam | epoch: 004 | loss: 0.17886 - acc: 0.9369 -- iter: 00850/28000
Training Step: 1698  | total loss: [1m[32m0.17377[0m[0m | time: 22.791s
| Adam | epoch: 004 | loss: 0.17377 - acc: 0.9353 -- iter: 00900/28000
Training Step: 1699  | total loss: [1m[32m0.17533[0m[0m | time: 24.067s
| Adam | epoch: 004 | loss: 0.17533 - acc: 0.9317 -- iter: 00950/28000
Training Step: 1700  | total loss: [1m[32m0.17323[0m[0m | time: 53.828s
| Adam | epoch: 004 | loss: 0.17323 - acc: 0.9326 | val_loss: 0.21017 - val_acc: 0.9227 -- iter: 01000/28000
--
Training Step: 1701  | total loss: [1m[32m0.16010[0m[0m | time: 55.123s
| Adam | epoch: 004 | loss: 0.16010 - acc: 0.9393 -- iter: 01050/28000
Training Step: 1702  | total loss: [1m[32m0.15803[0m[0m | time: 56.391s
| Adam | epoch: 004 | loss: 0.15803 - acc: 0.9394 -- iter: 01100/28000
Training Step: 1703  | total loss: [1m[32m0.15488[0m[0m | time: 57.653s
| Adam | epoch: 004 | loss: 0.15488 - acc: 0.9394 -- iter: 01150/28000
Training Step: 1704  | total loss: [1m[32m0.15853[0m[0m | time: 58.922s
| Adam | epoch: 004 | loss: 0.15853 - acc: 0.9375 -- iter: 01200/28000
Training Step: 1705  | total loss: [1m[32m0.16015[0m[0m | time: 60.189s
| Adam | epoch: 004 | loss: 0.16015 - acc: 0.9377 -- iter: 01250/28000
Training Step: 1706  | total loss: [1m[32m0.16039[0m[0m | time: 61.437s
| Adam | epoch: 004 | loss: 0.16039 - acc: 0.9380 -- iter: 01300/28000
Training Step: 1707  | total loss: [1m[32m0.15947[0m[0m | time: 62.690s
| Adam | epoch: 004 | loss: 0.15947 - acc: 0.9342 -- iter: 01350/28000
Training Step: 1708  | total loss: [1m[32m0.15440[0m[0m | time: 63.964s
| Adam | epoch: 004 | loss: 0.15440 - acc: 0.9388 -- iter: 01400/28000
Training Step: 1709  | total loss: [1m[32m0.17829[0m[0m | time: 65.219s
| Adam | epoch: 004 | loss: 0.17829 - acc: 0.9369 -- iter: 01450/28000
Training Step: 1710  | total loss: [1m[32m0.17255[0m[0m | time: 66.471s
| Adam | epoch: 004 | loss: 0.17255 - acc: 0.9392 -- iter: 01500/28000
Training Step: 1711  | total loss: [1m[32m0.17606[0m[0m | time: 67.745s
| Adam | epoch: 004 | loss: 0.17606 - acc: 0.9373 -- iter: 01550/28000
Training Step: 1712  | total loss: [1m[32m0.16695[0m[0m | time: 69.006s
| Adam | epoch: 004 | loss: 0.16695 - acc: 0.9395 -- iter: 01600/28000
Training Step: 1713  | total loss: [1m[32m0.16459[0m[0m | time: 70.271s
| Adam | epoch: 004 | loss: 0.16459 - acc: 0.9416 -- iter: 01650/28000
Training Step: 1714  | total loss: [1m[32m0.15793[0m[0m | time: 71.553s
| Adam | epoch: 004 | loss: 0.15793 - acc: 0.9434 -- iter: 01700/28000
Training Step: 1715  | total loss: [1m[32m0.16018[0m[0m | time: 72.821s
| Adam | epoch: 004 | loss: 0.16018 - acc: 0.9431 -- iter: 01750/28000
Training Step: 1716  | total loss: [1m[32m0.16941[0m[0m | time: 74.076s
| Adam | epoch: 004 | loss: 0.16941 - acc: 0.9388 -- iter: 01800/28000
Training Step: 1717  | total loss: [1m[32m0.16384[0m[0m | time: 75.342s
| Adam | epoch: 004 | loss: 0.16384 - acc: 0.9409 -- iter: 01850/28000
Training Step: 1718  | total loss: [1m[32m0.17933[0m[0m | time: 76.629s
| Adam | epoch: 004 | loss: 0.17933 - acc: 0.9388 -- iter: 01900/28000
Training Step: 1719  | total loss: [1m[32m0.17834[0m[0m | time: 77.908s
| Adam | epoch: 004 | loss: 0.17834 - acc: 0.9369 -- iter: 01950/28000
Training Step: 1720  | total loss: [1m[32m0.17093[0m[0m | time: 79.251s
| Adam | epoch: 004 | loss: 0.17093 - acc: 0.9392 -- iter: 02000/28000
Training Step: 1721  | total loss: [1m[32m0.17383[0m[0m | time: 80.552s
| Adam | epoch: 004 | loss: 0.17383 - acc: 0.9393 -- iter: 02050/28000
Training Step: 1722  | total loss: [1m[32m0.16604[0m[0m | time: 81.853s
| Adam | epoch: 004 | loss: 0.16604 - acc: 0.9414 -- iter: 02100/28000
Training Step: 1723  | total loss: [1m[32m0.17749[0m[0m | time: 83.147s
| Adam | epoch: 004 | loss: 0.17749 - acc: 0.9372 -- iter: 02150/28000
Training Step: 1724  | total loss: [1m[32m0.17431[0m[0m | time: 84.449s
| Adam | epoch: 004 | loss: 0.17431 - acc: 0.9355 -- iter: 02200/28000
Training Step: 1725  | total loss: [1m[32m0.16684[0m[0m | time: 114.810s
| Adam | epoch: 004 | loss: 0.16684 - acc: 0.9400 | val_loss: 0.19195 - val_acc: 0.9315 -- iter: 02250/28000
--
Training Step: 1726  | total loss: [1m[32m0.16801[0m[0m | time: 116.129s
| Adam | epoch: 004 | loss: 0.16801 - acc: 0.9380 -- iter: 02300/28000
Training Step: 1727  | total loss: [1m[32m0.18155[0m[0m | time: 117.426s
| Adam | epoch: 004 | loss: 0.18155 - acc: 0.9362 -- iter: 02350/28000
Training Step: 1728  | total loss: [1m[32m0.18202[0m[0m | time: 118.702s
| Adam | epoch: 004 | loss: 0.18202 - acc: 0.9346 -- iter: 02400/28000
Training Step: 1729  | total loss: [1m[32m0.17836[0m[0m | time: 119.998s
| Adam | epoch: 004 | loss: 0.17836 - acc: 0.9351 -- iter: 02450/28000
Training Step: 1730  | total loss: [1m[32m0.18334[0m[0m | time: 121.253s
| Adam | epoch: 004 | loss: 0.18334 - acc: 0.9316 -- iter: 02500/28000
Training Step: 1731  | total loss: [1m[32m0.18437[0m[0m | time: 122.521s
| Adam | epoch: 004 | loss: 0.18437 - acc: 0.9324 -- iter: 02550/28000
Training Step: 1732  | total loss: [1m[32m0.18534[0m[0m | time: 123.901s
| Adam | epoch: 004 | loss: 0.18534 - acc: 0.9312 -- iter: 02600/28000
Training Step: 1733  | total loss: [1m[32m0.19013[0m[0m | time: 125.145s
| Adam | epoch: 004 | loss: 0.19013 - acc: 0.9281 -- iter: 02650/28000
Training Step: 1734  | total loss: [1m[32m0.18451[0m[0m | time: 126.425s
| Adam | epoch: 004 | loss: 0.18451 - acc: 0.9293 -- iter: 02700/28000
Training Step: 1735  | total loss: [1m[32m0.17975[0m[0m | time: 127.701s
| Adam | epoch: 004 | loss: 0.17975 - acc: 0.9303 -- iter: 02750/28000
Training Step: 1736  | total loss: [1m[32m0.17707[0m[0m | time: 128.957s
| Adam | epoch: 004 | loss: 0.17707 - acc: 0.9313 -- iter: 02800/28000
Training Step: 1737  | total loss: [1m[32m0.17046[0m[0m | time: 130.229s
| Adam | epoch: 004 | loss: 0.17046 - acc: 0.9322 -- iter: 02850/28000
Training Step: 1738  | total loss: [1m[32m0.16718[0m[0m | time: 131.510s
| Adam | epoch: 004 | loss: 0.16718 - acc: 0.9350 -- iter: 02900/28000
Training Step: 1739  | total loss: [1m[32m0.16765[0m[0m | time: 132.785s
| Adam | epoch: 004 | loss: 0.16765 - acc: 0.9355 -- iter: 02950/28000
Training Step: 1740  | total loss: [1m[32m0.17178[0m[0m | time: 134.063s
| Adam | epoch: 004 | loss: 0.17178 - acc: 0.9339 -- iter: 03000/28000
Training Step: 1741  | total loss: [1m[32m0.16476[0m[0m | time: 135.352s
| Adam | epoch: 004 | loss: 0.16476 - acc: 0.9345 -- iter: 03050/28000
Training Step: 1742  | total loss: [1m[32m0.16833[0m[0m | time: 136.641s
| Adam | epoch: 004 | loss: 0.16833 - acc: 0.9351 -- iter: 03100/28000
Training Step: 1743  | total loss: [1m[32m0.17050[0m[0m | time: 137.872s
| Adam | epoch: 004 | loss: 0.17050 - acc: 0.9336 -- iter: 03150/28000
Training Step: 1744  | total loss: [1m[32m0.17094[0m[0m | time: 139.136s
| Adam | epoch: 004 | loss: 0.17094 - acc: 0.9302 -- iter: 03200/28000
Training Step: 1745  | total loss: [1m[32m0.16544[0m[0m | time: 140.479s
| Adam | epoch: 004 | loss: 0.16544 - acc: 0.9332 -- iter: 03250/28000
Training Step: 1746  | total loss: [1m[32m0.16571[0m[0m | time: 141.753s
| Adam | epoch: 004 | loss: 0.16571 - acc: 0.9319 -- iter: 03300/28000
Training Step: 1747  | total loss: [1m[32m0.18090[0m[0m | time: 143.051s
| Adam | epoch: 004 | loss: 0.18090 - acc: 0.9287 -- iter: 03350/28000
Training Step: 1748  | total loss: [1m[32m0.18308[0m[0m | time: 144.366s
| Adam | epoch: 004 | loss: 0.18308 - acc: 0.9318 -- iter: 03400/28000
Training Step: 1749  | total loss: [1m[32m0.18519[0m[0m | time: 145.638s
| Adam | epoch: 004 | loss: 0.18519 - acc: 0.9286 -- iter: 03450/28000
Training Step: 1750  | total loss: [1m[32m0.18652[0m[0m | time: 175.896s
| Adam | epoch: 004 | loss: 0.18652 - acc: 0.9278 | val_loss: 0.19634 - val_acc: 0.9279 -- iter: 03500/28000
--
Training Step: 1751  | total loss: [1m[32m0.17599[0m[0m | time: 177.191s
| Adam | epoch: 004 | loss: 0.17599 - acc: 0.9350 -- iter: 03550/28000
Training Step: 1752  | total loss: [1m[32m0.17454[0m[0m | time: 178.429s
| Adam | epoch: 004 | loss: 0.17454 - acc: 0.9355 -- iter: 03600/28000
Training Step: 1753  | total loss: [1m[32m0.18189[0m[0m | time: 179.700s
| Adam | epoch: 004 | loss: 0.18189 - acc: 0.9359 -- iter: 03650/28000
Training Step: 1754  | total loss: [1m[32m0.17278[0m[0m | time: 180.921s
| Adam | epoch: 004 | loss: 0.17278 - acc: 0.9403 -- iter: 03700/28000
Training Step: 1755  | total loss: [1m[32m0.18805[0m[0m | time: 182.181s
| Adam | epoch: 004 | loss: 0.18805 - acc: 0.9323 -- iter: 03750/28000
Training Step: 1756  | total loss: [1m[32m0.19366[0m[0m | time: 183.441s
| Adam | epoch: 004 | loss: 0.19366 - acc: 0.9311 -- iter: 03800/28000
Training Step: 1757  | total loss: [1m[32m0.19669[0m[0m | time: 184.711s
| Adam | epoch: 004 | loss: 0.19669 - acc: 0.9300 -- iter: 03850/28000
Training Step: 1758  | total loss: [1m[32m0.19163[0m[0m | time: 185.971s
| Adam | epoch: 004 | loss: 0.19163 - acc: 0.9270 -- iter: 03900/28000
Training Step: 1759  | total loss: [1m[32m0.19663[0m[0m | time: 187.241s
| Adam | epoch: 004 | loss: 0.19663 - acc: 0.9303 -- iter: 03950/28000
Training Step: 1760  | total loss: [1m[32m0.19075[0m[0m | time: 188.524s
| Adam | epoch: 004 | loss: 0.19075 - acc: 0.9333 -- iter: 04000/28000
Training Step: 1761  | total loss: [1m[32m0.20827[0m[0m | time: 189.776s
| Adam | epoch: 004 | loss: 0.20827 - acc: 0.9239 -- iter: 04050/28000
Training Step: 1762  | total loss: [1m[32m0.20758[0m[0m | time: 191.039s
| Adam | epoch: 004 | loss: 0.20758 - acc: 0.9235 -- iter: 04100/28000
Training Step: 1763  | total loss: [1m[32m0.20480[0m[0m | time: 192.298s
| Adam | epoch: 004 | loss: 0.20480 - acc: 0.9232 -- iter: 04150/28000
Training Step: 1764  | total loss: [1m[32m0.20408[0m[0m | time: 193.534s
| Adam | epoch: 004 | loss: 0.20408 - acc: 0.9289 -- iter: 04200/28000
Training Step: 1765  | total loss: [1m[32m0.20023[0m[0m | time: 194.788s
| Adam | epoch: 004 | loss: 0.20023 - acc: 0.9300 -- iter: 04250/28000
Training Step: 1766  | total loss: [1m[32m0.19879[0m[0m | time: 196.039s
| Adam | epoch: 004 | loss: 0.19879 - acc: 0.9290 -- iter: 04300/28000
Training Step: 1767  | total loss: [1m[32m0.19202[0m[0m | time: 197.266s
| Adam | epoch: 004 | loss: 0.19202 - acc: 0.9341 -- iter: 04350/28000
Training Step: 1768  | total loss: [1m[32m0.18805[0m[0m | time: 198.558s
| Adam | epoch: 004 | loss: 0.18805 - acc: 0.9347 -- iter: 04400/28000
Training Step: 1769  | total loss: [1m[32m0.18737[0m[0m | time: 199.860s
| Adam | epoch: 004 | loss: 0.18737 - acc: 0.9352 -- iter: 04450/28000
Training Step: 1770  | total loss: [1m[32m0.18937[0m[0m | time: 201.097s
| Adam | epoch: 004 | loss: 0.18937 - acc: 0.9357 -- iter: 04500/28000
Training Step: 1771  | total loss: [1m[32m0.19214[0m[0m | time: 202.331s
| Adam | epoch: 004 | loss: 0.19214 - acc: 0.9341 -- iter: 04550/28000
Training Step: 1772  | total loss: [1m[32m0.18352[0m[0m | time: 203.560s
| Adam | epoch: 004 | loss: 0.18352 - acc: 0.9367 -- iter: 04600/28000
Training Step: 1773  | total loss: [1m[32m0.18749[0m[0m | time: 204.754s
| Adam | epoch: 004 | loss: 0.18749 - acc: 0.9370 -- iter: 04650/28000
Training Step: 1774  | total loss: [1m[32m0.18605[0m[0m | time: 205.972s
| Adam | epoch: 004 | loss: 0.18605 - acc: 0.9413 -- iter: 04700/28000
Training Step: 1775  | total loss: [1m[32m0.18291[0m[0m | time: 236.124s
| Adam | epoch: 004 | loss: 0.18291 - acc: 0.9412 | val_loss: 0.18570 - val_acc: 0.9328 -- iter: 04750/28000
--
Training Step: 1776  | total loss: [1m[32m0.18117[0m[0m | time: 237.433s
| Adam | epoch: 004 | loss: 0.18117 - acc: 0.9391 -- iter: 04800/28000
Training Step: 1777  | total loss: [1m[32m0.18036[0m[0m | time: 238.694s
| Adam | epoch: 004 | loss: 0.18036 - acc: 0.9392 -- iter: 04850/28000
Training Step: 1778  | total loss: [1m[32m0.17353[0m[0m | time: 239.951s
| Adam | epoch: 004 | loss: 0.17353 - acc: 0.9393 -- iter: 04900/28000
Training Step: 1779  | total loss: [1m[32m0.17157[0m[0m | time: 241.197s
| Adam | epoch: 004 | loss: 0.17157 - acc: 0.9373 -- iter: 04950/28000
Training Step: 1780  | total loss: [1m[32m0.16714[0m[0m | time: 242.457s
| Adam | epoch: 004 | loss: 0.16714 - acc: 0.9376 -- iter: 05000/28000
Training Step: 1781  | total loss: [1m[32m0.16096[0m[0m | time: 243.726s
| Adam | epoch: 004 | loss: 0.16096 - acc: 0.9418 -- iter: 05050/28000
Training Step: 1782  | total loss: [1m[32m0.15320[0m[0m | time: 244.968s
| Adam | epoch: 004 | loss: 0.15320 - acc: 0.9457 -- iter: 05100/28000
Training Step: 1783  | total loss: [1m[32m0.15118[0m[0m | time: 246.223s
| Adam | epoch: 004 | loss: 0.15118 - acc: 0.9451 -- iter: 05150/28000
Training Step: 1784  | total loss: [1m[32m0.16489[0m[0m | time: 247.466s
| Adam | epoch: 004 | loss: 0.16489 - acc: 0.9426 -- iter: 05200/28000
Training Step: 1785  | total loss: [1m[32m0.15742[0m[0m | time: 248.711s
| Adam | epoch: 004 | loss: 0.15742 - acc: 0.9463 -- iter: 05250/28000
Training Step: 1786  | total loss: [1m[32m0.18102[0m[0m | time: 249.967s
| Adam | epoch: 004 | loss: 0.18102 - acc: 0.9437 -- iter: 05300/28000
Training Step: 1787  | total loss: [1m[32m0.16970[0m[0m | time: 251.213s
| Adam | epoch: 004 | loss: 0.16970 - acc: 0.9473 -- iter: 05350/28000
Training Step: 1788  | total loss: [1m[32m0.16031[0m[0m | time: 252.456s
| Adam | epoch: 004 | loss: 0.16031 - acc: 0.9486 -- iter: 05400/28000
Training Step: 1789  | total loss: [1m[32m0.15950[0m[0m | time: 253.686s
| Adam | epoch: 004 | loss: 0.15950 - acc: 0.9477 -- iter: 05450/28000
Training Step: 1790  | total loss: [1m[32m0.15126[0m[0m | time: 254.929s
| Adam | epoch: 004 | loss: 0.15126 - acc: 0.9510 -- iter: 05500/28000
Training Step: 1791  | total loss: [1m[32m0.16446[0m[0m | time: 256.158s
| Adam | epoch: 004 | loss: 0.16446 - acc: 0.9499 -- iter: 05550/28000
Training Step: 1792  | total loss: [1m[32m0.15301[0m[0m | time: 257.390s
| Adam | epoch: 004 | loss: 0.15301 - acc: 0.9549 -- iter: 05600/28000
Training Step: 1793  | total loss: [1m[32m0.14976[0m[0m | time: 258.628s
| Adam | epoch: 004 | loss: 0.14976 - acc: 0.9554 -- iter: 05650/28000
Training Step: 1794  | total loss: [1m[32m0.15583[0m[0m | time: 259.897s
| Adam | epoch: 004 | loss: 0.15583 - acc: 0.9538 -- iter: 05700/28000
Training Step: 1795  | total loss: [1m[32m0.15559[0m[0m | time: 261.116s
| Adam | epoch: 004 | loss: 0.15559 - acc: 0.9545 -- iter: 05750/28000
Training Step: 1796  | total loss: [1m[32m0.17262[0m[0m | time: 262.330s
| Adam | epoch: 004 | loss: 0.17262 - acc: 0.9490 -- iter: 05800/28000
Training Step: 1797  | total loss: [1m[32m0.16876[0m[0m | time: 263.571s
| Adam | epoch: 004 | loss: 0.16876 - acc: 0.9481 -- iter: 05850/28000
Training Step: 1798  | total loss: [1m[32m0.17282[0m[0m | time: 264.800s
| Adam | epoch: 004 | loss: 0.17282 - acc: 0.9453 -- iter: 05900/28000
Training Step: 1799  | total loss: [1m[32m0.18249[0m[0m | time: 266.146s
| Adam | epoch: 004 | loss: 0.18249 - acc: 0.9428 -- iter: 05950/28000
Training Step: 1800  | total loss: [1m[32m0.19951[0m[0m | time: 296.182s
| Adam | epoch: 004 | loss: 0.19951 - acc: 0.9325 | val_loss: 0.20471 - val_acc: 0.9237 -- iter: 06000/28000
--
Training Step: 1801  | total loss: [1m[32m0.20895[0m[0m | time: 297.549s
| Adam | epoch: 004 | loss: 0.20895 - acc: 0.9272 -- iter: 06050/28000
Training Step: 1802  | total loss: [1m[32m0.20882[0m[0m | time: 298.896s
| Adam | epoch: 004 | loss: 0.20882 - acc: 0.9265 -- iter: 06100/28000
Training Step: 1803  | total loss: [1m[32m0.19457[0m[0m | time: 300.262s
| Adam | epoch: 004 | loss: 0.19457 - acc: 0.9339 -- iter: 06150/28000
Training Step: 1804  | total loss: [1m[32m0.18083[0m[0m | time: 301.630s
| Adam | epoch: 004 | loss: 0.18083 - acc: 0.9405 -- iter: 06200/28000
Training Step: 1805  | total loss: [1m[32m0.17230[0m[0m | time: 302.983s
| Adam | epoch: 004 | loss: 0.17230 - acc: 0.9424 -- iter: 06250/28000
Training Step: 1806  | total loss: [1m[32m0.16908[0m[0m | time: 304.314s
| Adam | epoch: 004 | loss: 0.16908 - acc: 0.9422 -- iter: 06300/28000
Training Step: 1807  | total loss: [1m[32m0.16972[0m[0m | time: 305.656s
| Adam | epoch: 004 | loss: 0.16972 - acc: 0.9460 -- iter: 06350/28000
Training Step: 1808  | total loss: [1m[32m0.17139[0m[0m | time: 307.010s
| Adam | epoch: 004 | loss: 0.17139 - acc: 0.9474 -- iter: 06400/28000
Training Step: 1809  | total loss: [1m[32m0.17484[0m[0m | time: 308.283s
| Adam | epoch: 004 | loss: 0.17484 - acc: 0.9426 -- iter: 06450/28000
Training Step: 1810  | total loss: [1m[32m0.17300[0m[0m | time: 309.538s
| Adam | epoch: 004 | loss: 0.17300 - acc: 0.9424 -- iter: 06500/28000
Training Step: 1811  | total loss: [1m[32m0.18035[0m[0m | time: 310.782s
| Adam | epoch: 004 | loss: 0.18035 - acc: 0.9361 -- iter: 06550/28000
Training Step: 1812  | total loss: [1m[32m0.18038[0m[0m | time: 312.055s
| Adam | epoch: 004 | loss: 0.18038 - acc: 0.9345 -- iter: 06600/28000
Training Step: 1813  | total loss: [1m[32m0.19137[0m[0m | time: 313.338s
| Adam | epoch: 004 | loss: 0.19137 - acc: 0.9291 -- iter: 06650/28000
Training Step: 1814  | total loss: [1m[32m0.18613[0m[0m | time: 314.615s
| Adam | epoch: 004 | loss: 0.18613 - acc: 0.9322 -- iter: 06700/28000
Training Step: 1815  | total loss: [1m[32m0.17324[0m[0m | time: 315.891s
| Adam | epoch: 004 | loss: 0.17324 - acc: 0.9389 -- iter: 06750/28000
Training Step: 1816  | total loss: [1m[32m0.17620[0m[0m | time: 317.152s
| Adam | epoch: 004 | loss: 0.17620 - acc: 0.9371 -- iter: 06800/28000
Training Step: 1817  | total loss: [1m[32m0.17659[0m[0m | time: 318.440s
| Adam | epoch: 004 | loss: 0.17659 - acc: 0.9333 -- iter: 06850/28000
Training Step: 1818  | total loss: [1m[32m0.19295[0m[0m | time: 319.760s
| Adam | epoch: 004 | loss: 0.19295 - acc: 0.9300 -- iter: 06900/28000
Training Step: 1819  | total loss: [1m[32m0.19863[0m[0m | time: 321.041s
| Adam | epoch: 004 | loss: 0.19863 - acc: 0.9270 -- iter: 06950/28000
Training Step: 1820  | total loss: [1m[32m0.18946[0m[0m | time: 322.290s
| Adam | epoch: 004 | loss: 0.18946 - acc: 0.9303 -- iter: 07000/28000
Training Step: 1821  | total loss: [1m[32m0.22195[0m[0m | time: 323.560s
| Adam | epoch: 004 | loss: 0.22195 - acc: 0.9213 -- iter: 07050/28000
Training Step: 1822  | total loss: [1m[32m0.21842[0m[0m | time: 324.806s
| Adam | epoch: 004 | loss: 0.21842 - acc: 0.9232 -- iter: 07100/28000
Training Step: 1823  | total loss: [1m[32m0.21381[0m[0m | time: 326.059s
| Adam | epoch: 004 | loss: 0.21381 - acc: 0.9188 -- iter: 07150/28000
Training Step: 1824  | total loss: [1m[32m0.21113[0m[0m | time: 327.330s
| Adam | epoch: 004 | loss: 0.21113 - acc: 0.9190 -- iter: 07200/28000
Training Step: 1825  | total loss: [1m[32m0.21370[0m[0m | time: 357.543s
| Adam | epoch: 004 | loss: 0.21370 - acc: 0.9111 | val_loss: 0.18824 - val_acc: 0.9321 -- iter: 07250/28000
--
Training Step: 1826  | total loss: [1m[32m0.20548[0m[0m | time: 358.791s
| Adam | epoch: 004 | loss: 0.20548 - acc: 0.9140 -- iter: 07300/28000
Training Step: 1827  | total loss: [1m[32m0.19501[0m[0m | time: 360.055s
| Adam | epoch: 004 | loss: 0.19501 - acc: 0.9206 -- iter: 07350/28000
Training Step: 1828  | total loss: [1m[32m0.18528[0m[0m | time: 361.328s
| Adam | epoch: 004 | loss: 0.18528 - acc: 0.9245 -- iter: 07400/28000
Training Step: 1829  | total loss: [1m[32m0.18487[0m[0m | time: 362.576s
| Adam | epoch: 004 | loss: 0.18487 - acc: 0.9261 -- iter: 07450/28000
Training Step: 1830  | total loss: [1m[32m0.18610[0m[0m | time: 363.805s
| Adam | epoch: 004 | loss: 0.18610 - acc: 0.9274 -- iter: 07500/28000
Training Step: 1831  | total loss: [1m[32m0.18195[0m[0m | time: 365.073s
| Adam | epoch: 004 | loss: 0.18195 - acc: 0.9307 -- iter: 07550/28000
Training Step: 1832  | total loss: [1m[32m0.18459[0m[0m | time: 366.314s
| Adam | epoch: 004 | loss: 0.18459 - acc: 0.9256 -- iter: 07600/28000
Training Step: 1833  | total loss: [1m[32m0.18214[0m[0m | time: 367.558s
| Adam | epoch: 004 | loss: 0.18214 - acc: 0.9251 -- iter: 07650/28000
Training Step: 1834  | total loss: [1m[32m0.17447[0m[0m | time: 368.829s
| Adam | epoch: 004 | loss: 0.17447 - acc: 0.9286 -- iter: 07700/28000
Training Step: 1835  | total loss: [1m[32m0.18046[0m[0m | time: 370.112s
| Adam | epoch: 004 | loss: 0.18046 - acc: 0.9257 -- iter: 07750/28000
Training Step: 1836  | total loss: [1m[32m0.19272[0m[0m | time: 371.392s
| Adam | epoch: 004 | loss: 0.19272 - acc: 0.9271 -- iter: 07800/28000
Training Step: 1837  | total loss: [1m[32m0.19460[0m[0m | time: 372.668s
| Adam | epoch: 004 | loss: 0.19460 - acc: 0.9324 -- iter: 07850/28000
Training Step: 1838  | total loss: [1m[32m0.19682[0m[0m | time: 374.011s
| Adam | epoch: 004 | loss: 0.19682 - acc: 0.9332 -- iter: 07900/28000
Training Step: 1839  | total loss: [1m[32m0.20395[0m[0m | time: 375.338s
| Adam | epoch: 004 | loss: 0.20395 - acc: 0.9279 -- iter: 07950/28000
Training Step: 1840  | total loss: [1m[32m0.20489[0m[0m | time: 376.614s
| Adam | epoch: 004 | loss: 0.20489 - acc: 0.9231 -- iter: 08000/28000
Training Step: 1841  | total loss: [1m[32m0.19405[0m[0m | time: 377.861s
| Adam | epoch: 004 | loss: 0.19405 - acc: 0.9268 -- iter: 08050/28000
Training Step: 1842  | total loss: [1m[32m0.18730[0m[0m | time: 379.153s
| Adam | epoch: 004 | loss: 0.18730 - acc: 0.9321 -- iter: 08100/28000
Training Step: 1843  | total loss: [1m[32m0.17882[0m[0m | time: 380.436s
| Adam | epoch: 004 | loss: 0.17882 - acc: 0.9329 -- iter: 08150/28000
Training Step: 1844  | total loss: [1m[32m0.17584[0m[0m | time: 381.690s
| Adam | epoch: 004 | loss: 0.17584 - acc: 0.9376 -- iter: 08200/28000
Training Step: 1845  | total loss: [1m[32m0.17806[0m[0m | time: 382.950s
| Adam | epoch: 004 | loss: 0.17806 - acc: 0.9318 -- iter: 08250/28000
Training Step: 1846  | total loss: [1m[32m0.19022[0m[0m | time: 384.225s
| Adam | epoch: 004 | loss: 0.19022 - acc: 0.9267 -- iter: 08300/28000
Training Step: 1847  | total loss: [1m[32m0.19457[0m[0m | time: 385.478s
| Adam | epoch: 004 | loss: 0.19457 - acc: 0.9260 -- iter: 08350/28000
Training Step: 1848  | total loss: [1m[32m0.18916[0m[0m | time: 386.750s
| Adam | epoch: 004 | loss: 0.18916 - acc: 0.9274 -- iter: 08400/28000
Training Step: 1849  | total loss: [1m[32m0.18184[0m[0m | time: 388.034s
| Adam | epoch: 004 | loss: 0.18184 - acc: 0.9326 -- iter: 08450/28000
Training Step: 1850  | total loss: [1m[32m0.19107[0m[0m | time: 418.414s
| Adam | epoch: 004 | loss: 0.19107 - acc: 0.9314 | val_loss: 0.18146 - val_acc: 0.9359 -- iter: 08500/28000
--
Training Step: 1851  | total loss: [1m[32m0.18678[0m[0m | time: 419.681s
| Adam | epoch: 004 | loss: 0.18678 - acc: 0.9342 -- iter: 08550/28000
Training Step: 1852  | total loss: [1m[32m0.18197[0m[0m | time: 420.906s
| Adam | epoch: 004 | loss: 0.18197 - acc: 0.9348 -- iter: 08600/28000
Training Step: 1853  | total loss: [1m[32m0.17994[0m[0m | time: 422.120s
| Adam | epoch: 004 | loss: 0.17994 - acc: 0.9333 -- iter: 08650/28000
Training Step: 1854  | total loss: [1m[32m0.18608[0m[0m | time: 423.380s
| Adam | epoch: 004 | loss: 0.18608 - acc: 0.9340 -- iter: 08700/28000
Training Step: 1855  | total loss: [1m[32m0.18082[0m[0m | time: 424.609s
| Adam | epoch: 004 | loss: 0.18082 - acc: 0.9346 -- iter: 08750/28000
Training Step: 1856  | total loss: [1m[32m0.17059[0m[0m | time: 425.827s
| Adam | epoch: 004 | loss: 0.17059 - acc: 0.9371 -- iter: 08800/28000
Training Step: 1857  | total loss: [1m[32m0.16794[0m[0m | time: 427.089s
| Adam | epoch: 004 | loss: 0.16794 - acc: 0.9394 -- iter: 08850/28000
Training Step: 1858  | total loss: [1m[32m0.16504[0m[0m | time: 428.375s
| Adam | epoch: 004 | loss: 0.16504 - acc: 0.9395 -- iter: 08900/28000
Training Step: 1859  | total loss: [1m[32m0.16080[0m[0m | time: 429.602s
| Adam | epoch: 004 | loss: 0.16080 - acc: 0.9395 -- iter: 08950/28000
Training Step: 1860  | total loss: [1m[32m0.15250[0m[0m | time: 430.812s
| Adam | epoch: 004 | loss: 0.15250 - acc: 0.9436 -- iter: 09000/28000
Training Step: 1861  | total loss: [1m[32m0.15999[0m[0m | time: 432.073s
| Adam | epoch: 004 | loss: 0.15999 - acc: 0.9412 -- iter: 09050/28000
Training Step: 1862  | total loss: [1m[32m0.16477[0m[0m | time: 433.307s
| Adam | epoch: 004 | loss: 0.16477 - acc: 0.9351 -- iter: 09100/28000
Training Step: 1863  | total loss: [1m[32m0.15665[0m[0m | time: 434.576s
| Adam | epoch: 004 | loss: 0.15665 - acc: 0.9376 -- iter: 09150/28000
Training Step: 1864  | total loss: [1m[32m0.16048[0m[0m | time: 435.837s
| Adam | epoch: 004 | loss: 0.16048 - acc: 0.9338 -- iter: 09200/28000
Training Step: 1865  | total loss: [1m[32m0.15829[0m[0m | time: 437.081s
| Adam | epoch: 004 | loss: 0.15829 - acc: 0.9365 -- iter: 09250/28000
Training Step: 1866  | total loss: [1m[32m0.14738[0m[0m | time: 438.330s
| Adam | epoch: 004 | loss: 0.14738 - acc: 0.9428 -- iter: 09300/28000
Training Step: 1867  | total loss: [1m[32m0.15073[0m[0m | time: 439.620s
| Adam | epoch: 004 | loss: 0.15073 - acc: 0.9365 -- iter: 09350/28000
Training Step: 1868  | total loss: [1m[32m0.16020[0m[0m | time: 440.875s
| Adam | epoch: 004 | loss: 0.16020 - acc: 0.9369 -- iter: 09400/28000
Training Step: 1869  | total loss: [1m[32m0.18276[0m[0m | time: 442.124s
| Adam | epoch: 004 | loss: 0.18276 - acc: 0.9332 -- iter: 09450/28000
Training Step: 1870  | total loss: [1m[32m0.17361[0m[0m | time: 443.397s
| Adam | epoch: 004 | loss: 0.17361 - acc: 0.9359 -- iter: 09500/28000
Training Step: 1871  | total loss: [1m[32m0.16766[0m[0m | time: 444.672s
| Adam | epoch: 004 | loss: 0.16766 - acc: 0.9363 -- iter: 09550/28000
Training Step: 1872  | total loss: [1m[32m0.18683[0m[0m | time: 445.979s
| Adam | epoch: 004 | loss: 0.18683 - acc: 0.9327 -- iter: 09600/28000
Training Step: 1873  | total loss: [1m[32m0.17391[0m[0m | time: 447.276s
| Adam | epoch: 004 | loss: 0.17391 - acc: 0.9374 -- iter: 09650/28000
Training Step: 1874  | total loss: [1m[32m0.17440[0m[0m | time: 448.538s
| Adam | epoch: 004 | loss: 0.17440 - acc: 0.9336 -- iter: 09700/28000
Training Step: 1875  | total loss: [1m[32m0.16328[0m[0m | time: 480.127s
| Adam | epoch: 004 | loss: 0.16328 - acc: 0.9383 | val_loss: 0.18415 - val_acc: 0.9336 -- iter: 09750/28000
--
Training Step: 1876  | total loss: [1m[32m0.16266[0m[0m | time: 481.408s
| Adam | epoch: 004 | loss: 0.16266 - acc: 0.9405 -- iter: 09800/28000
Training Step: 1877  | total loss: [1m[32m0.16401[0m[0m | time: 482.647s
| Adam | epoch: 004 | loss: 0.16401 - acc: 0.9404 -- iter: 09850/28000
Training Step: 1878  | total loss: [1m[32m0.15667[0m[0m | time: 483.873s
| Adam | epoch: 004 | loss: 0.15667 - acc: 0.9444 -- iter: 09900/28000
Training Step: 1879  | total loss: [1m[32m0.15277[0m[0m | time: 485.115s
| Adam | epoch: 004 | loss: 0.15277 - acc: 0.9459 -- iter: 09950/28000
Training Step: 1880  | total loss: [1m[32m0.14530[0m[0m | time: 486.340s
| Adam | epoch: 004 | loss: 0.14530 - acc: 0.9473 -- iter: 10000/28000
Training Step: 1881  | total loss: [1m[32m0.15046[0m[0m | time: 487.582s
| Adam | epoch: 004 | loss: 0.15046 - acc: 0.9466 -- iter: 10050/28000
Training Step: 1882  | total loss: [1m[32m0.14156[0m[0m | time: 488.817s
| Adam | epoch: 004 | loss: 0.14156 - acc: 0.9499 -- iter: 10100/28000
Training Step: 1883  | total loss: [1m[32m0.14894[0m[0m | time: 490.080s
| Adam | epoch: 004 | loss: 0.14894 - acc: 0.9429 -- iter: 10150/28000
Training Step: 1884  | total loss: [1m[32m0.14234[0m[0m | time: 491.311s
| Adam | epoch: 004 | loss: 0.14234 - acc: 0.9487 -- iter: 10200/28000
Training Step: 1885  | total loss: [1m[32m0.15203[0m[0m | time: 492.625s
| Adam | epoch: 004 | loss: 0.15203 - acc: 0.9478 -- iter: 10250/28000
Training Step: 1886  | total loss: [1m[32m0.15807[0m[0m | time: 493.844s
| Adam | epoch: 004 | loss: 0.15807 - acc: 0.9450 -- iter: 10300/28000
Training Step: 1887  | total loss: [1m[32m0.15811[0m[0m | time: 495.103s
| Adam | epoch: 004 | loss: 0.15811 - acc: 0.9425 -- iter: 10350/28000
Training Step: 1888  | total loss: [1m[32m0.16255[0m[0m | time: 496.351s
| Adam | epoch: 004 | loss: 0.16255 - acc: 0.9403 -- iter: 10400/28000
Training Step: 1889  | total loss: [1m[32m0.17778[0m[0m | time: 497.604s
| Adam | epoch: 004 | loss: 0.17778 - acc: 0.9342 -- iter: 10450/28000
Training Step: 1890  | total loss: [1m[32m0.18214[0m[0m | time: 498.859s
| Adam | epoch: 004 | loss: 0.18214 - acc: 0.9328 -- iter: 10500/28000
Training Step: 1891  | total loss: [1m[32m0.18287[0m[0m | time: 500.136s
| Adam | epoch: 004 | loss: 0.18287 - acc: 0.9295 -- iter: 10550/28000
Training Step: 1892  | total loss: [1m[32m0.17414[0m[0m | time: 501.376s
| Adam | epoch: 004 | loss: 0.17414 - acc: 0.9326 -- iter: 10600/28000
Training Step: 1893  | total loss: [1m[32m0.16611[0m[0m | time: 502.625s
| Adam | epoch: 004 | loss: 0.16611 - acc: 0.9353 -- iter: 10650/28000
Training Step: 1894  | total loss: [1m[32m0.15874[0m[0m | time: 503.863s
| Adam | epoch: 004 | loss: 0.15874 - acc: 0.9398 -- iter: 10700/28000
Training Step: 1895  | total loss: [1m[32m0.15740[0m[0m | time: 505.107s
| Adam | epoch: 004 | loss: 0.15740 - acc: 0.9378 -- iter: 10750/28000
Training Step: 1896  | total loss: [1m[32m0.14996[0m[0m | time: 506.357s
| Adam | epoch: 004 | loss: 0.14996 - acc: 0.9420 -- iter: 10800/28000
Training Step: 1897  | total loss: [1m[32m0.14445[0m[0m | time: 507.660s
| Adam | epoch: 004 | loss: 0.14445 - acc: 0.9438 -- iter: 10850/28000
Training Step: 1898  | total loss: [1m[32m0.14297[0m[0m | time: 508.952s
| Adam | epoch: 004 | loss: 0.14297 - acc: 0.9434 -- iter: 10900/28000
Training Step: 1899  | total loss: [1m[32m0.14577[0m[0m | time: 510.222s
| Adam | epoch: 004 | loss: 0.14577 - acc: 0.9451 -- iter: 10950/28000
Training Step: 1900  | total loss: [1m[32m0.14126[0m[0m | time: 540.167s
| Adam | epoch: 004 | loss: 0.14126 - acc: 0.9466 | val_loss: 0.21982 - val_acc: 0.9194 -- iter: 11000/28000
--
Training Step: 1901  | total loss: [1m[32m0.13794[0m[0m | time: 541.451s
| Adam | epoch: 004 | loss: 0.13794 - acc: 0.9479 -- iter: 11050/28000
Training Step: 1902  | total loss: [1m[32m0.15340[0m[0m | time: 542.693s
| Adam | epoch: 004 | loss: 0.15340 - acc: 0.9451 -- iter: 11100/28000
Training Step: 1903  | total loss: [1m[32m0.15349[0m[0m | time: 543.940s
| Adam | epoch: 004 | loss: 0.15349 - acc: 0.9426 -- iter: 11150/28000
Training Step: 1904  | total loss: [1m[32m0.15092[0m[0m | time: 545.186s
| Adam | epoch: 004 | loss: 0.15092 - acc: 0.9444 -- iter: 11200/28000
Training Step: 1905  | total loss: [1m[32m0.14602[0m[0m | time: 546.464s
| Adam | epoch: 004 | loss: 0.14602 - acc: 0.9439 -- iter: 11250/28000
Training Step: 1906  | total loss: [1m[32m0.14948[0m[0m | time: 547.732s
| Adam | epoch: 004 | loss: 0.14948 - acc: 0.9435 -- iter: 11300/28000
Training Step: 1907  | total loss: [1m[32m0.15456[0m[0m | time: 549.015s
| Adam | epoch: 004 | loss: 0.15456 - acc: 0.9432 -- iter: 11350/28000
Training Step: 1908  | total loss: [1m[32m0.15914[0m[0m | time: 550.274s
| Adam | epoch: 004 | loss: 0.15914 - acc: 0.9429 -- iter: 11400/28000
Training Step: 1909  | total loss: [1m[32m0.15673[0m[0m | time: 551.521s
| Adam | epoch: 004 | loss: 0.15673 - acc: 0.9426 -- iter: 11450/28000
Training Step: 1910  | total loss: [1m[32m0.15700[0m[0m | time: 552.782s
| Adam | epoch: 004 | loss: 0.15700 - acc: 0.9383 -- iter: 11500/28000
Training Step: 1911  | total loss: [1m[32m0.14744[0m[0m | time: 554.019s
| Adam | epoch: 004 | loss: 0.14744 - acc: 0.9425 -- iter: 11550/28000
Training Step: 1912  | total loss: [1m[32m0.15045[0m[0m | time: 555.261s
| Adam | epoch: 004 | loss: 0.15045 - acc: 0.9462 -- iter: 11600/28000
Training Step: 1913  | total loss: [1m[32m0.14464[0m[0m | time: 556.482s
| Adam | epoch: 004 | loss: 0.14464 - acc: 0.9476 -- iter: 11650/28000
Training Step: 1914  | total loss: [1m[32m0.14766[0m[0m | time: 557.722s
| Adam | epoch: 004 | loss: 0.14766 - acc: 0.9429 -- iter: 11700/28000
Training Step: 1915  | total loss: [1m[32m0.14366[0m[0m | time: 559.003s
| Adam | epoch: 004 | loss: 0.14366 - acc: 0.9446 -- iter: 11750/28000
Training Step: 1916  | total loss: [1m[32m0.15182[0m[0m | time: 560.233s
| Adam | epoch: 004 | loss: 0.15182 - acc: 0.9461 -- iter: 11800/28000
Training Step: 1917  | total loss: [1m[32m0.14911[0m[0m | time: 561.511s
| Adam | epoch: 004 | loss: 0.14911 - acc: 0.9495 -- iter: 11850/28000
Training Step: 1918  | total loss: [1m[32m0.14267[0m[0m | time: 562.753s
| Adam | epoch: 004 | loss: 0.14267 - acc: 0.9525 -- iter: 11900/28000
Training Step: 1919  | total loss: [1m[32m0.14591[0m[0m | time: 563.999s
| Adam | epoch: 004 | loss: 0.14591 - acc: 0.9533 -- iter: 11950/28000
Training Step: 1920  | total loss: [1m[32m0.15454[0m[0m | time: 565.224s
| Adam | epoch: 004 | loss: 0.15454 - acc: 0.9520 -- iter: 12000/28000
Training Step: 1921  | total loss: [1m[32m0.15028[0m[0m | time: 566.454s
| Adam | epoch: 004 | loss: 0.15028 - acc: 0.9548 -- iter: 12050/28000
Training Step: 1922  | total loss: [1m[32m0.15138[0m[0m | time: 567.684s
| Adam | epoch: 004 | loss: 0.15138 - acc: 0.9533 -- iter: 12100/28000
Training Step: 1923  | total loss: [1m[32m0.14584[0m[0m | time: 568.933s
| Adam | epoch: 004 | loss: 0.14584 - acc: 0.9540 -- iter: 12150/28000
Training Step: 1924  | total loss: [1m[32m0.16233[0m[0m | time: 570.158s
| Adam | epoch: 004 | loss: 0.16233 - acc: 0.9466 -- iter: 12200/28000
Training Step: 1925  | total loss: [1m[32m0.16238[0m[0m | time: 600.139s
| Adam | epoch: 004 | loss: 0.16238 - acc: 0.9399 | val_loss: 0.18071 - val_acc: 0.9349 -- iter: 12250/28000
--
Training Step: 1926  | total loss: [1m[32m0.15598[0m[0m | time: 601.428s
| Adam | epoch: 004 | loss: 0.15598 - acc: 0.9439 -- iter: 12300/28000
Training Step: 1927  | total loss: [1m[32m0.15369[0m[0m | time: 602.691s
| Adam | epoch: 004 | loss: 0.15369 - acc: 0.9455 -- iter: 12350/28000
Training Step: 1928  | total loss: [1m[32m0.14483[0m[0m | time: 604.031s
| Adam | epoch: 004 | loss: 0.14483 - acc: 0.9490 -- iter: 12400/28000
Training Step: 1929  | total loss: [1m[32m0.13593[0m[0m | time: 605.321s
| Adam | epoch: 004 | loss: 0.13593 - acc: 0.9541 -- iter: 12450/28000
Training Step: 1930  | total loss: [1m[32m0.14443[0m[0m | time: 606.608s
| Adam | epoch: 004 | loss: 0.14443 - acc: 0.9547 -- iter: 12500/28000
Training Step: 1931  | total loss: [1m[32m0.14056[0m[0m | time: 607.897s
| Adam | epoch: 004 | loss: 0.14056 - acc: 0.9572 -- iter: 12550/28000
Training Step: 1932  | total loss: [1m[32m0.13947[0m[0m | time: 609.223s
| Adam | epoch: 004 | loss: 0.13947 - acc: 0.9555 -- iter: 12600/28000
Training Step: 1933  | total loss: [1m[32m0.15275[0m[0m | time: 610.473s
| Adam | epoch: 004 | loss: 0.15275 - acc: 0.9479 -- iter: 12650/28000
Training Step: 1934  | total loss: [1m[32m0.15230[0m[0m | time: 611.718s
| Adam | epoch: 004 | loss: 0.15230 - acc: 0.9491 -- iter: 12700/28000
Training Step: 1935  | total loss: [1m[32m0.14471[0m[0m | time: 612.984s
| Adam | epoch: 004 | loss: 0.14471 - acc: 0.9502 -- iter: 12750/28000
Training Step: 1936  | total loss: [1m[32m0.13884[0m[0m | time: 614.227s
| Adam | epoch: 004 | loss: 0.13884 - acc: 0.9532 -- iter: 12800/28000
Training Step: 1937  | total loss: [1m[32m0.14093[0m[0m | time: 615.490s
| Adam | epoch: 004 | loss: 0.14093 - acc: 0.9539 -- iter: 12850/28000
Training Step: 1938  | total loss: [1m[32m0.13945[0m[0m | time: 616.745s
| Adam | epoch: 004 | loss: 0.13945 - acc: 0.9565 -- iter: 12900/28000
Training Step: 1939  | total loss: [1m[32m0.14469[0m[0m | time: 617.985s
| Adam | epoch: 004 | loss: 0.14469 - acc: 0.9568 -- iter: 12950/28000
Training Step: 1940  | total loss: [1m[32m0.13452[0m[0m | time: 619.273s
| Adam | epoch: 004 | loss: 0.13452 - acc: 0.9612 -- iter: 13000/28000
Training Step: 1941  | total loss: [1m[32m0.14231[0m[0m | time: 620.529s
| Adam | epoch: 004 | loss: 0.14231 - acc: 0.9610 -- iter: 13050/28000
Training Step: 1942  | total loss: [1m[32m0.14485[0m[0m | time: 621.792s
| Adam | epoch: 004 | loss: 0.14485 - acc: 0.9569 -- iter: 13100/28000
Training Step: 1943  | total loss: [1m[32m0.14578[0m[0m | time: 623.035s
| Adam | epoch: 004 | loss: 0.14578 - acc: 0.9552 -- iter: 13150/28000
Training Step: 1944  | total loss: [1m[32m0.14563[0m[0m | time: 624.281s
| Adam | epoch: 004 | loss: 0.14563 - acc: 0.9517 -- iter: 13200/28000
Training Step: 1945  | total loss: [1m[32m0.14813[0m[0m | time: 625.551s
| Adam | epoch: 004 | loss: 0.14813 - acc: 0.9545 -- iter: 13250/28000
Training Step: 1946  | total loss: [1m[32m0.15727[0m[0m | time: 626.811s
| Adam | epoch: 004 | loss: 0.15727 - acc: 0.9531 -- iter: 13300/28000
Training Step: 1947  | total loss: [1m[32m0.15252[0m[0m | time: 628.015s
| Adam | epoch: 004 | loss: 0.15252 - acc: 0.9538 -- iter: 13350/28000
Training Step: 1948  | total loss: [1m[32m0.16520[0m[0m | time: 629.208s
| Adam | epoch: 004 | loss: 0.16520 - acc: 0.9484 -- iter: 13400/28000
Training Step: 1949  | total loss: [1m[32m0.16172[0m[0m | time: 630.490s
| Adam | epoch: 004 | loss: 0.16172 - acc: 0.9516 -- iter: 13450/28000
Training Step: 1950  | total loss: [1m[32m0.17682[0m[0m | time: 661.159s
| Adam | epoch: 004 | loss: 0.17682 - acc: 0.9464 | val_loss: 0.19778 - val_acc: 0.9277 -- iter: 13500/28000
--
Training Step: 1951  | total loss: [1m[32m0.18701[0m[0m | time: 662.457s
| Adam | epoch: 004 | loss: 0.18701 - acc: 0.9398 -- iter: 13550/28000
Training Step: 1952  | total loss: [1m[32m0.18845[0m[0m | time: 663.705s
| Adam | epoch: 004 | loss: 0.18845 - acc: 0.9358 -- iter: 13600/28000
Training Step: 1953  | total loss: [1m[32m0.18289[0m[0m | time: 664.947s
| Adam | epoch: 004 | loss: 0.18289 - acc: 0.9382 -- iter: 13650/28000
Training Step: 1954  | total loss: [1m[32m0.17767[0m[0m | time: 666.298s
| Adam | epoch: 004 | loss: 0.17767 - acc: 0.9384 -- iter: 13700/28000
Training Step: 1955  | total loss: [1m[32m0.17825[0m[0m | time: 667.538s
| Adam | epoch: 004 | loss: 0.17825 - acc: 0.9386 -- iter: 13750/28000
Training Step: 1956  | total loss: [1m[32m0.17556[0m[0m | time: 668.781s
| Adam | epoch: 004 | loss: 0.17556 - acc: 0.9407 -- iter: 13800/28000
Training Step: 1957  | total loss: [1m[32m0.16540[0m[0m | time: 670.033s
| Adam | epoch: 004 | loss: 0.16540 - acc: 0.9446 -- iter: 13850/28000
Training Step: 1958  | total loss: [1m[32m0.17121[0m[0m | time: 671.279s
| Adam | epoch: 004 | loss: 0.17121 - acc: 0.9402 -- iter: 13900/28000
Training Step: 1959  | total loss: [1m[32m0.17441[0m[0m | time: 672.569s
| Adam | epoch: 004 | loss: 0.17441 - acc: 0.9401 -- iter: 13950/28000
Training Step: 1960  | total loss: [1m[32m0.16574[0m[0m | time: 673.839s
| Adam | epoch: 004 | loss: 0.16574 - acc: 0.9441 -- iter: 14000/28000
Training Step: 1961  | total loss: [1m[32m0.15704[0m[0m | time: 675.112s
| Adam | epoch: 004 | loss: 0.15704 - acc: 0.9477 -- iter: 14050/28000
Training Step: 1962  | total loss: [1m[32m0.15234[0m[0m | time: 676.370s
| Adam | epoch: 004 | loss: 0.15234 - acc: 0.9509 -- iter: 14100/28000
Training Step: 1963  | total loss: [1m[32m0.14190[0m[0m | time: 677.632s
| Adam | epoch: 004 | loss: 0.14190 - acc: 0.9539 -- iter: 14150/28000
Training Step: 1964  | total loss: [1m[32m0.14507[0m[0m | time: 678.971s
| Adam | epoch: 004 | loss: 0.14507 - acc: 0.9525 -- iter: 14200/28000
Training Step: 1965  | total loss: [1m[32m0.16644[0m[0m | time: 680.231s
| Adam | epoch: 004 | loss: 0.16644 - acc: 0.9472 -- iter: 14250/28000
Training Step: 1966  | total loss: [1m[32m0.15605[0m[0m | time: 681.490s
| Adam | epoch: 004 | loss: 0.15605 - acc: 0.9525 -- iter: 14300/28000
Training Step: 1967  | total loss: [1m[32m0.15896[0m[0m | time: 682.745s
| Adam | epoch: 004 | loss: 0.15896 - acc: 0.9452 -- iter: 14350/28000
Training Step: 1968  | total loss: [1m[32m0.16300[0m[0m | time: 684.033s
| Adam | epoch: 004 | loss: 0.16300 - acc: 0.9407 -- iter: 14400/28000
Training Step: 1969  | total loss: [1m[32m0.15562[0m[0m | time: 685.280s
| Adam | epoch: 004 | loss: 0.15562 - acc: 0.9447 -- iter: 14450/28000
Training Step: 1970  | total loss: [1m[32m0.15298[0m[0m | time: 686.541s
| Adam | epoch: 004 | loss: 0.15298 - acc: 0.9462 -- iter: 14500/28000
Training Step: 1971  | total loss: [1m[32m0.14899[0m[0m | time: 687.817s
| Adam | epoch: 004 | loss: 0.14899 - acc: 0.9496 -- iter: 14550/28000
Training Step: 1972  | total loss: [1m[32m0.14314[0m[0m | time: 689.066s
| Adam | epoch: 004 | loss: 0.14314 - acc: 0.9526 -- iter: 14600/28000
Training Step: 1973  | total loss: [1m[32m0.13226[0m[0m | time: 690.324s
| Adam | epoch: 004 | loss: 0.13226 - acc: 0.9574 -- iter: 14650/28000
Training Step: 1974  | total loss: [1m[32m0.15081[0m[0m | time: 691.562s
| Adam | epoch: 004 | loss: 0.15081 - acc: 0.9516 -- iter: 14700/28000
Training Step: 1975  | total loss: [1m[32m0.15372[0m[0m | time: 721.984s
| Adam | epoch: 004 | loss: 0.15372 - acc: 0.9525 | val_loss: 0.18723 - val_acc: 0.9360 -- iter: 14750/28000
--
Training Step: 1976  | total loss: [1m[32m0.14571[0m[0m | time: 723.308s
| Adam | epoch: 004 | loss: 0.14571 - acc: 0.9552 -- iter: 14800/28000
Training Step: 1977  | total loss: [1m[32m0.14445[0m[0m | time: 724.699s
| Adam | epoch: 004 | loss: 0.14445 - acc: 0.9517 -- iter: 14850/28000
Training Step: 1978  | total loss: [1m[32m0.14462[0m[0m | time: 726.068s
| Adam | epoch: 004 | loss: 0.14462 - acc: 0.9485 -- iter: 14900/28000
Training Step: 1979  | total loss: [1m[32m0.16258[0m[0m | time: 727.498s
| Adam | epoch: 004 | loss: 0.16258 - acc: 0.9437 -- iter: 14950/28000
Training Step: 1980  | total loss: [1m[32m0.15172[0m[0m | time: 728.807s
| Adam | epoch: 004 | loss: 0.15172 - acc: 0.9493 -- iter: 15000/28000
Training Step: 1981  | total loss: [1m[32m0.14404[0m[0m | time: 730.084s
| Adam | epoch: 004 | loss: 0.14404 - acc: 0.9524 -- iter: 15050/28000
Training Step: 1982  | total loss: [1m[32m0.13835[0m[0m | time: 731.369s
| Adam | epoch: 004 | loss: 0.13835 - acc: 0.9531 -- iter: 15100/28000
Training Step: 1983  | total loss: [1m[32m0.15626[0m[0m | time: 732.642s
| Adam | epoch: 004 | loss: 0.15626 - acc: 0.9498 -- iter: 15150/28000
Training Step: 1984  | total loss: [1m[32m0.16222[0m[0m | time: 733.864s
| Adam | epoch: 004 | loss: 0.16222 - acc: 0.9508 -- iter: 15200/28000
Training Step: 1985  | total loss: [1m[32m0.18041[0m[0m | time: 735.119s
| Adam | epoch: 004 | loss: 0.18041 - acc: 0.9478 -- iter: 15250/28000
Training Step: 1986  | total loss: [1m[32m0.19445[0m[0m | time: 736.364s
| Adam | epoch: 004 | loss: 0.19445 - acc: 0.9450 -- iter: 15300/28000
Training Step: 1987  | total loss: [1m[32m0.18084[0m[0m | time: 737.610s
| Adam | epoch: 004 | loss: 0.18084 - acc: 0.9505 -- iter: 15350/28000
Training Step: 1988  | total loss: [1m[32m0.17488[0m[0m | time: 739.032s
| Adam | epoch: 004 | loss: 0.17488 - acc: 0.9514 -- iter: 15400/28000
Training Step: 1989  | total loss: [1m[32m0.17173[0m[0m | time: 740.467s
| Adam | epoch: 004 | loss: 0.17173 - acc: 0.9523 -- iter: 15450/28000
Training Step: 1990  | total loss: [1m[32m0.17297[0m[0m | time: 741.892s
| Adam | epoch: 004 | loss: 0.17297 - acc: 0.9531 -- iter: 15500/28000
Training Step: 1991  | total loss: [1m[32m0.16679[0m[0m | time: 743.135s
| Adam | epoch: 004 | loss: 0.16679 - acc: 0.9558 -- iter: 15550/28000
Training Step: 1992  | total loss: [1m[32m0.15903[0m[0m | time: 744.391s
| Adam | epoch: 004 | loss: 0.15903 - acc: 0.9582 -- iter: 15600/28000
Training Step: 1993  | total loss: [1m[32m0.15153[0m[0m | time: 745.663s
| Adam | epoch: 004 | loss: 0.15153 - acc: 0.9584 -- iter: 15650/28000
Training Step: 1994  | total loss: [1m[32m0.14364[0m[0m | time: 746.911s
| Adam | epoch: 004 | loss: 0.14364 - acc: 0.9625 -- iter: 15700/28000
Training Step: 1995  | total loss: [1m[32m0.17072[0m[0m | time: 748.168s
| Adam | epoch: 004 | loss: 0.17072 - acc: 0.9523 -- iter: 15750/28000
Training Step: 1996  | total loss: [1m[32m0.17344[0m[0m | time: 749.388s
| Adam | epoch: 004 | loss: 0.17344 - acc: 0.9490 -- iter: 15800/28000
Training Step: 1997  | total loss: [1m[32m0.16334[0m[0m | time: 750.626s
| Adam | epoch: 004 | loss: 0.16334 - acc: 0.9521 -- iter: 15850/28000
Training Step: 1998  | total loss: [1m[32m0.15510[0m[0m | time: 751.888s
| Adam | epoch: 004 | loss: 0.15510 - acc: 0.9549 -- iter: 15900/28000
Training Step: 1999  | total loss: [1m[32m0.16149[0m[0m | time: 753.166s
| Adam | epoch: 004 | loss: 0.16149 - acc: 0.9494 -- iter: 15950/28000
Training Step: 2000  | total loss: [1m[32m0.16101[0m[0m | time: 783.305s
| Adam | epoch: 004 | loss: 0.16101 - acc: 0.9505 | val_loss: 0.25169 - val_acc: 0.9062 -- iter: 16000/28000
--
Training Step: 2001  | total loss: [1m[32m0.15463[0m[0m | time: 784.615s
| Adam | epoch: 004 | loss: 0.15463 - acc: 0.9534 -- iter: 16050/28000
Training Step: 2002  | total loss: [1m[32m0.15723[0m[0m | time: 785.969s
| Adam | epoch: 004 | loss: 0.15723 - acc: 0.9461 -- iter: 16100/28000
Training Step: 2003  | total loss: [1m[32m0.14716[0m[0m | time: 787.466s
| Adam | epoch: 004 | loss: 0.14716 - acc: 0.9495 -- iter: 16150/28000
Training Step: 2004  | total loss: [1m[32m0.16659[0m[0m | time: 788.871s
| Adam | epoch: 004 | loss: 0.16659 - acc: 0.9465 -- iter: 16200/28000
Training Step: 2005  | total loss: [1m[32m0.18769[0m[0m | time: 790.119s
| Adam | epoch: 004 | loss: 0.18769 - acc: 0.9379 -- iter: 16250/28000
Training Step: 2006  | total loss: [1m[32m0.19046[0m[0m | time: 791.369s
| Adam | epoch: 004 | loss: 0.19046 - acc: 0.9341 -- iter: 16300/28000
Training Step: 2007  | total loss: [1m[32m0.18010[0m[0m | time: 792.627s
| Adam | epoch: 004 | loss: 0.18010 - acc: 0.9367 -- iter: 16350/28000
Training Step: 2008  | total loss: [1m[32m0.17906[0m[0m | time: 793.875s
| Adam | epoch: 004 | loss: 0.17906 - acc: 0.9370 -- iter: 16400/28000
Training Step: 2009  | total loss: [1m[32m0.17935[0m[0m | time: 795.097s
| Adam | epoch: 004 | loss: 0.17935 - acc: 0.9373 -- iter: 16450/28000
Training Step: 2010  | total loss: [1m[32m0.18316[0m[0m | time: 796.317s
| Adam | epoch: 004 | loss: 0.18316 - acc: 0.9336 -- iter: 16500/28000
Training Step: 2011  | total loss: [1m[32m0.18255[0m[0m | time: 797.554s
| Adam | epoch: 004 | loss: 0.18255 - acc: 0.9342 -- iter: 16550/28000
Training Step: 2012  | total loss: [1m[32m0.18023[0m[0m | time: 798.796s
| Adam | epoch: 004 | loss: 0.18023 - acc: 0.9368 -- iter: 16600/28000
Training Step: 2013  | total loss: [1m[32m0.18563[0m[0m | time: 800.050s
| Adam | epoch: 004 | loss: 0.18563 - acc: 0.9331 -- iter: 16650/28000
Training Step: 2014  | total loss: [1m[32m0.17930[0m[0m | time: 801.291s
| Adam | epoch: 004 | loss: 0.17930 - acc: 0.9378 -- iter: 16700/28000
Training Step: 2015  | total loss: [1m[32m0.18139[0m[0m | time: 802.508s
| Adam | epoch: 004 | loss: 0.18139 - acc: 0.9380 -- iter: 16750/28000
Training Step: 2016  | total loss: [1m[32m0.17213[0m[0m | time: 803.711s
| Adam | epoch: 004 | loss: 0.17213 - acc: 0.9402 -- iter: 16800/28000
Training Step: 2017  | total loss: [1m[32m0.16664[0m[0m | time: 804.979s
| Adam | epoch: 004 | loss: 0.16664 - acc: 0.9402 -- iter: 16850/28000
Training Step: 2018  | total loss: [1m[32m0.17458[0m[0m | time: 806.228s
| Adam | epoch: 004 | loss: 0.17458 - acc: 0.9362 -- iter: 16900/28000
Training Step: 2019  | total loss: [1m[32m0.17005[0m[0m | time: 807.505s
| Adam | epoch: 004 | loss: 0.17005 - acc: 0.9386 -- iter: 16950/28000
Training Step: 2020  | total loss: [1m[32m0.16000[0m[0m | time: 808.750s
| Adam | epoch: 004 | loss: 0.16000 - acc: 0.9427 -- iter: 17000/28000
Training Step: 2021  | total loss: [1m[32m0.15649[0m[0m | time: 809.985s
| Adam | epoch: 004 | loss: 0.15649 - acc: 0.9424 -- iter: 17050/28000
Training Step: 2022  | total loss: [1m[32m0.15311[0m[0m | time: 811.238s
| Adam | epoch: 004 | loss: 0.15311 - acc: 0.9462 -- iter: 17100/28000
Training Step: 2023  | total loss: [1m[32m0.15761[0m[0m | time: 812.477s
| Adam | epoch: 004 | loss: 0.15761 - acc: 0.9416 -- iter: 17150/28000
Training Step: 2024  | total loss: [1m[32m0.16327[0m[0m | time: 813.700s
| Adam | epoch: 004 | loss: 0.16327 - acc: 0.9374 -- iter: 17200/28000
Training Step: 2025  | total loss: [1m[32m0.15807[0m[0m | time: 843.574s
| Adam | epoch: 004 | loss: 0.15807 - acc: 0.9417 | val_loss: 0.18870 - val_acc: 0.9292 -- iter: 17250/28000
--
Training Step: 2026  | total loss: [1m[32m0.15397[0m[0m | time: 844.859s
| Adam | epoch: 004 | loss: 0.15397 - acc: 0.9415 -- iter: 17300/28000
Training Step: 2027  | total loss: [1m[32m0.14250[0m[0m | time: 846.102s
| Adam | epoch: 004 | loss: 0.14250 - acc: 0.9474 -- iter: 17350/28000
Training Step: 2028  | total loss: [1m[32m0.13938[0m[0m | time: 847.356s
| Adam | epoch: 004 | loss: 0.13938 - acc: 0.9466 -- iter: 17400/28000
Training Step: 2029  | total loss: [1m[32m0.13753[0m[0m | time: 848.612s
| Adam | epoch: 004 | loss: 0.13753 - acc: 0.9480 -- iter: 17450/28000
Training Step: 2030  | total loss: [1m[32m0.13654[0m[0m | time: 849.868s
| Adam | epoch: 004 | loss: 0.13654 - acc: 0.9472 -- iter: 17500/28000
Training Step: 2031  | total loss: [1m[32m0.16094[0m[0m | time: 851.126s
| Adam | epoch: 004 | loss: 0.16094 - acc: 0.9444 -- iter: 17550/28000
Training Step: 2032  | total loss: [1m[32m0.15628[0m[0m | time: 852.403s
| Adam | epoch: 004 | loss: 0.15628 - acc: 0.9440 -- iter: 17600/28000
Training Step: 2033  | total loss: [1m[32m0.15424[0m[0m | time: 853.641s
| Adam | epoch: 004 | loss: 0.15424 - acc: 0.9416 -- iter: 17650/28000
Training Step: 2034  | total loss: [1m[32m0.15526[0m[0m | time: 854.903s
| Adam | epoch: 004 | loss: 0.15526 - acc: 0.9394 -- iter: 17700/28000
Training Step: 2035  | total loss: [1m[32m0.15095[0m[0m | time: 856.126s
| Adam | epoch: 004 | loss: 0.15095 - acc: 0.9435 -- iter: 17750/28000
Training Step: 2036  | total loss: [1m[32m0.15069[0m[0m | time: 857.362s
| Adam | epoch: 004 | loss: 0.15069 - acc: 0.9411 -- iter: 17800/28000
Training Step: 2037  | total loss: [1m[32m0.14712[0m[0m | time: 858.643s
| Adam | epoch: 004 | loss: 0.14712 - acc: 0.9430 -- iter: 17850/28000
Training Step: 2038  | total loss: [1m[32m0.16596[0m[0m | time: 859.897s
| Adam | epoch: 004 | loss: 0.16596 - acc: 0.9367 -- iter: 17900/28000
Training Step: 2039  | total loss: [1m[32m0.16416[0m[0m | time: 861.148s
| Adam | epoch: 004 | loss: 0.16416 - acc: 0.9351 -- iter: 17950/28000
Training Step: 2040  | total loss: [1m[32m0.15749[0m[0m | time: 862.489s
| Adam | epoch: 004 | loss: 0.15749 - acc: 0.9376 -- iter: 18000/28000
Training Step: 2041  | total loss: [1m[32m0.16607[0m[0m | time: 863.750s
| Adam | epoch: 004 | loss: 0.16607 - acc: 0.9358 -- iter: 18050/28000
Training Step: 2042  | total loss: [1m[32m0.17839[0m[0m | time: 865.010s
| Adam | epoch: 004 | loss: 0.17839 - acc: 0.9302 -- iter: 18100/28000
Training Step: 2043  | total loss: [1m[32m0.18505[0m[0m | time: 866.241s
| Adam | epoch: 004 | loss: 0.18505 - acc: 0.9272 -- iter: 18150/28000
Training Step: 2044  | total loss: [1m[32m0.17479[0m[0m | time: 867.480s
| Adam | epoch: 004 | loss: 0.17479 - acc: 0.9305 -- iter: 18200/28000
Training Step: 2045  | total loss: [1m[32m0.17232[0m[0m | time: 868.697s
| Adam | epoch: 004 | loss: 0.17232 - acc: 0.9314 -- iter: 18250/28000
Training Step: 2046  | total loss: [1m[32m0.16407[0m[0m | time: 869.942s
| Adam | epoch: 004 | loss: 0.16407 - acc: 0.9343 -- iter: 18300/28000
Training Step: 2047  | total loss: [1m[32m0.15548[0m[0m | time: 871.188s
| Adam | epoch: 004 | loss: 0.15548 - acc: 0.9389 -- iter: 18350/28000
Training Step: 2048  | total loss: [1m[32m0.15429[0m[0m | time: 872.439s
| Adam | epoch: 004 | loss: 0.15429 - acc: 0.9390 -- iter: 18400/28000
Training Step: 2049  | total loss: [1m[32m0.15753[0m[0m | time: 873.654s
| Adam | epoch: 004 | loss: 0.15753 - acc: 0.9351 -- iter: 18450/28000
Training Step: 2050  | total loss: [1m[32m0.16938[0m[0m | time: 903.692s
| Adam | epoch: 004 | loss: 0.16938 - acc: 0.9336 | val_loss: 0.18921 - val_acc: 0.9289 -- iter: 18500/28000
--
Training Step: 2051  | total loss: [1m[32m0.16707[0m[0m | time: 905.068s
| Adam | epoch: 004 | loss: 0.16707 - acc: 0.9342 -- iter: 18550/28000
Training Step: 2052  | total loss: [1m[32m0.17038[0m[0m | time: 906.405s
| Adam | epoch: 004 | loss: 0.17038 - acc: 0.9328 -- iter: 18600/28000
Training Step: 2053  | total loss: [1m[32m0.16976[0m[0m | time: 907.695s
| Adam | epoch: 004 | loss: 0.16976 - acc: 0.9375 -- iter: 18650/28000
Training Step: 2054  | total loss: [1m[32m0.17602[0m[0m | time: 909.018s
| Adam | epoch: 004 | loss: 0.17602 - acc: 0.9398 -- iter: 18700/28000
Training Step: 2055  | total loss: [1m[32m0.16685[0m[0m | time: 910.245s
| Adam | epoch: 004 | loss: 0.16685 - acc: 0.9438 -- iter: 18750/28000
Training Step: 2056  | total loss: [1m[32m0.15985[0m[0m | time: 911.526s
| Adam | epoch: 004 | loss: 0.15985 - acc: 0.9474 -- iter: 18800/28000
Training Step: 2057  | total loss: [1m[32m0.15517[0m[0m | time: 912.801s
| Adam | epoch: 004 | loss: 0.15517 - acc: 0.9507 -- iter: 18850/28000
Training Step: 2058  | total loss: [1m[32m0.16632[0m[0m | time: 914.039s
| Adam | epoch: 004 | loss: 0.16632 - acc: 0.9456 -- iter: 18900/28000
Training Step: 2059  | total loss: [1m[32m0.15518[0m[0m | time: 915.289s
| Adam | epoch: 004 | loss: 0.15518 - acc: 0.9510 -- iter: 18950/28000
Training Step: 2060  | total loss: [1m[32m0.15387[0m[0m | time: 916.546s
| Adam | epoch: 004 | loss: 0.15387 - acc: 0.9499 -- iter: 19000/28000
Training Step: 2061  | total loss: [1m[32m0.16223[0m[0m | time: 917.848s
| Adam | epoch: 004 | loss: 0.16223 - acc: 0.9489 -- iter: 19050/28000
Training Step: 2062  | total loss: [1m[32m0.16532[0m[0m | time: 919.179s
| Adam | epoch: 004 | loss: 0.16532 - acc: 0.9460 -- iter: 19100/28000
Training Step: 2063  | total loss: [1m[32m0.15697[0m[0m | time: 920.476s
| Adam | epoch: 004 | loss: 0.15697 - acc: 0.9494 -- iter: 19150/28000
Training Step: 2064  | total loss: [1m[32m0.14532[0m[0m | time: 921.743s
| Adam | epoch: 004 | loss: 0.14532 - acc: 0.9545 -- iter: 19200/28000
Training Step: 2065  | total loss: [1m[32m0.14715[0m[0m | time: 923.023s
| Adam | epoch: 004 | loss: 0.14715 - acc: 0.9550 -- iter: 19250/28000
Training Step: 2066  | total loss: [1m[32m0.15609[0m[0m | time: 924.230s
| Adam | epoch: 004 | loss: 0.15609 - acc: 0.9535 -- iter: 19300/28000
Training Step: 2067  | total loss: [1m[32m0.15795[0m[0m | time: 925.465s
| Adam | epoch: 004 | loss: 0.15795 - acc: 0.9522 -- iter: 19350/28000
Training Step: 2068  | total loss: [1m[32m0.16897[0m[0m | time: 926.714s
| Adam | epoch: 004 | loss: 0.16897 - acc: 0.9490 -- iter: 19400/28000
Training Step: 2069  | total loss: [1m[32m0.16403[0m[0m | time: 927.942s
| Adam | epoch: 004 | loss: 0.16403 - acc: 0.9501 -- iter: 19450/28000
Training Step: 2070  | total loss: [1m[32m0.15618[0m[0m | time: 929.192s
| Adam | epoch: 004 | loss: 0.15618 - acc: 0.9531 -- iter: 19500/28000
Training Step: 2071  | total loss: [1m[32m0.15383[0m[0m | time: 930.485s
| Adam | epoch: 004 | loss: 0.15383 - acc: 0.9518 -- iter: 19550/28000
Training Step: 2072  | total loss: [1m[32m0.15327[0m[0m | time: 931.773s
| Adam | epoch: 004 | loss: 0.15327 - acc: 0.9486 -- iter: 19600/28000
Training Step: 2073  | total loss: [1m[32m0.15189[0m[0m | time: 933.035s
| Adam | epoch: 004 | loss: 0.15189 - acc: 0.9477 -- iter: 19650/28000
Training Step: 2074  | total loss: [1m[32m0.16170[0m[0m | time: 934.324s
| Adam | epoch: 004 | loss: 0.16170 - acc: 0.9410 -- iter: 19700/28000
Training Step: 2075  | total loss: [1m[32m0.15485[0m[0m | time: 964.556s
| Adam | epoch: 004 | loss: 0.15485 - acc: 0.9409 | val_loss: 0.20912 - val_acc: 0.9231 -- iter: 19750/28000
--
Training Step: 2076  | total loss: [1m[32m0.15079[0m[0m | time: 965.808s
| Adam | epoch: 004 | loss: 0.15079 - acc: 0.9448 -- iter: 19800/28000
Training Step: 2077  | total loss: [1m[32m0.14849[0m[0m | time: 967.096s
| Adam | epoch: 004 | loss: 0.14849 - acc: 0.9443 -- iter: 19850/28000
Training Step: 2078  | total loss: [1m[32m0.15757[0m[0m | time: 968.354s
| Adam | epoch: 004 | loss: 0.15757 - acc: 0.9399 -- iter: 19900/28000
Training Step: 2079  | total loss: [1m[32m0.15465[0m[0m | time: 969.619s
| Adam | epoch: 004 | loss: 0.15465 - acc: 0.9419 -- iter: 19950/28000
Training Step: 2080  | total loss: [1m[32m0.15270[0m[0m | time: 970.876s
| Adam | epoch: 004 | loss: 0.15270 - acc: 0.9417 -- iter: 20000/28000
Training Step: 2081  | total loss: [1m[32m0.15356[0m[0m | time: 972.119s
| Adam | epoch: 004 | loss: 0.15356 - acc: 0.9455 -- iter: 20050/28000
Training Step: 2082  | total loss: [1m[32m0.14142[0m[0m | time: 973.350s
| Adam | epoch: 004 | loss: 0.14142 - acc: 0.9510 -- iter: 20100/28000
Training Step: 2083  | total loss: [1m[32m0.13538[0m[0m | time: 974.591s
| Adam | epoch: 004 | loss: 0.13538 - acc: 0.9539 -- iter: 20150/28000
Training Step: 2084  | total loss: [1m[32m0.13334[0m[0m | time: 975.832s
| Adam | epoch: 004 | loss: 0.13334 - acc: 0.9545 -- iter: 20200/28000
Training Step: 2085  | total loss: [1m[32m0.13193[0m[0m | time: 977.065s
| Adam | epoch: 004 | loss: 0.13193 - acc: 0.9570 -- iter: 20250/28000
Training Step: 2086  | total loss: [1m[32m0.12512[0m[0m | time: 978.287s
| Adam | epoch: 004 | loss: 0.12512 - acc: 0.9593 -- iter: 20300/28000
Training Step: 2087  | total loss: [1m[32m0.12992[0m[0m | time: 979.536s
| Adam | epoch: 004 | loss: 0.12992 - acc: 0.9554 -- iter: 20350/28000
Training Step: 2088  | total loss: [1m[32m0.13233[0m[0m | time: 980.761s
| Adam | epoch: 004 | loss: 0.13233 - acc: 0.9519 -- iter: 20400/28000
Training Step: 2089  | total loss: [1m[32m0.12818[0m[0m | time: 982.015s
| Adam | epoch: 004 | loss: 0.12818 - acc: 0.9527 -- iter: 20450/28000
Training Step: 2090  | total loss: [1m[32m0.12270[0m[0m | time: 983.265s
| Adam | epoch: 004 | loss: 0.12270 - acc: 0.9534 -- iter: 20500/28000
Training Step: 2091  | total loss: [1m[32m0.11844[0m[0m | time: 984.526s
| Adam | epoch: 004 | loss: 0.11844 - acc: 0.9561 -- iter: 20550/28000
Training Step: 2092  | total loss: [1m[32m0.12978[0m[0m | time: 985.784s
| Adam | epoch: 004 | loss: 0.12978 - acc: 0.9505 -- iter: 20600/28000
Training Step: 2093  | total loss: [1m[32m0.13512[0m[0m | time: 987.017s
| Adam | epoch: 004 | loss: 0.13512 - acc: 0.9454 -- iter: 20650/28000
Training Step: 2094  | total loss: [1m[32m0.13513[0m[0m | time: 988.279s
| Adam | epoch: 004 | loss: 0.13513 - acc: 0.9429 -- iter: 20700/28000
Training Step: 2095  | total loss: [1m[32m0.13286[0m[0m | time: 989.537s
| Adam | epoch: 004 | loss: 0.13286 - acc: 0.9466 -- iter: 20750/28000
Training Step: 2096  | total loss: [1m[32m0.13972[0m[0m | time: 990.788s
| Adam | epoch: 004 | loss: 0.13972 - acc: 0.9439 -- iter: 20800/28000
Training Step: 2097  | total loss: [1m[32m0.13858[0m[0m | time: 992.054s
| Adam | epoch: 004 | loss: 0.13858 - acc: 0.9455 -- iter: 20850/28000
Training Step: 2098  | total loss: [1m[32m0.12908[0m[0m | time: 993.274s
| Adam | epoch: 004 | loss: 0.12908 - acc: 0.9510 -- iter: 20900/28000
Training Step: 2099  | total loss: [1m[32m0.12865[0m[0m | time: 994.504s
| Adam | epoch: 004 | loss: 0.12865 - acc: 0.9499 -- iter: 20950/28000
Training Step: 2100  | total loss: [1m[32m0.13922[0m[0m | time: 1024.654s
| Adam | epoch: 004 | loss: 0.13922 - acc: 0.9489 | val_loss: 0.18936 - val_acc: 0.9344 -- iter: 21000/28000
--
Training Step: 2101  | total loss: [1m[32m0.13356[0m[0m | time: 1025.896s
| Adam | epoch: 004 | loss: 0.13356 - acc: 0.9500 -- iter: 21050/28000
Training Step: 2102  | total loss: [1m[32m0.12837[0m[0m | time: 1027.118s
| Adam | epoch: 004 | loss: 0.12837 - acc: 0.9530 -- iter: 21100/28000
Training Step: 2103  | total loss: [1m[32m0.13646[0m[0m | time: 1028.333s
| Adam | epoch: 004 | loss: 0.13646 - acc: 0.9537 -- iter: 21150/28000
Training Step: 2104  | total loss: [1m[32m0.12910[0m[0m | time: 1029.557s
| Adam | epoch: 004 | loss: 0.12910 - acc: 0.9543 -- iter: 21200/28000
Training Step: 2105  | total loss: [1m[32m0.12174[0m[0m | time: 1030.807s
| Adam | epoch: 004 | loss: 0.12174 - acc: 0.9569 -- iter: 21250/28000
Training Step: 2106  | total loss: [1m[32m0.12830[0m[0m | time: 1032.037s
| Adam | epoch: 004 | loss: 0.12830 - acc: 0.9512 -- iter: 21300/28000
Training Step: 2107  | total loss: [1m[32m0.13109[0m[0m | time: 1033.280s
| Adam | epoch: 004 | loss: 0.13109 - acc: 0.9521 -- iter: 21350/28000
Training Step: 2108  | total loss: [1m[32m0.12622[0m[0m | time: 1034.488s
| Adam | epoch: 004 | loss: 0.12622 - acc: 0.9549 -- iter: 21400/28000
Training Step: 2109  | total loss: [1m[32m0.13687[0m[0m | time: 1035.727s
| Adam | epoch: 004 | loss: 0.13687 - acc: 0.9534 -- iter: 21450/28000
Training Step: 2110  | total loss: [1m[32m0.15057[0m[0m | time: 1036.971s
| Adam | epoch: 004 | loss: 0.15057 - acc: 0.9521 -- iter: 21500/28000
Training Step: 2111  | total loss: [1m[32m0.16001[0m[0m | time: 1038.224s
| Adam | epoch: 004 | loss: 0.16001 - acc: 0.9488 -- iter: 21550/28000
Training Step: 2112  | total loss: [1m[32m0.16086[0m[0m | time: 1039.502s
| Adam | epoch: 004 | loss: 0.16086 - acc: 0.9480 -- iter: 21600/28000
Training Step: 2113  | total loss: [1m[32m0.15763[0m[0m | time: 1040.746s
| Adam | epoch: 004 | loss: 0.15763 - acc: 0.9492 -- iter: 21650/28000
Training Step: 2114  | total loss: [1m[32m0.15911[0m[0m | time: 1041.967s
| Adam | epoch: 004 | loss: 0.15911 - acc: 0.9442 -- iter: 21700/28000
Training Step: 2115  | total loss: [1m[32m0.16142[0m[0m | time: 1043.196s
| Adam | epoch: 004 | loss: 0.16142 - acc: 0.9418 -- iter: 21750/28000
Training Step: 2116  | total loss: [1m[32m0.15328[0m[0m | time: 1044.475s
| Adam | epoch: 004 | loss: 0.15328 - acc: 0.9436 -- iter: 21800/28000
Training Step: 2117  | total loss: [1m[32m0.15173[0m[0m | time: 1045.753s
| Adam | epoch: 004 | loss: 0.15173 - acc: 0.9453 -- iter: 21850/28000
Training Step: 2118  | total loss: [1m[32m0.16105[0m[0m | time: 1047.000s
| Adam | epoch: 004 | loss: 0.16105 - acc: 0.9388 -- iter: 21900/28000
Training Step: 2119  | total loss: [1m[32m0.16164[0m[0m | time: 1048.240s
| Adam | epoch: 004 | loss: 0.16164 - acc: 0.9389 -- iter: 21950/28000
Training Step: 2120  | total loss: [1m[32m0.17334[0m[0m | time: 1049.470s
| Adam | epoch: 004 | loss: 0.17334 - acc: 0.9370 -- iter: 22000/28000
Training Step: 2121  | total loss: [1m[32m0.16353[0m[0m | time: 1050.705s
| Adam | epoch: 004 | loss: 0.16353 - acc: 0.9393 -- iter: 22050/28000
Training Step: 2122  | total loss: [1m[32m0.16419[0m[0m | time: 1051.923s
| Adam | epoch: 004 | loss: 0.16419 - acc: 0.9414 -- iter: 22100/28000
Training Step: 2123  | total loss: [1m[32m0.15790[0m[0m | time: 1053.145s
| Adam | epoch: 004 | loss: 0.15790 - acc: 0.9452 -- iter: 22150/28000
Training Step: 2124  | total loss: [1m[32m0.15299[0m[0m | time: 1054.418s
| Adam | epoch: 004 | loss: 0.15299 - acc: 0.9427 -- iter: 22200/28000
Training Step: 2125  | total loss: [1m[32m0.14438[0m[0m | time: 1084.545s
| Adam | epoch: 004 | loss: 0.14438 - acc: 0.9484 | val_loss: 0.21550 - val_acc: 0.9212 -- iter: 22250/28000
--
Training Step: 2126  | total loss: [1m[32m0.15033[0m[0m | time: 1085.801s
| Adam | epoch: 004 | loss: 0.15033 - acc: 0.9456 -- iter: 22300/28000
Training Step: 2127  | total loss: [1m[32m0.14702[0m[0m | time: 1087.051s
| Adam | epoch: 004 | loss: 0.14702 - acc: 0.9450 -- iter: 22350/28000
Training Step: 2128  | total loss: [1m[32m0.14710[0m[0m | time: 1088.295s
| Adam | epoch: 004 | loss: 0.14710 - acc: 0.9465 -- iter: 22400/28000
Training Step: 2129  | total loss: [1m[32m0.15525[0m[0m | time: 1089.516s
| Adam | epoch: 004 | loss: 0.15525 - acc: 0.9419 -- iter: 22450/28000
Training Step: 2130  | total loss: [1m[32m0.16000[0m[0m | time: 1090.762s
| Adam | epoch: 004 | loss: 0.16000 - acc: 0.9357 -- iter: 22500/28000
Training Step: 2131  | total loss: [1m[32m0.15715[0m[0m | time: 1091.975s
| Adam | epoch: 004 | loss: 0.15715 - acc: 0.9401 -- iter: 22550/28000
Training Step: 2132  | total loss: [1m[32m0.14625[0m[0m | time: 1093.189s
| Adam | epoch: 004 | loss: 0.14625 - acc: 0.9461 -- iter: 22600/28000
Training Step: 2133  | total loss: [1m[32m0.14653[0m[0m | time: 1094.426s
| Adam | epoch: 004 | loss: 0.14653 - acc: 0.9495 -- iter: 22650/28000
Training Step: 2134  | total loss: [1m[32m0.13964[0m[0m | time: 1095.638s
| Adam | epoch: 004 | loss: 0.13964 - acc: 0.9525 -- iter: 22700/28000
Training Step: 2135  | total loss: [1m[32m0.13110[0m[0m | time: 1096.919s
| Adam | epoch: 004 | loss: 0.13110 - acc: 0.9573 -- iter: 22750/28000
Training Step: 2136  | total loss: [1m[32m0.13426[0m[0m | time: 1098.236s
| Adam | epoch: 004 | loss: 0.13426 - acc: 0.9556 -- iter: 22800/28000
Training Step: 2137  | total loss: [1m[32m0.15130[0m[0m | time: 1099.565s
| Adam | epoch: 004 | loss: 0.15130 - acc: 0.9500 -- iter: 22850/28000
Training Step: 2138  | total loss: [1m[32m0.15615[0m[0m | time: 1100.850s
| Adam | epoch: 004 | loss: 0.15615 - acc: 0.9510 -- iter: 22900/28000
Training Step: 2139  | total loss: [1m[32m0.15804[0m[0m | time: 1102.163s
| Adam | epoch: 004 | loss: 0.15804 - acc: 0.9459 -- iter: 22950/28000
Training Step: 2140  | total loss: [1m[32m0.15307[0m[0m | time: 1103.482s
| Adam | epoch: 004 | loss: 0.15307 - acc: 0.9473 -- iter: 23000/28000
Training Step: 2141  | total loss: [1m[32m0.14789[0m[0m | time: 1104.736s
| Adam | epoch: 004 | loss: 0.14789 - acc: 0.9506 -- iter: 23050/28000
Training Step: 2142  | total loss: [1m[32m0.14361[0m[0m | time: 1105.966s
| Adam | epoch: 004 | loss: 0.14361 - acc: 0.9515 -- iter: 23100/28000
Training Step: 2143  | total loss: [1m[32m0.15139[0m[0m | time: 1107.222s
| Adam | epoch: 004 | loss: 0.15139 - acc: 0.9464 -- iter: 23150/28000
Training Step: 2144  | total loss: [1m[32m0.14423[0m[0m | time: 1108.466s
| Adam | epoch: 004 | loss: 0.14423 - acc: 0.9497 -- iter: 23200/28000
Training Step: 2145  | total loss: [1m[32m0.13917[0m[0m | time: 1109.699s
| Adam | epoch: 004 | loss: 0.13917 - acc: 0.9488 -- iter: 23250/28000
Training Step: 2146  | total loss: [1m[32m0.13297[0m[0m | time: 1110.958s
| Adam | epoch: 004 | loss: 0.13297 - acc: 0.9499 -- iter: 23300/28000
Training Step: 2147  | total loss: [1m[32m0.12807[0m[0m | time: 1112.202s
| Adam | epoch: 004 | loss: 0.12807 - acc: 0.9529 -- iter: 23350/28000
Training Step: 2148  | total loss: [1m[32m0.12090[0m[0m | time: 1113.454s
| Adam | epoch: 004 | loss: 0.12090 - acc: 0.9556 -- iter: 23400/28000
Training Step: 2149  | total loss: [1m[32m0.12970[0m[0m | time: 1114.735s
| Adam | epoch: 004 | loss: 0.12970 - acc: 0.9540 -- iter: 23450/28000
Training Step: 2150  | total loss: [1m[32m0.12889[0m[0m | time: 1144.630s
| Adam | epoch: 004 | loss: 0.12889 - acc: 0.9546 | val_loss: 0.18667 - val_acc: 0.9321 -- iter: 23500/28000
--
Training Step: 2151  | total loss: [1m[32m0.11927[0m[0m | time: 1145.868s
| Adam | epoch: 004 | loss: 0.11927 - acc: 0.9592 -- iter: 23550/28000
Training Step: 2152  | total loss: [1m[32m0.11550[0m[0m | time: 1147.098s
| Adam | epoch: 004 | loss: 0.11550 - acc: 0.9593 -- iter: 23600/28000
Training Step: 2153  | total loss: [1m[32m0.12645[0m[0m | time: 1148.348s
| Adam | epoch: 004 | loss: 0.12645 - acc: 0.9553 -- iter: 23650/28000
Training Step: 2154  | total loss: [1m[32m0.12497[0m[0m | time: 1149.590s
| Adam | epoch: 004 | loss: 0.12497 - acc: 0.9538 -- iter: 23700/28000
Training Step: 2155  | total loss: [1m[32m0.11945[0m[0m | time: 1150.823s
| Adam | epoch: 004 | loss: 0.11945 - acc: 0.9564 -- iter: 23750/28000
Training Step: 2156  | total loss: [1m[32m0.11490[0m[0m | time: 1152.068s
| Adam | epoch: 004 | loss: 0.11490 - acc: 0.9588 -- iter: 23800/28000
Training Step: 2157  | total loss: [1m[32m0.11106[0m[0m | time: 1153.310s
| Adam | epoch: 004 | loss: 0.11106 - acc: 0.9589 -- iter: 23850/28000
Training Step: 2158  | total loss: [1m[32m0.10918[0m[0m | time: 1154.567s
| Adam | epoch: 004 | loss: 0.10918 - acc: 0.9610 -- iter: 23900/28000
Training Step: 2159  | total loss: [1m[32m0.12814[0m[0m | time: 1155.788s
| Adam | epoch: 004 | loss: 0.12814 - acc: 0.9589 -- iter: 23950/28000
Training Step: 2160  | total loss: [1m[32m0.11824[0m[0m | time: 1157.034s
| Adam | epoch: 004 | loss: 0.11824 - acc: 0.9630 -- iter: 24000/28000
Training Step: 2161  | total loss: [1m[32m0.12043[0m[0m | time: 1158.290s
| Adam | epoch: 004 | loss: 0.12043 - acc: 0.9607 -- iter: 24050/28000
Training Step: 2162  | total loss: [1m[32m0.11462[0m[0m | time: 1159.552s
| Adam | epoch: 004 | loss: 0.11462 - acc: 0.9626 -- iter: 24100/28000
Training Step: 2163  | total loss: [1m[32m0.11513[0m[0m | time: 1160.793s
| Adam | epoch: 004 | loss: 0.11513 - acc: 0.9604 -- iter: 24150/28000
Training Step: 2164  | total loss: [1m[32m0.11508[0m[0m | time: 1162.016s
| Adam | epoch: 004 | loss: 0.11508 - acc: 0.9603 -- iter: 24200/28000
Training Step: 2165  | total loss: [1m[32m0.12071[0m[0m | time: 1163.306s
| Adam | epoch: 004 | loss: 0.12071 - acc: 0.9603 -- iter: 24250/28000
Training Step: 2166  | total loss: [1m[32m0.11443[0m[0m | time: 1164.573s
| Adam | epoch: 004 | loss: 0.11443 - acc: 0.9603 -- iter: 24300/28000
Training Step: 2167  | total loss: [1m[32m0.13184[0m[0m | time: 1165.859s
| Adam | epoch: 004 | loss: 0.13184 - acc: 0.9542 -- iter: 24350/28000
Training Step: 2168  | total loss: [1m[32m0.16117[0m[0m | time: 1167.087s
| Adam | epoch: 004 | loss: 0.16117 - acc: 0.9488 -- iter: 24400/28000
Training Step: 2169  | total loss: [1m[32m0.16495[0m[0m | time: 1168.304s
| Adam | epoch: 004 | loss: 0.16495 - acc: 0.9439 -- iter: 24450/28000
Training Step: 2170  | total loss: [1m[32m0.17330[0m[0m | time: 1169.542s
| Adam | epoch: 004 | loss: 0.17330 - acc: 0.9435 -- iter: 24500/28000
Training Step: 2171  | total loss: [1m[32m0.19382[0m[0m | time: 1170.779s
| Adam | epoch: 004 | loss: 0.19382 - acc: 0.9312 -- iter: 24550/28000
Training Step: 2172  | total loss: [1m[32m0.18506[0m[0m | time: 1172.018s
| Adam | epoch: 004 | loss: 0.18506 - acc: 0.9341 -- iter: 24600/28000
Training Step: 2173  | total loss: [1m[32m0.18744[0m[0m | time: 1173.288s
| Adam | epoch: 004 | loss: 0.18744 - acc: 0.9327 -- iter: 24650/28000
Training Step: 2174  | total loss: [1m[32m0.18177[0m[0m | time: 1174.564s
| Adam | epoch: 004 | loss: 0.18177 - acc: 0.9354 -- iter: 24700/28000
Training Step: 2175  | total loss: [1m[32m0.18093[0m[0m | time: 1204.861s
| Adam | epoch: 004 | loss: 0.18093 - acc: 0.9339 | val_loss: 0.27261 - val_acc: 0.8879 -- iter: 24750/28000
--
Training Step: 2176  | total loss: [1m[32m0.18314[0m[0m | time: 1206.124s
| Adam | epoch: 004 | loss: 0.18314 - acc: 0.9285 -- iter: 24800/28000
Training Step: 2177  | total loss: [1m[32m0.18398[0m[0m | time: 1207.376s
| Adam | epoch: 004 | loss: 0.18398 - acc: 0.9256 -- iter: 24850/28000
Training Step: 2178  | total loss: [1m[32m0.18466[0m[0m | time: 1208.638s
| Adam | epoch: 004 | loss: 0.18466 - acc: 0.9251 -- iter: 24900/28000
Training Step: 2179  | total loss: [1m[32m0.18627[0m[0m | time: 1210.013s
| Adam | epoch: 004 | loss: 0.18627 - acc: 0.9266 -- iter: 24950/28000
Training Step: 2180  | total loss: [1m[32m0.17999[0m[0m | time: 1211.367s
| Adam | epoch: 004 | loss: 0.17999 - acc: 0.9299 -- iter: 25000/28000
Training Step: 2181  | total loss: [1m[32m0.17437[0m[0m | time: 1212.679s
| Adam | epoch: 004 | loss: 0.17437 - acc: 0.9349 -- iter: 25050/28000
Training Step: 2182  | total loss: [1m[32m0.17876[0m[0m | time: 1213.917s
| Adam | epoch: 004 | loss: 0.17876 - acc: 0.9314 -- iter: 25100/28000
Training Step: 2183  | total loss: [1m[32m0.18905[0m[0m | time: 1215.131s
| Adam | epoch: 004 | loss: 0.18905 - acc: 0.9303 -- iter: 25150/28000
Training Step: 2184  | total loss: [1m[32m0.18653[0m[0m | time: 1216.347s
| Adam | epoch: 004 | loss: 0.18653 - acc: 0.9293 -- iter: 25200/28000
Training Step: 2185  | total loss: [1m[32m0.17775[0m[0m | time: 1217.579s
| Adam | epoch: 004 | loss: 0.17775 - acc: 0.9303 -- iter: 25250/28000
Training Step: 2186  | total loss: [1m[32m0.16837[0m[0m | time: 1218.819s
| Adam | epoch: 004 | loss: 0.16837 - acc: 0.9333 -- iter: 25300/28000
Training Step: 2187  | total loss: [1m[32m0.15806[0m[0m | time: 1220.040s
| Adam | epoch: 004 | loss: 0.15806 - acc: 0.9400 -- iter: 25350/28000
Training Step: 2188  | total loss: [1m[32m0.15189[0m[0m | time: 1221.272s
| Adam | epoch: 004 | loss: 0.15189 - acc: 0.9420 -- iter: 25400/28000
Training Step: 2189  | total loss: [1m[32m0.17101[0m[0m | time: 1222.519s
| Adam | epoch: 004 | loss: 0.17101 - acc: 0.9318 -- iter: 25450/28000
Training Step: 2190  | total loss: [1m[32m0.18255[0m[0m | time: 1223.737s
| Adam | epoch: 004 | loss: 0.18255 - acc: 0.9306 -- iter: 25500/28000
Training Step: 2191  | total loss: [1m[32m0.18015[0m[0m | time: 1224.979s
| Adam | epoch: 004 | loss: 0.18015 - acc: 0.9315 -- iter: 25550/28000
Training Step: 2192  | total loss: [1m[32m0.17311[0m[0m | time: 1226.221s
| Adam | epoch: 004 | loss: 0.17311 - acc: 0.9364 -- iter: 25600/28000
Training Step: 2193  | total loss: [1m[32m0.16603[0m[0m | time: 1227.452s
| Adam | epoch: 004 | loss: 0.16603 - acc: 0.9367 -- iter: 25650/28000
Training Step: 2194  | total loss: [1m[32m0.15772[0m[0m | time: 1228.757s
| Adam | epoch: 004 | loss: 0.15772 - acc: 0.9391 -- iter: 25700/28000
Training Step: 2195  | total loss: [1m[32m0.16227[0m[0m | time: 1230.064s
| Adam | epoch: 004 | loss: 0.16227 - acc: 0.9392 -- iter: 25750/28000
Training Step: 2196  | total loss: [1m[32m0.17869[0m[0m | time: 1231.387s
| Adam | epoch: 004 | loss: 0.17869 - acc: 0.9332 -- iter: 25800/28000
Training Step: 2197  | total loss: [1m[32m0.16560[0m[0m | time: 1232.629s
| Adam | epoch: 004 | loss: 0.16560 - acc: 0.9399 -- iter: 25850/28000
Training Step: 2198  | total loss: [1m[32m0.16202[0m[0m | time: 1233.855s
| Adam | epoch: 004 | loss: 0.16202 - acc: 0.9419 -- iter: 25900/28000
Training Step: 2199  | total loss: [1m[32m0.16024[0m[0m | time: 1235.144s
| Adam | epoch: 004 | loss: 0.16024 - acc: 0.9457 -- iter: 25950/28000
Training Step: 2200  | total loss: [1m[32m0.15435[0m[0m | time: 1264.898s
| Adam | epoch: 004 | loss: 0.15435 - acc: 0.9452 | val_loss: 0.18327 - val_acc: 0.9364 -- iter: 26000/28000
--
Training Step: 2201  | total loss: [1m[32m0.16074[0m[0m | time: 1266.166s
| Adam | epoch: 004 | loss: 0.16074 - acc: 0.9446 -- iter: 26050/28000
Training Step: 2202  | total loss: [1m[32m0.15275[0m[0m | time: 1267.418s
| Adam | epoch: 004 | loss: 0.15275 - acc: 0.9462 -- iter: 26100/28000
Training Step: 2203  | total loss: [1m[32m0.16147[0m[0m | time: 1268.703s
| Adam | epoch: 004 | loss: 0.16147 - acc: 0.9436 -- iter: 26150/28000
Training Step: 2204  | total loss: [1m[32m0.15767[0m[0m | time: 1269.971s
| Adam | epoch: 004 | loss: 0.15767 - acc: 0.9472 -- iter: 26200/28000
Training Step: 2205  | total loss: [1m[32m0.14870[0m[0m | time: 1271.247s
| Adam | epoch: 004 | loss: 0.14870 - acc: 0.9505 -- iter: 26250/28000
Training Step: 2206  | total loss: [1m[32m0.14334[0m[0m | time: 1272.521s
| Adam | epoch: 004 | loss: 0.14334 - acc: 0.9534 -- iter: 26300/28000
Training Step: 2207  | total loss: [1m[32m0.13473[0m[0m | time: 1273.863s
| Adam | epoch: 004 | loss: 0.13473 - acc: 0.9581 -- iter: 26350/28000
Training Step: 2208  | total loss: [1m[32m0.12317[0m[0m | time: 1275.174s
| Adam | epoch: 004 | loss: 0.12317 - acc: 0.9623 -- iter: 26400/28000
Training Step: 2209  | total loss: [1m[32m0.12062[0m[0m | time: 1276.456s
| Adam | epoch: 004 | loss: 0.12062 - acc: 0.9621 -- iter: 26450/28000
Training Step: 2210  | total loss: [1m[32m0.12285[0m[0m | time: 1277.754s
| Adam | epoch: 004 | loss: 0.12285 - acc: 0.9599 -- iter: 26500/28000
Training Step: 2211  | total loss: [1m[32m0.14187[0m[0m | time: 1279.050s
| Adam | epoch: 004 | loss: 0.14187 - acc: 0.9499 -- iter: 26550/28000
Training Step: 2212  | total loss: [1m[32m0.15392[0m[0m | time: 1280.311s
| Adam | epoch: 004 | loss: 0.15392 - acc: 0.9469 -- iter: 26600/28000
Training Step: 2213  | total loss: [1m[32m0.15071[0m[0m | time: 1281.583s
| Adam | epoch: 004 | loss: 0.15071 - acc: 0.9462 -- iter: 26650/28000
Training Step: 2214  | total loss: [1m[32m0.14471[0m[0m | time: 1282.869s
| Adam | epoch: 004 | loss: 0.14471 - acc: 0.9476 -- iter: 26700/28000
Training Step: 2215  | total loss: [1m[32m0.14328[0m[0m | time: 1284.158s
| Adam | epoch: 004 | loss: 0.14328 - acc: 0.9488 -- iter: 26750/28000
Training Step: 2216  | total loss: [1m[32m0.13412[0m[0m | time: 1285.429s
| Adam | epoch: 004 | loss: 0.13412 - acc: 0.9539 -- iter: 26800/28000
Training Step: 2217  | total loss: [1m[32m0.12785[0m[0m | time: 1286.721s
| Adam | epoch: 004 | loss: 0.12785 - acc: 0.9565 -- iter: 26850/28000
Training Step: 2218  | total loss: [1m[32m0.11837[0m[0m | time: 1288.006s
| Adam | epoch: 004 | loss: 0.11837 - acc: 0.9609 -- iter: 26900/28000
Training Step: 2219  | total loss: [1m[32m0.12668[0m[0m | time: 1289.279s
| Adam | epoch: 004 | loss: 0.12668 - acc: 0.9608 -- iter: 26950/28000
Training Step: 2220  | total loss: [1m[32m0.13550[0m[0m | time: 1290.604s
| Adam | epoch: 004 | loss: 0.13550 - acc: 0.9587 -- iter: 27000/28000
Training Step: 2221  | total loss: [1m[32m0.13896[0m[0m | time: 1291.863s
| Adam | epoch: 004 | loss: 0.13896 - acc: 0.9528 -- iter: 27050/28000
Training Step: 2222  | total loss: [1m[32m0.13964[0m[0m | time: 1293.092s
| Adam | epoch: 004 | loss: 0.13964 - acc: 0.9516 -- iter: 27100/28000
Training Step: 2223  | total loss: [1m[32m0.14008[0m[0m | time: 1294.331s
| Adam | epoch: 004 | loss: 0.14008 - acc: 0.9484 -- iter: 27150/28000
Training Step: 2224  | total loss: [1m[32m0.13758[0m[0m | time: 1295.601s
| Adam | epoch: 004 | loss: 0.13758 - acc: 0.9496 -- iter: 27200/28000
Training Step: 2225  | total loss: [1m[32m0.15102[0m[0m | time: 1325.687s
| Adam | epoch: 004 | loss: 0.15102 - acc: 0.9386 | val_loss: 0.21168 - val_acc: 0.9222 -- iter: 27250/28000
--
Training Step: 2226  | total loss: [1m[32m0.14397[0m[0m | time: 1326.960s
| Adam | epoch: 004 | loss: 0.14397 - acc: 0.9427 -- iter: 27300/28000
Training Step: 2227  | total loss: [1m[32m0.14974[0m[0m | time: 1328.178s
| Adam | epoch: 004 | loss: 0.14974 - acc: 0.9405 -- iter: 27350/28000
Training Step: 2228  | total loss: [1m[32m0.14591[0m[0m | time: 1329.419s
| Adam | epoch: 004 | loss: 0.14591 - acc: 0.9424 -- iter: 27400/28000
Training Step: 2229  | total loss: [1m[32m0.16165[0m[0m | time: 1330.670s
| Adam | epoch: 004 | loss: 0.16165 - acc: 0.9382 -- iter: 27450/28000
Training Step: 2230  | total loss: [1m[32m0.16530[0m[0m | time: 1331.881s
| Adam | epoch: 004 | loss: 0.16530 - acc: 0.9404 -- iter: 27500/28000
Training Step: 2231  | total loss: [1m[32m0.16070[0m[0m | time: 1333.101s
| Adam | epoch: 004 | loss: 0.16070 - acc: 0.9403 -- iter: 27550/28000
Training Step: 2232  | total loss: [1m[32m0.18221[0m[0m | time: 1334.345s
| Adam | epoch: 004 | loss: 0.18221 - acc: 0.9323 -- iter: 27600/28000
Training Step: 2233  | total loss: [1m[32m0.17189[0m[0m | time: 1335.638s
| Adam | epoch: 004 | loss: 0.17189 - acc: 0.9331 -- iter: 27650/28000
Training Step: 2234  | total loss: [1m[32m0.16629[0m[0m | time: 1336.887s
| Adam | epoch: 004 | loss: 0.16629 - acc: 0.9318 -- iter: 27700/28000
Training Step: 2235  | total loss: [1m[32m0.15586[0m[0m | time: 1338.135s
| Adam | epoch: 004 | loss: 0.15586 - acc: 0.9366 -- iter: 27750/28000
Training Step: 2236  | total loss: [1m[32m0.17589[0m[0m | time: 1339.406s
| Adam | epoch: 004 | loss: 0.17589 - acc: 0.9309 -- iter: 27800/28000
Training Step: 2237  | total loss: [1m[32m0.19873[0m[0m | time: 1340.627s
| Adam | epoch: 004 | loss: 0.19873 - acc: 0.9258 -- iter: 27850/28000
Training Step: 2238  | total loss: [1m[32m0.19643[0m[0m | time: 1341.808s
| Adam | epoch: 004 | loss: 0.19643 - acc: 0.9272 -- iter: 27900/28000
Training Step: 2239  | total loss: [1m[32m0.19853[0m[0m | time: 1343.078s
| Adam | epoch: 004 | loss: 0.19853 - acc: 0.9265 -- iter: 27950/28000
Training Step: 2240  | total loss: [1m[32m0.19278[0m[0m | time: 1344.295s
| Adam | epoch: 004 | loss: 0.19278 - acc: 0.9279 -- iter: 28000/28000
Training Step: 2241  | total loss: [1m[32m0.18580[0m[0m | time: 1.235s
| Adam | epoch: 005 | loss: 0.18580 - acc: 0.9311 -- iter: 00050/28000
Training Step: 2242  | total loss: [1m[32m0.18734[0m[0m | time: 2.493s
| Adam | epoch: 005 | loss: 0.18734 - acc: 0.9280 -- iter: 00100/28000
Training Step: 2243  | total loss: [1m[32m0.18298[0m[0m | time: 3.734s
| Adam | epoch: 005 | loss: 0.18298 - acc: 0.9312 -- iter: 00150/28000
Training Step: 2244  | total loss: [1m[32m0.17994[0m[0m | time: 4.990s
| Adam | epoch: 005 | loss: 0.17994 - acc: 0.9321 -- iter: 00200/28000
Training Step: 2245  | total loss: [1m[32m0.17731[0m[0m | time: 6.233s
| Adam | epoch: 005 | loss: 0.17731 - acc: 0.9329 -- iter: 00250/28000
Training Step: 2246  | total loss: [1m[32m0.18314[0m[0m | time: 7.463s
| Adam | epoch: 005 | loss: 0.18314 - acc: 0.9336 -- iter: 00300/28000
Training Step: 2247  | total loss: [1m[32m0.18290[0m[0m | time: 8.704s
| Adam | epoch: 005 | loss: 0.18290 - acc: 0.9322 -- iter: 00350/28000
Training Step: 2248  | total loss: [1m[32m0.18762[0m[0m | time: 9.942s
| Adam | epoch: 005 | loss: 0.18762 - acc: 0.9290 -- iter: 00400/28000
Training Step: 2249  | total loss: [1m[32m0.18534[0m[0m | time: 11.183s
| Adam | epoch: 005 | loss: 0.18534 - acc: 0.9301 -- iter: 00450/28000
Training Step: 2250  | total loss: [1m[32m0.18666[0m[0m | time: 40.857s
| Adam | epoch: 005 | loss: 0.18666 - acc: 0.9311 | val_loss: 0.20156 - val_acc: 0.9230 -- iter: 00500/28000
--
Training Step: 2251  | total loss: [1m[32m0.18491[0m[0m | time: 42.128s
| Adam | epoch: 005 | loss: 0.18491 - acc: 0.9320 -- iter: 00550/28000
Training Step: 2252  | total loss: [1m[32m0.18286[0m[0m | time: 43.354s
| Adam | epoch: 005 | loss: 0.18286 - acc: 0.9348 -- iter: 00600/28000
Training Step: 2253  | total loss: [1m[32m0.17428[0m[0m | time: 44.602s
| Adam | epoch: 005 | loss: 0.17428 - acc: 0.9353 -- iter: 00650/28000
Training Step: 2254  | total loss: [1m[32m0.16585[0m[0m | time: 45.839s
| Adam | epoch: 005 | loss: 0.16585 - acc: 0.9358 -- iter: 00700/28000
Training Step: 2255  | total loss: [1m[32m0.16302[0m[0m | time: 47.095s
| Adam | epoch: 005 | loss: 0.16302 - acc: 0.9362 -- iter: 00750/28000
Training Step: 2256  | total loss: [1m[32m0.15396[0m[0m | time: 48.345s
| Adam | epoch: 005 | loss: 0.15396 - acc: 0.9406 -- iter: 00800/28000
Training Step: 2257  | total loss: [1m[32m0.15617[0m[0m | time: 49.594s
| Adam | epoch: 005 | loss: 0.15617 - acc: 0.9425 -- iter: 00850/28000
Training Step: 2258  | total loss: [1m[32m0.16000[0m[0m | time: 50.849s
| Adam | epoch: 005 | loss: 0.16000 - acc: 0.9423 -- iter: 00900/28000
Training Step: 2259  | total loss: [1m[32m0.15756[0m[0m | time: 52.118s
| Adam | epoch: 005 | loss: 0.15756 - acc: 0.9380 -- iter: 00950/28000
Training Step: 2260  | total loss: [1m[32m0.15827[0m[0m | time: 53.395s
| Adam | epoch: 005 | loss: 0.15827 - acc: 0.9382 -- iter: 01000/28000
Training Step: 2261  | total loss: [1m[32m0.15439[0m[0m | time: 54.674s
| Adam | epoch: 005 | loss: 0.15439 - acc: 0.9384 -- iter: 01050/28000
Training Step: 2262  | total loss: [1m[32m0.14394[0m[0m | time: 55.913s
| Adam | epoch: 005 | loss: 0.14394 - acc: 0.9426 -- iter: 01100/28000
Training Step: 2263  | total loss: [1m[32m0.14632[0m[0m | time: 57.163s
| Adam | epoch: 005 | loss: 0.14632 - acc: 0.9423 -- iter: 01150/28000
Training Step: 2264  | total loss: [1m[32m0.14093[0m[0m | time: 58.426s
| Adam | epoch: 005 | loss: 0.14093 - acc: 0.9421 -- iter: 01200/28000
Training Step: 2265  | total loss: [1m[32m0.14560[0m[0m | time: 59.680s
| Adam | epoch: 005 | loss: 0.14560 - acc: 0.9399 -- iter: 01250/28000
Training Step: 2266  | total loss: [1m[32m0.14558[0m[0m | time: 60.930s
| Adam | epoch: 005 | loss: 0.14558 - acc: 0.9399 -- iter: 01300/28000
Training Step: 2267  | total loss: [1m[32m0.14241[0m[0m | time: 62.180s
| Adam | epoch: 005 | loss: 0.14241 - acc: 0.9399 -- iter: 01350/28000
Training Step: 2268  | total loss: [1m[32m0.14359[0m[0m | time: 63.435s
| Adam | epoch: 005 | loss: 0.14359 - acc: 0.9379 -- iter: 01400/28000
Training Step: 2269  | total loss: [1m[32m0.13846[0m[0m | time: 64.727s
| Adam | epoch: 005 | loss: 0.13846 - acc: 0.9401 -- iter: 01450/28000
Training Step: 2270  | total loss: [1m[32m0.15987[0m[0m | time: 66.007s
| Adam | epoch: 005 | loss: 0.15987 - acc: 0.9401 -- iter: 01500/28000
Training Step: 2271  | total loss: [1m[32m0.15484[0m[0m | time: 67.312s
| Adam | epoch: 005 | loss: 0.15484 - acc: 0.9441 -- iter: 01550/28000
Training Step: 2272  | total loss: [1m[32m0.16061[0m[0m | time: 68.572s
| Adam | epoch: 005 | loss: 0.16061 - acc: 0.9417 -- iter: 01600/28000
Training Step: 2273  | total loss: [1m[32m0.15322[0m[0m | time: 69.821s
| Adam | epoch: 005 | loss: 0.15322 - acc: 0.9435 -- iter: 01650/28000
Training Step: 2274  | total loss: [1m[32m0.15108[0m[0m | time: 71.094s
| Adam | epoch: 005 | loss: 0.15108 - acc: 0.9452 -- iter: 01700/28000
Training Step: 2275  | total loss: [1m[32m0.14354[0m[0m | time: 101.166s
| Adam | epoch: 005 | loss: 0.14354 - acc: 0.9506 | val_loss: 0.19637 - val_acc: 0.9253 -- iter: 01750/28000
--
Training Step: 2276  | total loss: [1m[32m0.14481[0m[0m | time: 102.442s
| Adam | epoch: 005 | loss: 0.14481 - acc: 0.9496 -- iter: 01800/28000
Training Step: 2277  | total loss: [1m[32m0.15192[0m[0m | time: 103.690s
| Adam | epoch: 005 | loss: 0.15192 - acc: 0.9486 -- iter: 01850/28000
Training Step: 2278  | total loss: [1m[32m0.14939[0m[0m | time: 104.927s
| Adam | epoch: 005 | loss: 0.14939 - acc: 0.9478 -- iter: 01900/28000
Training Step: 2279  | total loss: [1m[32m0.16150[0m[0m | time: 106.195s
| Adam | epoch: 005 | loss: 0.16150 - acc: 0.9450 -- iter: 01950/28000
Training Step: 2280  | total loss: [1m[32m0.15383[0m[0m | time: 107.461s
| Adam | epoch: 005 | loss: 0.15383 - acc: 0.9485 -- iter: 02000/28000
Training Step: 2281  | total loss: [1m[32m0.14721[0m[0m | time: 108.713s
| Adam | epoch: 005 | loss: 0.14721 - acc: 0.9496 -- iter: 02050/28000
Training Step: 2282  | total loss: [1m[32m0.15193[0m[0m | time: 109.965s
| Adam | epoch: 005 | loss: 0.15193 - acc: 0.9487 -- iter: 02100/28000
Training Step: 2283  | total loss: [1m[32m0.14509[0m[0m | time: 111.234s
| Adam | epoch: 005 | loss: 0.14509 - acc: 0.9498 -- iter: 02150/28000
Training Step: 2284  | total loss: [1m[32m0.15258[0m[0m | time: 112.501s
| Adam | epoch: 005 | loss: 0.15258 - acc: 0.9468 -- iter: 02200/28000
Training Step: 2285  | total loss: [1m[32m0.14660[0m[0m | time: 113.776s
| Adam | epoch: 005 | loss: 0.14660 - acc: 0.9481 -- iter: 02250/28000
Training Step: 2286  | total loss: [1m[32m0.13895[0m[0m | time: 115.058s
| Adam | epoch: 005 | loss: 0.13895 - acc: 0.9533 -- iter: 02300/28000
Training Step: 2287  | total loss: [1m[32m0.13996[0m[0m | time: 116.338s
| Adam | epoch: 005 | loss: 0.13996 - acc: 0.9540 -- iter: 02350/28000
Training Step: 2288  | total loss: [1m[32m0.15860[0m[0m | time: 117.596s
| Adam | epoch: 005 | loss: 0.15860 - acc: 0.9486 -- iter: 02400/28000
Training Step: 2289  | total loss: [1m[32m0.15890[0m[0m | time: 118.853s
| Adam | epoch: 005 | loss: 0.15890 - acc: 0.9457 -- iter: 02450/28000
Training Step: 2290  | total loss: [1m[32m0.15634[0m[0m | time: 120.107s
| Adam | epoch: 005 | loss: 0.15634 - acc: 0.9452 -- iter: 02500/28000
Training Step: 2291  | total loss: [1m[32m0.16350[0m[0m | time: 121.379s
| Adam | epoch: 005 | loss: 0.16350 - acc: 0.9426 -- iter: 02550/28000
Training Step: 2292  | total loss: [1m[32m0.16529[0m[0m | time: 122.650s
| Adam | epoch: 005 | loss: 0.16529 - acc: 0.9444 -- iter: 02600/28000
Training Step: 2293  | total loss: [1m[32m0.16745[0m[0m | time: 123.904s
| Adam | epoch: 005 | loss: 0.16745 - acc: 0.9419 -- iter: 02650/28000
Training Step: 2294  | total loss: [1m[32m0.17040[0m[0m | time: 125.146s
| Adam | epoch: 005 | loss: 0.17040 - acc: 0.9397 -- iter: 02700/28000
Training Step: 2295  | total loss: [1m[32m0.16477[0m[0m | time: 126.430s
| Adam | epoch: 005 | loss: 0.16477 - acc: 0.9398 -- iter: 02750/28000
Training Step: 2296  | total loss: [1m[32m0.15913[0m[0m | time: 127.703s
| Adam | epoch: 005 | loss: 0.15913 - acc: 0.9418 -- iter: 02800/28000
Training Step: 2297  | total loss: [1m[32m0.15640[0m[0m | time: 128.959s
| Adam | epoch: 005 | loss: 0.15640 - acc: 0.9416 -- iter: 02850/28000
Training Step: 2298  | total loss: [1m[32m0.15225[0m[0m | time: 130.215s
| Adam | epoch: 005 | loss: 0.15225 - acc: 0.9435 -- iter: 02900/28000
Training Step: 2299  | total loss: [1m[32m0.15053[0m[0m | time: 131.474s
| Adam | epoch: 005 | loss: 0.15053 - acc: 0.9431 -- iter: 02950/28000
Training Step: 2300  | total loss: [1m[32m0.15161[0m[0m | time: 161.405s
| Adam | epoch: 005 | loss: 0.15161 - acc: 0.9428 | val_loss: 0.21391 - val_acc: 0.9166 -- iter: 03000/28000
--
Training Step: 2301  | total loss: [1m[32m0.15904[0m[0m | time: 162.728s
| Adam | epoch: 005 | loss: 0.15904 - acc: 0.9405 -- iter: 03050/28000
Training Step: 2302  | total loss: [1m[32m0.15213[0m[0m | time: 164.057s
| Adam | epoch: 005 | loss: 0.15213 - acc: 0.9425 -- iter: 03100/28000
Training Step: 2303  | total loss: [1m[32m0.15705[0m[0m | time: 165.402s
| Adam | epoch: 005 | loss: 0.15705 - acc: 0.9402 -- iter: 03150/28000
Training Step: 2304  | total loss: [1m[32m0.16245[0m[0m | time: 166.708s
| Adam | epoch: 005 | loss: 0.16245 - acc: 0.9382 -- iter: 03200/28000
Training Step: 2305  | total loss: [1m[32m0.16341[0m[0m | time: 168.046s
| Adam | epoch: 005 | loss: 0.16341 - acc: 0.9364 -- iter: 03250/28000
Training Step: 2306  | total loss: [1m[32m0.15670[0m[0m | time: 169.309s
| Adam | epoch: 005 | loss: 0.15670 - acc: 0.9387 -- iter: 03300/28000
Training Step: 2307  | total loss: [1m[32m0.15494[0m[0m | time: 170.554s
| Adam | epoch: 005 | loss: 0.15494 - acc: 0.9389 -- iter: 03350/28000
Training Step: 2308  | total loss: [1m[32m0.17269[0m[0m | time: 171.814s
| Adam | epoch: 005 | loss: 0.17269 - acc: 0.9370 -- iter: 03400/28000
Training Step: 2309  | total loss: [1m[32m0.17699[0m[0m | time: 173.073s
| Adam | epoch: 005 | loss: 0.17699 - acc: 0.9373 -- iter: 03450/28000
Training Step: 2310  | total loss: [1m[32m0.17814[0m[0m | time: 174.371s
| Adam | epoch: 005 | loss: 0.17814 - acc: 0.9356 -- iter: 03500/28000
Training Step: 2311  | total loss: [1m[32m0.18158[0m[0m | time: 175.593s
| Adam | epoch: 005 | loss: 0.18158 - acc: 0.9320 -- iter: 03550/28000
Training Step: 2312  | total loss: [1m[32m0.16953[0m[0m | time: 176.859s
| Adam | epoch: 005 | loss: 0.16953 - acc: 0.9388 -- iter: 03600/28000
Training Step: 2313  | total loss: [1m[32m0.16770[0m[0m | time: 178.103s
| Adam | epoch: 005 | loss: 0.16770 - acc: 0.9389 -- iter: 03650/28000
Training Step: 2314  | total loss: [1m[32m0.17689[0m[0m | time: 179.356s
| Adam | epoch: 005 | loss: 0.17689 - acc: 0.9410 -- iter: 03700/28000
Training Step: 2315  | total loss: [1m[32m0.16811[0m[0m | time: 180.592s
| Adam | epoch: 005 | loss: 0.16811 - acc: 0.9449 -- iter: 03750/28000
Training Step: 2316  | total loss: [1m[32m0.18966[0m[0m | time: 181.829s
| Adam | epoch: 005 | loss: 0.18966 - acc: 0.9364 -- iter: 03800/28000
Training Step: 2317  | total loss: [1m[32m0.19361[0m[0m | time: 183.077s
| Adam | epoch: 005 | loss: 0.19361 - acc: 0.9348 -- iter: 03850/28000
Training Step: 2318  | total loss: [1m[32m0.19520[0m[0m | time: 184.344s
| Adam | epoch: 005 | loss: 0.19520 - acc: 0.9353 -- iter: 03900/28000
Training Step: 2319  | total loss: [1m[32m0.18760[0m[0m | time: 185.585s
| Adam | epoch: 005 | loss: 0.18760 - acc: 0.9358 -- iter: 03950/28000
Training Step: 2320  | total loss: [1m[32m0.19300[0m[0m | time: 186.830s
| Adam | epoch: 005 | loss: 0.19300 - acc: 0.9382 -- iter: 04000/28000
Training Step: 2321  | total loss: [1m[32m0.18822[0m[0m | time: 188.095s
| Adam | epoch: 005 | loss: 0.18822 - acc: 0.9384 -- iter: 04050/28000
Training Step: 2322  | total loss: [1m[32m0.20204[0m[0m | time: 189.336s
| Adam | epoch: 005 | loss: 0.20204 - acc: 0.9305 -- iter: 04100/28000
Training Step: 2323  | total loss: [1m[32m0.20349[0m[0m | time: 190.597s
| Adam | epoch: 005 | loss: 0.20349 - acc: 0.9255 -- iter: 04150/28000
Training Step: 2324  | total loss: [1m[32m0.20103[0m[0m | time: 191.834s
| Adam | epoch: 005 | loss: 0.20103 - acc: 0.9249 -- iter: 04200/28000
Training Step: 2325  | total loss: [1m[32m0.20236[0m[0m | time: 221.811s
| Adam | epoch: 005 | loss: 0.20236 - acc: 0.9304 | val_loss: 0.18415 - val_acc: 0.9347 -- iter: 04250/28000
--
Training Step: 2326  | total loss: [1m[32m0.19879[0m[0m | time: 223.130s
| Adam | epoch: 005 | loss: 0.19879 - acc: 0.9314 -- iter: 04300/28000
Training Step: 2327  | total loss: [1m[32m0.19403[0m[0m | time: 224.359s
| Adam | epoch: 005 | loss: 0.19403 - acc: 0.9303 -- iter: 04350/28000
Training Step: 2328  | total loss: [1m[32m0.18564[0m[0m | time: 225.589s
| Adam | epoch: 005 | loss: 0.18564 - acc: 0.9352 -- iter: 04400/28000
Training Step: 2329  | total loss: [1m[32m0.17776[0m[0m | time: 226.799s
| Adam | epoch: 005 | loss: 0.17776 - acc: 0.9377 -- iter: 04450/28000
Training Step: 2330  | total loss: [1m[32m0.17693[0m[0m | time: 228.039s
| Adam | epoch: 005 | loss: 0.17693 - acc: 0.9339 -- iter: 04500/28000
Training Step: 2331  | total loss: [1m[32m0.17827[0m[0m | time: 229.271s
| Adam | epoch: 005 | loss: 0.17827 - acc: 0.9305 -- iter: 04550/28000
Training Step: 2332  | total loss: [1m[32m0.18027[0m[0m | time: 230.513s
| Adam | epoch: 005 | loss: 0.18027 - acc: 0.9315 -- iter: 04600/28000
Training Step: 2333  | total loss: [1m[32m0.17149[0m[0m | time: 231.729s
| Adam | epoch: 005 | loss: 0.17149 - acc: 0.9343 -- iter: 04650/28000
Training Step: 2334  | total loss: [1m[32m0.17254[0m[0m | time: 232.959s
| Adam | epoch: 005 | loss: 0.17254 - acc: 0.9329 -- iter: 04700/28000
Training Step: 2335  | total loss: [1m[32m0.17323[0m[0m | time: 234.242s
| Adam | epoch: 005 | loss: 0.17323 - acc: 0.9376 -- iter: 04750/28000
Training Step: 2336  | total loss: [1m[32m0.16797[0m[0m | time: 235.455s
| Adam | epoch: 005 | loss: 0.16797 - acc: 0.9399 -- iter: 04800/28000
Training Step: 2337  | total loss: [1m[32m0.16313[0m[0m | time: 236.684s
| Adam | epoch: 005 | loss: 0.16313 - acc: 0.9419 -- iter: 04850/28000
Training Step: 2338  | total loss: [1m[32m0.16043[0m[0m | time: 237.919s
| Adam | epoch: 005 | loss: 0.16043 - acc: 0.9417 -- iter: 04900/28000
Training Step: 2339  | total loss: [1m[32m0.15188[0m[0m | time: 239.136s
| Adam | epoch: 005 | loss: 0.15188 - acc: 0.9455 -- iter: 04950/28000
Training Step: 2340  | total loss: [1m[32m0.15339[0m[0m | time: 240.381s
| Adam | epoch: 005 | loss: 0.15339 - acc: 0.9450 -- iter: 05000/28000
Training Step: 2341  | total loss: [1m[32m0.15009[0m[0m | time: 241.615s
| Adam | epoch: 005 | loss: 0.15009 - acc: 0.9445 -- iter: 05050/28000
Training Step: 2342  | total loss: [1m[32m0.14448[0m[0m | time: 242.854s
| Adam | epoch: 005 | loss: 0.14448 - acc: 0.9480 -- iter: 05100/28000
Training Step: 2343  | total loss: [1m[32m0.13622[0m[0m | time: 244.092s
| Adam | epoch: 005 | loss: 0.13622 - acc: 0.9532 -- iter: 05150/28000
Training Step: 2344  | total loss: [1m[32m0.13304[0m[0m | time: 245.338s
| Adam | epoch: 005 | loss: 0.13304 - acc: 0.9539 -- iter: 05200/28000
Training Step: 2345  | total loss: [1m[32m0.14679[0m[0m | time: 246.593s
| Adam | epoch: 005 | loss: 0.14679 - acc: 0.9505 -- iter: 05250/28000
Training Step: 2346  | total loss: [1m[32m0.14200[0m[0m | time: 247.842s
| Adam | epoch: 005 | loss: 0.14200 - acc: 0.9535 -- iter: 05300/28000
Training Step: 2347  | total loss: [1m[32m0.16604[0m[0m | time: 249.081s
| Adam | epoch: 005 | loss: 0.16604 - acc: 0.9501 -- iter: 05350/28000
Training Step: 2348  | total loss: [1m[32m0.15621[0m[0m | time: 250.303s
| Adam | epoch: 005 | loss: 0.15621 - acc: 0.9531 -- iter: 05400/28000
Training Step: 2349  | total loss: [1m[32m0.14713[0m[0m | time: 251.535s
| Adam | epoch: 005 | loss: 0.14713 - acc: 0.9578 -- iter: 05450/28000
Training Step: 2350  | total loss: [1m[32m0.14321[0m[0m | time: 281.700s
| Adam | epoch: 005 | loss: 0.14321 - acc: 0.9600 | val_loss: 0.19472 - val_acc: 0.9330 -- iter: 05500/28000
--
Training Step: 2351  | total loss: [1m[32m0.13732[0m[0m | time: 282.950s
| Adam | epoch: 005 | loss: 0.13732 - acc: 0.9620 -- iter: 05550/28000
Training Step: 2352  | total loss: [1m[32m0.15476[0m[0m | time: 284.186s
| Adam | epoch: 005 | loss: 0.15476 - acc: 0.9598 -- iter: 05600/28000
Training Step: 2353  | total loss: [1m[32m0.14461[0m[0m | time: 285.400s
| Adam | epoch: 005 | loss: 0.14461 - acc: 0.9638 -- iter: 05650/28000
Training Step: 2354  | total loss: [1m[32m0.13974[0m[0m | time: 286.639s
| Adam | epoch: 005 | loss: 0.13974 - acc: 0.9654 -- iter: 05700/28000
Training Step: 2355  | total loss: [1m[32m0.14227[0m[0m | time: 287.875s
| Adam | epoch: 005 | loss: 0.14227 - acc: 0.9669 -- iter: 05750/28000
Training Step: 2356  | total loss: [1m[32m0.14126[0m[0m | time: 289.097s
| Adam | epoch: 005 | loss: 0.14126 - acc: 0.9662 -- iter: 05800/28000
Training Step: 2357  | total loss: [1m[32m0.15876[0m[0m | time: 290.353s
| Adam | epoch: 005 | loss: 0.15876 - acc: 0.9616 -- iter: 05850/28000
Training Step: 2358  | total loss: [1m[32m0.15528[0m[0m | time: 291.584s
| Adam | epoch: 005 | loss: 0.15528 - acc: 0.9614 -- iter: 05900/28000
Training Step: 2359  | total loss: [1m[32m0.16007[0m[0m | time: 292.809s
| Adam | epoch: 005 | loss: 0.16007 - acc: 0.9553 -- iter: 05950/28000
Training Step: 2360  | total loss: [1m[32m0.17441[0m[0m | time: 294.087s
| Adam | epoch: 005 | loss: 0.17441 - acc: 0.9518 -- iter: 06000/28000
Training Step: 2361  | total loss: [1m[32m0.18895[0m[0m | time: 295.332s
| Adam | epoch: 005 | loss: 0.18895 - acc: 0.9426 -- iter: 06050/28000
Training Step: 2362  | total loss: [1m[32m0.19285[0m[0m | time: 296.586s
| Adam | epoch: 005 | loss: 0.19285 - acc: 0.9383 -- iter: 06100/28000
Training Step: 2363  | total loss: [1m[32m0.19475[0m[0m | time: 297.801s
| Adam | epoch: 005 | loss: 0.19475 - acc: 0.9385 -- iter: 06150/28000
Training Step: 2364  | total loss: [1m[32m0.18136[0m[0m | time: 299.059s
| Adam | epoch: 005 | loss: 0.18136 - acc: 0.9446 -- iter: 06200/28000
Training Step: 2365  | total loss: [1m[32m0.16974[0m[0m | time: 300.269s
| Adam | epoch: 005 | loss: 0.16974 - acc: 0.9502 -- iter: 06250/28000
Training Step: 2366  | total loss: [1m[32m0.16147[0m[0m | time: 301.484s
| Adam | epoch: 005 | loss: 0.16147 - acc: 0.9532 -- iter: 06300/28000
Training Step: 2367  | total loss: [1m[32m0.15572[0m[0m | time: 302.712s
| Adam | epoch: 005 | loss: 0.15572 - acc: 0.9518 -- iter: 06350/28000
Training Step: 2368  | total loss: [1m[32m0.15083[0m[0m | time: 303.916s
| Adam | epoch: 005 | loss: 0.15083 - acc: 0.9547 -- iter: 06400/28000
Training Step: 2369  | total loss: [1m[32m0.15323[0m[0m | time: 305.135s
| Adam | epoch: 005 | loss: 0.15323 - acc: 0.9552 -- iter: 06450/28000
Training Step: 2370  | total loss: [1m[32m0.15629[0m[0m | time: 306.370s
| Adam | epoch: 005 | loss: 0.15629 - acc: 0.9537 -- iter: 06500/28000
Training Step: 2371  | total loss: [1m[32m0.15718[0m[0m | time: 307.601s
| Adam | epoch: 005 | loss: 0.15718 - acc: 0.9503 -- iter: 06550/28000
Training Step: 2372  | total loss: [1m[32m0.16314[0m[0m | time: 308.822s
| Adam | epoch: 005 | loss: 0.16314 - acc: 0.9453 -- iter: 06600/28000
Training Step: 2373  | total loss: [1m[32m0.16004[0m[0m | time: 310.101s
| Adam | epoch: 005 | loss: 0.16004 - acc: 0.9447 -- iter: 06650/28000
Training Step: 2374  | total loss: [1m[32m0.16680[0m[0m | time: 311.373s
| Adam | epoch: 005 | loss: 0.16680 - acc: 0.9403 -- iter: 06700/28000
Training Step: 2375  | total loss: [1m[32m0.15980[0m[0m | time: 341.197s
| Adam | epoch: 005 | loss: 0.15980 - acc: 0.9442 | val_loss: 0.24938 - val_acc: 0.9072 -- iter: 06750/28000
--
Training Step: 2376  | total loss: [1m[32m0.14777[0m[0m | time: 342.473s
| Adam | epoch: 005 | loss: 0.14777 - acc: 0.9498 -- iter: 06800/28000
Training Step: 2377  | total loss: [1m[32m0.15289[0m[0m | time: 343.706s
| Adam | epoch: 005 | loss: 0.15289 - acc: 0.9508 -- iter: 06850/28000
Training Step: 2378  | total loss: [1m[32m0.15475[0m[0m | time: 344.962s
| Adam | epoch: 005 | loss: 0.15475 - acc: 0.9458 -- iter: 06900/28000
Training Step: 2379  | total loss: [1m[32m0.17527[0m[0m | time: 346.211s
| Adam | epoch: 005 | loss: 0.17527 - acc: 0.9392 -- iter: 06950/28000
Training Step: 2380  | total loss: [1m[32m0.18101[0m[0m | time: 347.463s
| Adam | epoch: 005 | loss: 0.18101 - acc: 0.9393 -- iter: 07000/28000
Training Step: 2381  | total loss: [1m[32m0.17336[0m[0m | time: 348.723s
| Adam | epoch: 005 | loss: 0.17336 - acc: 0.9413 -- iter: 07050/28000
Training Step: 2382  | total loss: [1m[32m0.20076[0m[0m | time: 349.949s
| Adam | epoch: 005 | loss: 0.20076 - acc: 0.9312 -- iter: 07100/28000
Training Step: 2383  | total loss: [1m[32m0.19727[0m[0m | time: 351.203s
| Adam | epoch: 005 | loss: 0.19727 - acc: 0.9341 -- iter: 07150/28000
Training Step: 2384  | total loss: [1m[32m0.19110[0m[0m | time: 352.421s
| Adam | epoch: 005 | loss: 0.19110 - acc: 0.9327 -- iter: 07200/28000
Training Step: 2385  | total loss: [1m[32m0.18979[0m[0m | time: 353.681s
| Adam | epoch: 005 | loss: 0.18979 - acc: 0.9274 -- iter: 07250/28000
Training Step: 2386  | total loss: [1m[32m0.19231[0m[0m | time: 354.950s
| Adam | epoch: 005 | loss: 0.19231 - acc: 0.9227 -- iter: 07300/28000
Training Step: 2387  | total loss: [1m[32m0.18880[0m[0m | time: 356.181s
| Adam | epoch: 005 | loss: 0.18880 - acc: 0.9224 -- iter: 07350/28000
Training Step: 2388  | total loss: [1m[32m0.17806[0m[0m | time: 357.418s
| Adam | epoch: 005 | loss: 0.17806 - acc: 0.9282 -- iter: 07400/28000
Training Step: 2389  | total loss: [1m[32m0.16718[0m[0m | time: 358.955s
| Adam | epoch: 005 | loss: 0.16718 - acc: 0.9313 -- iter: 07450/28000
Training Step: 2390  | total loss: [1m[32m0.16787[0m[0m | time: 360.184s
| Adam | epoch: 005 | loss: 0.16787 - acc: 0.9322 -- iter: 07500/28000
Training Step: 2391  | total loss: [1m[32m0.16858[0m[0m | time: 361.446s
| Adam | epoch: 005 | loss: 0.16858 - acc: 0.9350 -- iter: 07550/28000
Training Step: 2392  | total loss: [1m[32m0.16715[0m[0m | time: 362.693s
| Adam | epoch: 005 | loss: 0.16715 - acc: 0.9375 -- iter: 07600/28000
Training Step: 2393  | total loss: [1m[32m0.17237[0m[0m | time: 363.959s
| Adam | epoch: 005 | loss: 0.17237 - acc: 0.9317 -- iter: 07650/28000
Training Step: 2394  | total loss: [1m[32m0.17423[0m[0m | time: 365.236s
| Adam | epoch: 005 | loss: 0.17423 - acc: 0.9286 -- iter: 07700/28000
Training Step: 2395  | total loss: [1m[32m0.16556[0m[0m | time: 366.546s
| Adam | epoch: 005 | loss: 0.16556 - acc: 0.9317 -- iter: 07750/28000
Training Step: 2396  | total loss: [1m[32m0.16784[0m[0m | time: 367.866s
| Adam | epoch: 005 | loss: 0.16784 - acc: 0.9285 -- iter: 07800/28000
Training Step: 2397  | total loss: [1m[32m0.18320[0m[0m | time: 369.173s
| Adam | epoch: 005 | loss: 0.18320 - acc: 0.9317 -- iter: 07850/28000
Training Step: 2398  | total loss: [1m[32m0.18757[0m[0m | time: 370.453s
| Adam | epoch: 005 | loss: 0.18757 - acc: 0.9345 -- iter: 07900/28000
Training Step: 2399  | total loss: [1m[32m0.19050[0m[0m | time: 371.695s
| Adam | epoch: 005 | loss: 0.19050 - acc: 0.9331 -- iter: 07950/28000
Training Step: 2400  | total loss: [1m[32m0.19369[0m[0m | time: 401.610s
| Adam | epoch: 005 | loss: 0.19369 - acc: 0.9318 | val_loss: 0.17768 - val_acc: 0.9366 -- iter: 08000/28000
--
Training Step: 2401  | total loss: [1m[32m0.19587[0m[0m | time: 402.878s
| Adam | epoch: 005 | loss: 0.19587 - acc: 0.9266 -- iter: 08050/28000
Training Step: 2402  | total loss: [1m[32m0.18450[0m[0m | time: 404.140s
| Adam | epoch: 005 | loss: 0.18450 - acc: 0.9319 -- iter: 08100/28000
Training Step: 2403  | total loss: [1m[32m0.17458[0m[0m | time: 405.396s
| Adam | epoch: 005 | loss: 0.17458 - acc: 0.9367 -- iter: 08150/28000
Training Step: 2404  | total loss: [1m[32m0.16687[0m[0m | time: 406.634s
| Adam | epoch: 005 | loss: 0.16687 - acc: 0.9391 -- iter: 08200/28000
Training Step: 2405  | total loss: [1m[32m0.16417[0m[0m | time: 407.900s
| Adam | epoch: 005 | loss: 0.16417 - acc: 0.9412 -- iter: 08250/28000
Training Step: 2406  | total loss: [1m[32m0.16656[0m[0m | time: 409.151s
| Adam | epoch: 005 | loss: 0.16656 - acc: 0.9390 -- iter: 08300/28000
Training Step: 2407  | total loss: [1m[32m0.17409[0m[0m | time: 410.411s
| Adam | epoch: 005 | loss: 0.17409 - acc: 0.9311 -- iter: 08350/28000
Training Step: 2408  | total loss: [1m[32m0.18172[0m[0m | time: 411.663s
| Adam | epoch: 005 | loss: 0.18172 - acc: 0.9280 -- iter: 08400/28000
Training Step: 2409  | total loss: [1m[32m0.17588[0m[0m | time: 412.895s
| Adam | epoch: 005 | loss: 0.17588 - acc: 0.9312 -- iter: 08450/28000
Training Step: 2410  | total loss: [1m[32m0.16725[0m[0m | time: 414.185s
| Adam | epoch: 005 | loss: 0.16725 - acc: 0.9361 -- iter: 08500/28000
Training Step: 2411  | total loss: [1m[32m0.17704[0m[0m | time: 415.494s
| Adam | epoch: 005 | loss: 0.17704 - acc: 0.9345 -- iter: 08550/28000
Training Step: 2412  | total loss: [1m[32m0.17166[0m[0m | time: 416.741s
| Adam | epoch: 005 | loss: 0.17166 - acc: 0.9370 -- iter: 08600/28000
Training Step: 2413  | total loss: [1m[32m0.16755[0m[0m | time: 417.987s
| Adam | epoch: 005 | loss: 0.16755 - acc: 0.9353 -- iter: 08650/28000
Training Step: 2414  | total loss: [1m[32m0.16708[0m[0m | time: 419.313s
| Adam | epoch: 005 | loss: 0.16708 - acc: 0.9338 -- iter: 08700/28000
Training Step: 2415  | total loss: [1m[32m0.16875[0m[0m | time: 420.552s
| Adam | epoch: 005 | loss: 0.16875 - acc: 0.9344 -- iter: 08750/28000
Training Step: 2416  | total loss: [1m[32m0.16283[0m[0m | time: 421.769s
| Adam | epoch: 005 | loss: 0.16283 - acc: 0.9370 -- iter: 08800/28000
Training Step: 2417  | total loss: [1m[32m0.15304[0m[0m | time: 423.039s
| Adam | epoch: 005 | loss: 0.15304 - acc: 0.9413 -- iter: 08850/28000
Training Step: 2418  | total loss: [1m[32m0.15391[0m[0m | time: 424.291s
| Adam | epoch: 005 | loss: 0.15391 - acc: 0.9392 -- iter: 08900/28000
Training Step: 2419  | total loss: [1m[32m0.14981[0m[0m | time: 425.506s
| Adam | epoch: 005 | loss: 0.14981 - acc: 0.9412 -- iter: 08950/28000
Training Step: 2420  | total loss: [1m[32m0.14381[0m[0m | time: 426.753s
| Adam | epoch: 005 | loss: 0.14381 - acc: 0.9431 -- iter: 09000/28000
Training Step: 2421  | total loss: [1m[32m0.13513[0m[0m | time: 427.975s
| Adam | epoch: 005 | loss: 0.13513 - acc: 0.9488 -- iter: 09050/28000
Training Step: 2422  | total loss: [1m[32m0.14283[0m[0m | time: 429.192s
| Adam | epoch: 005 | loss: 0.14283 - acc: 0.9459 -- iter: 09100/28000
Training Step: 2423  | total loss: [1m[32m0.14368[0m[0m | time: 430.433s
| Adam | epoch: 005 | loss: 0.14368 - acc: 0.9453 -- iter: 09150/28000
Training Step: 2424  | total loss: [1m[32m0.13701[0m[0m | time: 431.654s
| Adam | epoch: 005 | loss: 0.13701 - acc: 0.9488 -- iter: 09200/28000
Training Step: 2425  | total loss: [1m[32m0.14401[0m[0m | time: 461.785s
| Adam | epoch: 005 | loss: 0.14401 - acc: 0.9479 | val_loss: 0.17621 - val_acc: 0.9387 -- iter: 09250/28000
--
Training Step: 2426  | total loss: [1m[32m0.14114[0m[0m | time: 463.053s
| Adam | epoch: 005 | loss: 0.14114 - acc: 0.9491 -- iter: 09300/28000
Training Step: 2427  | total loss: [1m[32m0.13214[0m[0m | time: 464.330s
| Adam | epoch: 005 | loss: 0.13214 - acc: 0.9542 -- iter: 09350/28000
Training Step: 2428  | total loss: [1m[32m0.13128[0m[0m | time: 465.564s
| Adam | epoch: 005 | loss: 0.13128 - acc: 0.9548 -- iter: 09400/28000
Training Step: 2429  | total loss: [1m[32m0.13820[0m[0m | time: 466.811s
| Adam | epoch: 005 | loss: 0.13820 - acc: 0.9533 -- iter: 09450/28000
Training Step: 2430  | total loss: [1m[32m0.16135[0m[0m | time: 468.048s
| Adam | epoch: 005 | loss: 0.16135 - acc: 0.9500 -- iter: 09500/28000
Training Step: 2431  | total loss: [1m[32m0.15241[0m[0m | time: 469.241s
| Adam | epoch: 005 | loss: 0.15241 - acc: 0.9530 -- iter: 09550/28000
Training Step: 2432  | total loss: [1m[32m0.14443[0m[0m | time: 470.476s
| Adam | epoch: 005 | loss: 0.14443 - acc: 0.9557 -- iter: 09600/28000
Training Step: 2433  | total loss: [1m[32m0.15985[0m[0m | time: 471.694s
| Adam | epoch: 005 | loss: 0.15985 - acc: 0.9521 -- iter: 09650/28000
Training Step: 2434  | total loss: [1m[32m0.14908[0m[0m | time: 472.929s
| Adam | epoch: 005 | loss: 0.14908 - acc: 0.9549 -- iter: 09700/28000
Training Step: 2435  | total loss: [1m[32m0.15093[0m[0m | time: 474.221s
| Adam | epoch: 005 | loss: 0.15093 - acc: 0.9494 -- iter: 09750/28000
Training Step: 2436  | total loss: [1m[32m0.14059[0m[0m | time: 475.468s
| Adam | epoch: 005 | loss: 0.14059 - acc: 0.9545 -- iter: 09800/28000
Training Step: 2437  | total loss: [1m[32m0.14259[0m[0m | time: 476.706s
| Adam | epoch: 005 | loss: 0.14259 - acc: 0.9550 -- iter: 09850/28000
Training Step: 2438  | total loss: [1m[32m0.14392[0m[0m | time: 477.946s
| Adam | epoch: 005 | loss: 0.14392 - acc: 0.9515 -- iter: 09900/28000
Training Step: 2439  | total loss: [1m[32m0.13823[0m[0m | time: 479.163s
| Adam | epoch: 005 | loss: 0.13823 - acc: 0.9544 -- iter: 09950/28000
Training Step: 2440  | total loss: [1m[32m0.13586[0m[0m | time: 480.392s
| Adam | epoch: 005 | loss: 0.13586 - acc: 0.9529 -- iter: 10000/28000
Training Step: 2441  | total loss: [1m[32m0.12937[0m[0m | time: 481.598s
| Adam | epoch: 005 | loss: 0.12937 - acc: 0.9536 -- iter: 10050/28000
Training Step: 2442  | total loss: [1m[32m0.13423[0m[0m | time: 482.824s
| Adam | epoch: 005 | loss: 0.13423 - acc: 0.9523 -- iter: 10100/28000
Training Step: 2443  | total loss: [1m[32m0.12557[0m[0m | time: 484.055s
| Adam | epoch: 005 | loss: 0.12557 - acc: 0.9570 -- iter: 10150/28000
Training Step: 2444  | total loss: [1m[32m0.13008[0m[0m | time: 485.273s
| Adam | epoch: 005 | loss: 0.13008 - acc: 0.9513 -- iter: 10200/28000
Training Step: 2445  | total loss: [1m[32m0.12704[0m[0m | time: 486.528s
| Adam | epoch: 005 | loss: 0.12704 - acc: 0.9502 -- iter: 10250/28000
Training Step: 2446  | total loss: [1m[32m0.13937[0m[0m | time: 487.776s
| Adam | epoch: 005 | loss: 0.13937 - acc: 0.9492 -- iter: 10300/28000
Training Step: 2447  | total loss: [1m[32m0.14547[0m[0m | time: 489.023s
| Adam | epoch: 005 | loss: 0.14547 - acc: 0.9463 -- iter: 10350/28000
Training Step: 2448  | total loss: [1m[32m0.14290[0m[0m | time: 490.253s
| Adam | epoch: 005 | loss: 0.14290 - acc: 0.9456 -- iter: 10400/28000
Training Step: 2449  | total loss: [1m[32m0.14468[0m[0m | time: 491.493s
| Adam | epoch: 005 | loss: 0.14468 - acc: 0.9431 -- iter: 10450/28000
Training Step: 2450  | total loss: [1m[32m0.15888[0m[0m | time: 521.129s
| Adam | epoch: 005 | loss: 0.15888 - acc: 0.9368 | val_loss: 0.18205 - val_acc: 0.9336 -- iter: 10500/28000
--
Training Step: 2451  | total loss: [1m[32m0.15752[0m[0m | time: 522.395s
| Adam | epoch: 005 | loss: 0.15752 - acc: 0.9351 -- iter: 10550/28000
Training Step: 2452  | total loss: [1m[32m0.15918[0m[0m | time: 523.619s
| Adam | epoch: 005 | loss: 0.15918 - acc: 0.9336 -- iter: 10600/28000
Training Step: 2453  | total loss: [1m[32m0.15342[0m[0m | time: 524.839s
| Adam | epoch: 005 | loss: 0.15342 - acc: 0.9362 -- iter: 10650/28000
Training Step: 2454  | total loss: [1m[32m0.14592[0m[0m | time: 526.098s
| Adam | epoch: 005 | loss: 0.14592 - acc: 0.9386 -- iter: 10700/28000
Training Step: 2455  | total loss: [1m[32m0.13868[0m[0m | time: 527.337s
| Adam | epoch: 005 | loss: 0.13868 - acc: 0.9407 -- iter: 10750/28000
Training Step: 2456  | total loss: [1m[32m0.13527[0m[0m | time: 528.552s
| Adam | epoch: 005 | loss: 0.13527 - acc: 0.9427 -- iter: 10800/28000
Training Step: 2457  | total loss: [1m[32m0.13250[0m[0m | time: 529.778s
| Adam | epoch: 005 | loss: 0.13250 - acc: 0.9464 -- iter: 10850/28000
Training Step: 2458  | total loss: [1m[32m0.12806[0m[0m | time: 531.014s
| Adam | epoch: 005 | loss: 0.12806 - acc: 0.9478 -- iter: 10900/28000
Training Step: 2459  | total loss: [1m[32m0.12760[0m[0m | time: 532.247s
| Adam | epoch: 005 | loss: 0.12760 - acc: 0.9470 -- iter: 10950/28000
Training Step: 2460  | total loss: [1m[32m0.12774[0m[0m | time: 533.489s
| Adam | epoch: 005 | loss: 0.12774 - acc: 0.9483 -- iter: 11000/28000
Training Step: 2461  | total loss: [1m[32m0.12170[0m[0m | time: 534.772s
| Adam | epoch: 005 | loss: 0.12170 - acc: 0.9535 -- iter: 11050/28000
Training Step: 2462  | total loss: [1m[32m0.11580[0m[0m | time: 536.007s
| Adam | epoch: 005 | loss: 0.11580 - acc: 0.9581 -- iter: 11100/28000
Training Step: 2463  | total loss: [1m[32m0.13225[0m[0m | time: 537.222s
| Adam | epoch: 005 | loss: 0.13225 - acc: 0.9563 -- iter: 11150/28000
Training Step: 2464  | total loss: [1m[32m0.13304[0m[0m | time: 538.456s
| Adam | epoch: 005 | loss: 0.13304 - acc: 0.9527 -- iter: 11200/28000
Training Step: 2465  | total loss: [1m[32m0.13013[0m[0m | time: 539.663s
| Adam | epoch: 005 | loss: 0.13013 - acc: 0.9534 -- iter: 11250/28000
Training Step: 2466  | total loss: [1m[32m0.12642[0m[0m | time: 540.898s
| Adam | epoch: 005 | loss: 0.12642 - acc: 0.9521 -- iter: 11300/28000
Training Step: 2467  | total loss: [1m[32m0.12944[0m[0m | time: 542.118s
| Adam | epoch: 005 | loss: 0.12944 - acc: 0.9529 -- iter: 11350/28000
Training Step: 2468  | total loss: [1m[32m0.13032[0m[0m | time: 543.354s
| Adam | epoch: 005 | loss: 0.13032 - acc: 0.9536 -- iter: 11400/28000
Training Step: 2469  | total loss: [1m[32m0.13770[0m[0m | time: 544.598s
| Adam | epoch: 005 | loss: 0.13770 - acc: 0.9542 -- iter: 11450/28000
Training Step: 2470  | total loss: [1m[32m0.13571[0m[0m | time: 545.798s
| Adam | epoch: 005 | loss: 0.13571 - acc: 0.9528 -- iter: 11500/28000
Training Step: 2471  | total loss: [1m[32m0.13442[0m[0m | time: 547.025s
| Adam | epoch: 005 | loss: 0.13442 - acc: 0.9495 -- iter: 11550/28000
Training Step: 2472  | total loss: [1m[32m0.12763[0m[0m | time: 548.255s
| Adam | epoch: 005 | loss: 0.12763 - acc: 0.9526 -- iter: 11600/28000
Training Step: 2473  | total loss: [1m[32m0.13275[0m[0m | time: 549.471s
| Adam | epoch: 005 | loss: 0.13275 - acc: 0.9553 -- iter: 11650/28000
Training Step: 2474  | total loss: [1m[32m0.12605[0m[0m | time: 550.717s
| Adam | epoch: 005 | loss: 0.12605 - acc: 0.9558 -- iter: 11700/28000
Training Step: 2475  | total loss: [1m[32m0.13072[0m[0m | time: 580.744s
| Adam | epoch: 005 | loss: 0.13072 - acc: 0.9522 | val_loss: 0.17839 - val_acc: 0.9376 -- iter: 11750/28000
--
Training Step: 2476  | total loss: [1m[32m0.12572[0m[0m | time: 582.005s
| Adam | epoch: 005 | loss: 0.12572 - acc: 0.9550 -- iter: 11800/28000
Training Step: 2477  | total loss: [1m[32m0.13501[0m[0m | time: 583.248s
| Adam | epoch: 005 | loss: 0.13501 - acc: 0.9555 -- iter: 11850/28000
Training Step: 2478  | total loss: [1m[32m0.13177[0m[0m | time: 584.556s
| Adam | epoch: 005 | loss: 0.13177 - acc: 0.9579 -- iter: 11900/28000
Training Step: 2479  | total loss: [1m[32m0.12482[0m[0m | time: 585.793s
| Adam | epoch: 005 | loss: 0.12482 - acc: 0.9601 -- iter: 11950/28000
Training Step: 2480  | total loss: [1m[32m0.12986[0m[0m | time: 587.012s
| Adam | epoch: 005 | loss: 0.12986 - acc: 0.9601 -- iter: 12000/28000
Training Step: 2481  | total loss: [1m[32m0.13875[0m[0m | time: 588.240s
| Adam | epoch: 005 | loss: 0.13875 - acc: 0.9581 -- iter: 12050/28000
Training Step: 2482  | total loss: [1m[32m0.13529[0m[0m | time: 589.487s
| Adam | epoch: 005 | loss: 0.13529 - acc: 0.9603 -- iter: 12100/28000
Training Step: 2483  | total loss: [1m[32m0.13498[0m[0m | time: 590.728s
| Adam | epoch: 005 | loss: 0.13498 - acc: 0.9583 -- iter: 12150/28000
Training Step: 2484  | total loss: [1m[32m0.12948[0m[0m | time: 591.958s
| Adam | epoch: 005 | loss: 0.12948 - acc: 0.9624 -- iter: 12200/28000
Training Step: 2485  | total loss: [1m[32m0.14179[0m[0m | time: 593.179s
| Adam | epoch: 005 | loss: 0.14179 - acc: 0.9542 -- iter: 12250/28000
Training Step: 2486  | total loss: [1m[32m0.14474[0m[0m | time: 594.481s
| Adam | epoch: 005 | loss: 0.14474 - acc: 0.9468 -- iter: 12300/28000
Training Step: 2487  | total loss: [1m[32m0.13891[0m[0m | time: 595.707s
| Adam | epoch: 005 | loss: 0.13891 - acc: 0.9501 -- iter: 12350/28000
Training Step: 2488  | total loss: [1m[32m0.13768[0m[0m | time: 596.959s
| Adam | epoch: 005 | loss: 0.13768 - acc: 0.9511 -- iter: 12400/28000
Training Step: 2489  | total loss: [1m[32m0.12875[0m[0m | time: 598.185s
| Adam | epoch: 005 | loss: 0.12875 - acc: 0.9560 -- iter: 12450/28000
Training Step: 2490  | total loss: [1m[32m0.12176[0m[0m | time: 599.388s
| Adam | epoch: 005 | loss: 0.12176 - acc: 0.9604 -- iter: 12500/28000
Training Step: 2491  | total loss: [1m[32m0.13028[0m[0m | time: 600.598s
| Adam | epoch: 005 | loss: 0.13028 - acc: 0.9603 -- iter: 12550/28000
Training Step: 2492  | total loss: [1m[32m0.12737[0m[0m | time: 601.810s
| Adam | epoch: 005 | loss: 0.12737 - acc: 0.9603 -- iter: 12600/28000
Training Step: 2493  | total loss: [1m[32m0.12723[0m[0m | time: 603.032s
| Adam | epoch: 005 | loss: 0.12723 - acc: 0.9583 -- iter: 12650/28000
Training Step: 2494  | total loss: [1m[32m0.13971[0m[0m | time: 604.232s
| Adam | epoch: 005 | loss: 0.13971 - acc: 0.9505 -- iter: 12700/28000
Training Step: 2495  | total loss: [1m[32m0.14064[0m[0m | time: 605.442s
| Adam | epoch: 005 | loss: 0.14064 - acc: 0.9494 -- iter: 12750/28000
Training Step: 2496  | total loss: [1m[32m0.13401[0m[0m | time: 606.680s
| Adam | epoch: 005 | loss: 0.13401 - acc: 0.9505 -- iter: 12800/28000
Training Step: 2497  | total loss: [1m[32m0.12830[0m[0m | time: 607.905s
| Adam | epoch: 005 | loss: 0.12830 - acc: 0.9534 -- iter: 12850/28000
Training Step: 2498  | total loss: [1m[32m0.13090[0m[0m | time: 609.183s
| Adam | epoch: 005 | loss: 0.13090 - acc: 0.9561 -- iter: 12900/28000
Training Step: 2499  | total loss: [1m[32m0.12889[0m[0m | time: 610.391s
| Adam | epoch: 005 | loss: 0.12889 - acc: 0.9585 -- iter: 12950/28000
Training Step: 2500  | total loss: [1m[32m0.13608[0m[0m | time: 640.008s
| Adam | epoch: 005 | loss: 0.13608 - acc: 0.9586 | val_loss: 0.17755 - val_acc: 0.9383 -- iter: 13000/28000
--
Training Step: 2501  | total loss: [1m[32m0.12711[0m[0m | time: 641.280s
| Adam | epoch: 005 | loss: 0.12711 - acc: 0.9628 -- iter: 13050/28000
Training Step: 2502  | total loss: [1m[32m0.13264[0m[0m | time: 642.515s
| Adam | epoch: 005 | loss: 0.13264 - acc: 0.9645 -- iter: 13100/28000
Training Step: 2503  | total loss: [1m[32m0.13434[0m[0m | time: 643.755s
| Adam | epoch: 005 | loss: 0.13434 - acc: 0.9600 -- iter: 13150/28000
Training Step: 2504  | total loss: [1m[32m0.13220[0m[0m | time: 645.035s
| Adam | epoch: 005 | loss: 0.13220 - acc: 0.9620 -- iter: 13200/28000
Training Step: 2505  | total loss: [1m[32m0.12931[0m[0m | time: 646.266s
| Adam | epoch: 005 | loss: 0.12931 - acc: 0.9598 -- iter: 13250/28000
Training Step: 2506  | total loss: [1m[32m0.13353[0m[0m | time: 647.494s
| Adam | epoch: 005 | loss: 0.13353 - acc: 0.9598 -- iter: 13300/28000
Training Step: 2507  | total loss: [1m[32m0.14061[0m[0m | time: 648.746s
| Adam | epoch: 005 | loss: 0.14061 - acc: 0.9579 -- iter: 13350/28000
Training Step: 2508  | total loss: [1m[32m0.13624[0m[0m | time: 650.001s
| Adam | epoch: 005 | loss: 0.13624 - acc: 0.9581 -- iter: 13400/28000
Training Step: 2509  | total loss: [1m[32m0.14738[0m[0m | time: 651.231s
| Adam | epoch: 005 | loss: 0.14738 - acc: 0.9503 -- iter: 13450/28000
Training Step: 2510  | total loss: [1m[32m0.14362[0m[0m | time: 652.478s
| Adam | epoch: 005 | loss: 0.14362 - acc: 0.9512 -- iter: 13500/28000
Training Step: 2511  | total loss: [1m[32m0.15264[0m[0m | time: 653.730s
| Adam | epoch: 005 | loss: 0.15264 - acc: 0.9501 -- iter: 13550/28000
Training Step: 2512  | total loss: [1m[32m0.16158[0m[0m | time: 655.023s
| Adam | epoch: 005 | loss: 0.16158 - acc: 0.9471 -- iter: 13600/28000
Training Step: 2513  | total loss: [1m[32m0.16842[0m[0m | time: 656.278s
| Adam | epoch: 005 | loss: 0.16842 - acc: 0.9424 -- iter: 13650/28000
Training Step: 2514  | total loss: [1m[32m0.16310[0m[0m | time: 657.501s
| Adam | epoch: 005 | loss: 0.16310 - acc: 0.9462 -- iter: 13700/28000
Training Step: 2515  | total loss: [1m[32m0.15992[0m[0m | time: 658.739s
| Adam | epoch: 005 | loss: 0.15992 - acc: 0.9475 -- iter: 13750/28000
Training Step: 2516  | total loss: [1m[32m0.15954[0m[0m | time: 659.986s
| Adam | epoch: 005 | loss: 0.15954 - acc: 0.9468 -- iter: 13800/28000
Training Step: 2517  | total loss: [1m[32m0.15684[0m[0m | time: 661.219s
| Adam | epoch: 005 | loss: 0.15684 - acc: 0.9501 -- iter: 13850/28000
Training Step: 2518  | total loss: [1m[32m0.14711[0m[0m | time: 662.441s
| Adam | epoch: 005 | loss: 0.14711 - acc: 0.9531 -- iter: 13900/28000
Training Step: 2519  | total loss: [1m[32m0.15450[0m[0m | time: 663.671s
| Adam | epoch: 005 | loss: 0.15450 - acc: 0.9498 -- iter: 13950/28000
Training Step: 2520  | total loss: [1m[32m0.15976[0m[0m | time: 664.926s
| Adam | epoch: 005 | loss: 0.15976 - acc: 0.9428 -- iter: 14000/28000
Training Step: 2521  | total loss: [1m[32m0.15231[0m[0m | time: 666.135s
| Adam | epoch: 005 | loss: 0.15231 - acc: 0.9425 -- iter: 14050/28000
Training Step: 2522  | total loss: [1m[32m0.14303[0m[0m | time: 667.375s
| Adam | epoch: 005 | loss: 0.14303 - acc: 0.9463 -- iter: 14100/28000
Training Step: 2523  | total loss: [1m[32m0.13811[0m[0m | time: 668.620s
| Adam | epoch: 005 | loss: 0.13811 - acc: 0.9496 -- iter: 14150/28000
Training Step: 2524  | total loss: [1m[32m0.12889[0m[0m | time: 669.838s
| Adam | epoch: 005 | loss: 0.12889 - acc: 0.9527 -- iter: 14200/28000
Training Step: 2525  | total loss: [1m[32m0.13398[0m[0m | time: 699.790s
| Adam | epoch: 005 | loss: 0.13398 - acc: 0.9494 | val_loss: 0.18381 - val_acc: 0.9341 -- iter: 14250/28000
--
Training Step: 2526  | total loss: [1m[32m0.15664[0m[0m | time: 701.074s
| Adam | epoch: 005 | loss: 0.15664 - acc: 0.9445 -- iter: 14300/28000
Training Step: 2527  | total loss: [1m[32m0.14595[0m[0m | time: 702.294s
| Adam | epoch: 005 | loss: 0.14595 - acc: 0.9500 -- iter: 14350/28000
Training Step: 2528  | total loss: [1m[32m0.14628[0m[0m | time: 703.539s
| Adam | epoch: 005 | loss: 0.14628 - acc: 0.9490 -- iter: 14400/28000
Training Step: 2529  | total loss: [1m[32m0.14915[0m[0m | time: 704.856s
| Adam | epoch: 005 | loss: 0.14915 - acc: 0.9481 -- iter: 14450/28000
Training Step: 2530  | total loss: [1m[32m0.14317[0m[0m | time: 706.114s
| Adam | epoch: 005 | loss: 0.14317 - acc: 0.9493 -- iter: 14500/28000
Training Step: 2531  | total loss: [1m[32m0.14109[0m[0m | time: 707.363s
| Adam | epoch: 005 | loss: 0.14109 - acc: 0.9484 -- iter: 14550/28000
Training Step: 2532  | total loss: [1m[32m0.14013[0m[0m | time: 708.599s
| Adam | epoch: 005 | loss: 0.14013 - acc: 0.9495 -- iter: 14600/28000
Training Step: 2533  | total loss: [1m[32m0.13436[0m[0m | time: 709.810s
| Adam | epoch: 005 | loss: 0.13436 - acc: 0.9506 -- iter: 14650/28000
Training Step: 2534  | total loss: [1m[32m0.12352[0m[0m | time: 711.055s
| Adam | epoch: 005 | loss: 0.12352 - acc: 0.9555 -- iter: 14700/28000
Training Step: 2535  | total loss: [1m[32m0.13982[0m[0m | time: 712.289s
| Adam | epoch: 005 | loss: 0.13982 - acc: 0.9520 -- iter: 14750/28000
Training Step: 2536  | total loss: [1m[32m0.14527[0m[0m | time: 713.518s
| Adam | epoch: 005 | loss: 0.14527 - acc: 0.9528 -- iter: 14800/28000
Training Step: 2537  | total loss: [1m[32m0.13709[0m[0m | time: 714.794s
| Adam | epoch: 005 | loss: 0.13709 - acc: 0.9535 -- iter: 14850/28000
Training Step: 2538  | total loss: [1m[32m0.13483[0m[0m | time: 716.007s
| Adam | epoch: 005 | loss: 0.13483 - acc: 0.9521 -- iter: 14900/28000
Training Step: 2539  | total loss: [1m[32m0.13492[0m[0m | time: 717.215s
| Adam | epoch: 005 | loss: 0.13492 - acc: 0.9449 -- iter: 14950/28000
Training Step: 2540  | total loss: [1m[32m0.15081[0m[0m | time: 718.409s
| Adam | epoch: 005 | loss: 0.15081 - acc: 0.9444 -- iter: 15000/28000
Training Step: 2541  | total loss: [1m[32m0.14048[0m[0m | time: 719.671s
| Adam | epoch: 005 | loss: 0.14048 - acc: 0.9500 -- iter: 15050/28000
Training Step: 2542  | total loss: [1m[32m0.13427[0m[0m | time: 720.933s
| Adam | epoch: 005 | loss: 0.13427 - acc: 0.9530 -- iter: 15100/28000
Training Step: 2543  | total loss: [1m[32m0.13127[0m[0m | time: 722.186s
| Adam | epoch: 005 | loss: 0.13127 - acc: 0.9537 -- iter: 15150/28000
Training Step: 2544  | total loss: [1m[32m0.15248[0m[0m | time: 723.421s
| Adam | epoch: 005 | loss: 0.15248 - acc: 0.9503 -- iter: 15200/28000
Training Step: 2545  | total loss: [1m[32m0.15503[0m[0m | time: 724.665s
| Adam | epoch: 005 | loss: 0.15503 - acc: 0.9513 -- iter: 15250/28000
Training Step: 2546  | total loss: [1m[32m0.16734[0m[0m | time: 725.929s
| Adam | epoch: 005 | loss: 0.16734 - acc: 0.9462 -- iter: 15300/28000
Training Step: 2547  | total loss: [1m[32m0.18032[0m[0m | time: 727.156s
| Adam | epoch: 005 | loss: 0.18032 - acc: 0.9415 -- iter: 15350/28000
Training Step: 2548  | total loss: [1m[32m0.16767[0m[0m | time: 728.411s
| Adam | epoch: 005 | loss: 0.16767 - acc: 0.9474 -- iter: 15400/28000
Training Step: 2549  | total loss: [1m[32m0.16395[0m[0m | time: 729.665s
| Adam | epoch: 005 | loss: 0.16395 - acc: 0.9487 -- iter: 15450/28000
Training Step: 2550  | total loss: [1m[32m0.16170[0m[0m | time: 759.408s
| Adam | epoch: 005 | loss: 0.16170 - acc: 0.9478 | val_loss: 0.17632 - val_acc: 0.9357 -- iter: 15500/28000
--
Training Step: 2551  | total loss: [1m[32m0.16265[0m[0m | time: 760.690s
| Adam | epoch: 005 | loss: 0.16265 - acc: 0.9490 -- iter: 15550/28000
Training Step: 2552  | total loss: [1m[32m0.15615[0m[0m | time: 761.965s
| Adam | epoch: 005 | loss: 0.15615 - acc: 0.9521 -- iter: 15600/28000
Training Step: 2553  | total loss: [1m[32m0.14738[0m[0m | time: 763.215s
| Adam | epoch: 005 | loss: 0.14738 - acc: 0.9549 -- iter: 15650/28000
Training Step: 2554  | total loss: [1m[32m0.14085[0m[0m | time: 764.447s
| Adam | epoch: 005 | loss: 0.14085 - acc: 0.9554 -- iter: 15700/28000
Training Step: 2555  | total loss: [1m[32m0.13424[0m[0m | time: 765.673s
| Adam | epoch: 005 | loss: 0.13424 - acc: 0.9579 -- iter: 15750/28000
Training Step: 2556  | total loss: [1m[32m0.16023[0m[0m | time: 767.019s
| Adam | epoch: 005 | loss: 0.16023 - acc: 0.9481 -- iter: 15800/28000
Training Step: 2557  | total loss: [1m[32m0.16439[0m[0m | time: 768.345s
| Adam | epoch: 005 | loss: 0.16439 - acc: 0.9433 -- iter: 15850/28000
Training Step: 2558  | total loss: [1m[32m0.16049[0m[0m | time: 769.626s
| Adam | epoch: 005 | loss: 0.16049 - acc: 0.9429 -- iter: 15900/28000
Training Step: 2559  | total loss: [1m[32m0.14978[0m[0m | time: 770.944s
| Adam | epoch: 005 | loss: 0.14978 - acc: 0.9467 -- iter: 15950/28000
Training Step: 2560  | total loss: [1m[32m0.14955[0m[0m | time: 772.217s
| Adam | epoch: 005 | loss: 0.14955 - acc: 0.9440 -- iter: 16000/28000
Training Step: 2561  | total loss: [1m[32m0.14744[0m[0m | time: 773.539s
| Adam | epoch: 005 | loss: 0.14744 - acc: 0.9456 -- iter: 16050/28000
Training Step: 2562  | total loss: [1m[32m0.14156[0m[0m | time: 774.854s
| Adam | epoch: 005 | loss: 0.14156 - acc: 0.9490 -- iter: 16100/28000
Training Step: 2563  | total loss: [1m[32m0.14407[0m[0m | time: 776.098s
| Adam | epoch: 005 | loss: 0.14407 - acc: 0.9461 -- iter: 16150/28000
Training Step: 2564  | total loss: [1m[32m0.13766[0m[0m | time: 777.321s
| Adam | epoch: 005 | loss: 0.13766 - acc: 0.9475 -- iter: 16200/28000
Training Step: 2565  | total loss: [1m[32m0.15613[0m[0m | time: 778.577s
| Adam | epoch: 005 | loss: 0.15613 - acc: 0.9448 -- iter: 16250/28000
Training Step: 2566  | total loss: [1m[32m0.18179[0m[0m | time: 779.815s
| Adam | epoch: 005 | loss: 0.18179 - acc: 0.9363 -- iter: 16300/28000
Training Step: 2567  | total loss: [1m[32m0.18448[0m[0m | time: 781.033s
| Adam | epoch: 005 | loss: 0.18448 - acc: 0.9327 -- iter: 16350/28000
Training Step: 2568  | total loss: [1m[32m0.17485[0m[0m | time: 782.482s
| Adam | epoch: 005 | loss: 0.17485 - acc: 0.9354 -- iter: 16400/28000
Training Step: 2569  | total loss: [1m[32m0.16988[0m[0m | time: 783.830s
| Adam | epoch: 005 | loss: 0.16988 - acc: 0.9379 -- iter: 16450/28000
Training Step: 2570  | total loss: [1m[32m0.17020[0m[0m | time: 785.077s
| Adam | epoch: 005 | loss: 0.17020 - acc: 0.9361 -- iter: 16500/28000
Training Step: 2571  | total loss: [1m[32m0.17563[0m[0m | time: 786.265s
| Adam | epoch: 005 | loss: 0.17563 - acc: 0.9305 -- iter: 16550/28000
Training Step: 2572  | total loss: [1m[32m0.18467[0m[0m | time: 787.585s
| Adam | epoch: 005 | loss: 0.18467 - acc: 0.9214 -- iter: 16600/28000
Training Step: 2573  | total loss: [1m[32m0.18316[0m[0m | time: 788.862s
| Adam | epoch: 005 | loss: 0.18316 - acc: 0.9233 -- iter: 16650/28000
Training Step: 2574  | total loss: [1m[32m0.18024[0m[0m | time: 790.171s
| Adam | epoch: 005 | loss: 0.18024 - acc: 0.9229 -- iter: 16700/28000
Training Step: 2575  | total loss: [1m[32m0.17441[0m[0m | time: 818.267s
| Adam | epoch: 005 | loss: 0.17441 - acc: 0.9247 | val_loss: 0.19853 - val_acc: 0.9255 -- iter: 16750/28000
--
Training Step: 2576  | total loss: [1m[32m0.17439[0m[0m | time: 819.482s
| Adam | epoch: 005 | loss: 0.17439 - acc: 0.9262 -- iter: 16800/28000
Training Step: 2577  | total loss: [1m[32m0.16575[0m[0m | time: 820.837s
| Adam | epoch: 005 | loss: 0.16575 - acc: 0.9296 -- iter: 16850/28000
Training Step: 2578  | total loss: [1m[32m0.16315[0m[0m | time: 822.569s
| Adam | epoch: 005 | loss: 0.16315 - acc: 0.9326 -- iter: 16900/28000
Training Step: 2579  | total loss: [1m[32m0.16960[0m[0m | time: 824.104s
| Adam | epoch: 005 | loss: 0.16960 - acc: 0.9273 -- iter: 16950/28000
Training Step: 2580  | total loss: [1m[32m0.16508[0m[0m | time: 825.813s
| Adam | epoch: 005 | loss: 0.16508 - acc: 0.9306 -- iter: 17000/28000
Training Step: 2581  | total loss: [1m[32m0.15756[0m[0m | time: 827.636s
| Adam | epoch: 005 | loss: 0.15756 - acc: 0.9336 -- iter: 17050/28000
Training Step: 2582  | total loss: [1m[32m0.15436[0m[0m | time: 829.406s
| Adam | epoch: 005 | loss: 0.15436 - acc: 0.9362 -- iter: 17100/28000
Training Step: 2583  | total loss: [1m[32m0.15000[0m[0m | time: 830.938s
| Adam | epoch: 005 | loss: 0.15000 - acc: 0.9406 -- iter: 17150/28000
Training Step: 2584  | total loss: [1m[32m0.15191[0m[0m | time: 832.415s
| Adam | epoch: 005 | loss: 0.15191 - acc: 0.9405 -- iter: 17200/28000
Training Step: 2585  | total loss: [1m[32m0.15671[0m[0m | time: 833.903s
| Adam | epoch: 005 | loss: 0.15671 - acc: 0.9385 -- iter: 17250/28000
Training Step: 2586  | total loss: [1m[32m0.14971[0m[0m | time: 835.514s
| Adam | epoch: 005 | loss: 0.14971 - acc: 0.9446 -- iter: 17300/28000
Training Step: 2587  | total loss: [1m[32m0.14614[0m[0m | time: 836.935s
| Adam | epoch: 005 | loss: 0.14614 - acc: 0.9422 -- iter: 17350/28000
Training Step: 2588  | total loss: [1m[32m0.13581[0m[0m | time: 838.300s
| Adam | epoch: 005 | loss: 0.13581 - acc: 0.9459 -- iter: 17400/28000
Training Step: 2589  | total loss: [1m[32m0.13075[0m[0m | time: 839.667s
| Adam | epoch: 005 | loss: 0.13075 - acc: 0.9473 -- iter: 17450/28000
Training Step: 2590  | total loss: [1m[32m0.12895[0m[0m | time: 840.925s
| Adam | epoch: 005 | loss: 0.12895 - acc: 0.9486 -- iter: 17500/28000
Training Step: 2591  | total loss: [1m[32m0.12642[0m[0m | time: 842.170s
| Adam | epoch: 005 | loss: 0.12642 - acc: 0.9498 -- iter: 17550/28000
Training Step: 2592  | total loss: [1m[32m0.14697[0m[0m | time: 843.536s
| Adam | epoch: 005 | loss: 0.14697 - acc: 0.9468 -- iter: 17600/28000
Training Step: 2593  | total loss: [1m[32m0.14298[0m[0m | time: 844.981s
| Adam | epoch: 005 | loss: 0.14298 - acc: 0.9481 -- iter: 17650/28000
Training Step: 2594  | total loss: [1m[32m0.14062[0m[0m | time: 846.398s
| Adam | epoch: 005 | loss: 0.14062 - acc: 0.9453 -- iter: 17700/28000
Training Step: 2595  | total loss: [1m[32m0.14285[0m[0m | time: 848.192s
| Adam | epoch: 005 | loss: 0.14285 - acc: 0.9408 -- iter: 17750/28000
Training Step: 2596  | total loss: [1m[32m0.13681[0m[0m | time: 849.590s
| Adam | epoch: 005 | loss: 0.13681 - acc: 0.9447 -- iter: 17800/28000
Training Step: 2597  | total loss: [1m[32m0.13780[0m[0m | time: 850.807s
| Adam | epoch: 005 | loss: 0.13780 - acc: 0.9422 -- iter: 17850/28000
Training Step: 2598  | total loss: [1m[32m0.13649[0m[0m | time: 852.527s
| Adam | epoch: 005 | loss: 0.13649 - acc: 0.9420 -- iter: 17900/28000
Training Step: 2599  | total loss: [1m[32m0.15230[0m[0m | time: 854.105s
| Adam | epoch: 005 | loss: 0.15230 - acc: 0.9378 -- iter: 17950/28000
Training Step: 2600  | total loss: [1m[32m0.15162[0m[0m | time: 890.055s
| Adam | epoch: 005 | loss: 0.15162 - acc: 0.9380 | val_loss: 0.19969 - val_acc: 0.9240 -- iter: 18000/28000
--
Training Step: 2601  | total loss: [1m[32m0.14402[0m[0m | time: 891.405s
| Adam | epoch: 005 | loss: 0.14402 - acc: 0.9422 -- iter: 18050/28000
Training Step: 2602  | total loss: [1m[32m0.15437[0m[0m | time: 892.883s
| Adam | epoch: 005 | loss: 0.15437 - acc: 0.9400 -- iter: 18100/28000
Training Step: 2603  | total loss: [1m[32m0.16842[0m[0m | time: 894.637s
| Adam | epoch: 005 | loss: 0.16842 - acc: 0.9340 -- iter: 18150/28000
Training Step: 2604  | total loss: [1m[32m0.17512[0m[0m | time: 896.347s
| Adam | epoch: 005 | loss: 0.17512 - acc: 0.9326 -- iter: 18200/28000
Training Step: 2605  | total loss: [1m[32m0.16660[0m[0m | time: 897.686s
| Adam | epoch: 005 | loss: 0.16660 - acc: 0.9373 -- iter: 18250/28000
Training Step: 2606  | total loss: [1m[32m0.16630[0m[0m | time: 898.976s
| Adam | epoch: 005 | loss: 0.16630 - acc: 0.9396 -- iter: 18300/28000
Training Step: 2607  | total loss: [1m[32m0.15809[0m[0m | time: 900.268s
| Adam | epoch: 005 | loss: 0.15809 - acc: 0.9416 -- iter: 18350/28000
Training Step: 2608  | total loss: [1m[32m0.14952[0m[0m | time: 901.593s
| Adam | epoch: 005 | loss: 0.14952 - acc: 0.9455 -- iter: 18400/28000
Training Step: 2609  | total loss: [1m[32m0.14827[0m[0m | time: 902.895s
| Adam | epoch: 005 | loss: 0.14827 - acc: 0.9449 -- iter: 18450/28000
Training Step: 2610  | total loss: [1m[32m0.15210[0m[0m | time: 904.182s
| Adam | epoch: 005 | loss: 0.15210 - acc: 0.9444 -- iter: 18500/28000
Training Step: 2611  | total loss: [1m[32m0.16562[0m[0m | time: 905.661s
| Adam | epoch: 005 | loss: 0.16562 - acc: 0.9400 -- iter: 18550/28000
Training Step: 2612  | total loss: [1m[32m0.16136[0m[0m | time: 906.801s
| Adam | epoch: 005 | loss: 0.16136 - acc: 0.9400 -- iter: 18600/28000
Training Step: 2613  | total loss: [1m[32m0.16423[0m[0m | time: 907.950s
| Adam | epoch: 005 | loss: 0.16423 - acc: 0.9400 -- iter: 18650/28000
Training Step: 2614  | total loss: [1m[32m0.16321[0m[0m | time: 909.075s
| Adam | epoch: 005 | loss: 0.16321 - acc: 0.9440 -- iter: 18700/28000
Training Step: 2615  | total loss: [1m[32m0.17123[0m[0m | time: 910.346s
| Adam | epoch: 005 | loss: 0.17123 - acc: 0.9436 -- iter: 18750/28000
Training Step: 2616  | total loss: [1m[32m0.16063[0m[0m | time: 911.908s
| Adam | epoch: 005 | loss: 0.16063 - acc: 0.9472 -- iter: 18800/28000
Training Step: 2617  | total loss: [1m[32m0.15292[0m[0m | time: 913.375s
| Adam | epoch: 005 | loss: 0.15292 - acc: 0.9505 -- iter: 18850/28000
Training Step: 2618  | total loss: [1m[32m0.14715[0m[0m | time: 914.792s
| Adam | epoch: 005 | loss: 0.14715 - acc: 0.9535 -- iter: 18900/28000
Training Step: 2619  | total loss: [1m[32m0.15656[0m[0m | time: 916.022s
| Adam | epoch: 005 | loss: 0.15656 - acc: 0.9501 -- iter: 18950/28000
Training Step: 2620  | total loss: [1m[32m0.14600[0m[0m | time: 917.128s
| Adam | epoch: 005 | loss: 0.14600 - acc: 0.9531 -- iter: 19000/28000
Training Step: 2621  | total loss: [1m[32m0.14473[0m[0m | time: 918.260s
| Adam | epoch: 005 | loss: 0.14473 - acc: 0.9518 -- iter: 19050/28000
Training Step: 2622  | total loss: [1m[32m0.15521[0m[0m | time: 919.326s
| Adam | epoch: 005 | loss: 0.15521 - acc: 0.9526 -- iter: 19100/28000
Training Step: 2623  | total loss: [1m[32m0.15368[0m[0m | time: 920.432s
| Adam | epoch: 005 | loss: 0.15368 - acc: 0.9534 -- iter: 19150/28000
Training Step: 2624  | total loss: [1m[32m0.14479[0m[0m | time: 921.505s
| Adam | epoch: 005 | loss: 0.14479 - acc: 0.9560 -- iter: 19200/28000
Training Step: 2625  | total loss: [1m[32m0.13383[0m[0m | time: 953.178s
| Adam | epoch: 005 | loss: 0.13383 - acc: 0.9604 | val_loss: 0.21507 - val_acc: 0.9265 -- iter: 19250/28000
--
Training Step: 2626  | total loss: [1m[32m0.13671[0m[0m | time: 954.415s
| Adam | epoch: 005 | loss: 0.13671 - acc: 0.9584 -- iter: 19300/28000
Training Step: 2627  | total loss: [1m[32m0.14716[0m[0m | time: 955.583s
| Adam | epoch: 005 | loss: 0.14716 - acc: 0.9565 -- iter: 19350/28000
Training Step: 2628  | total loss: [1m[32m0.15153[0m[0m | time: 956.735s
| Adam | epoch: 005 | loss: 0.15153 - acc: 0.9529 -- iter: 19400/28000
Training Step: 2629  | total loss: [1m[32m0.15995[0m[0m | time: 957.901s
| Adam | epoch: 005 | loss: 0.15995 - acc: 0.9476 -- iter: 19450/28000
Training Step: 2630  | total loss: [1m[32m0.15236[0m[0m | time: 959.074s
| Adam | epoch: 005 | loss: 0.15236 - acc: 0.9488 -- iter: 19500/28000
Training Step: 2631  | total loss: [1m[32m0.14770[0m[0m | time: 960.218s
| Adam | epoch: 005 | loss: 0.14770 - acc: 0.9520 -- iter: 19550/28000
Training Step: 2632  | total loss: [1m[32m0.14720[0m[0m | time: 961.372s
| Adam | epoch: 005 | loss: 0.14720 - acc: 0.9508 -- iter: 19600/28000
Training Step: 2633  | total loss: [1m[32m0.14633[0m[0m | time: 962.550s
| Adam | epoch: 005 | loss: 0.14633 - acc: 0.9497 -- iter: 19650/28000
Training Step: 2634  | total loss: [1m[32m0.14146[0m[0m | time: 963.702s
| Adam | epoch: 005 | loss: 0.14146 - acc: 0.9507 -- iter: 19700/28000
Training Step: 2635  | total loss: [1m[32m0.15092[0m[0m | time: 964.881s
| Adam | epoch: 005 | loss: 0.15092 - acc: 0.9476 -- iter: 19750/28000
Training Step: 2636  | total loss: [1m[32m0.14343[0m[0m | time: 966.039s
| Adam | epoch: 005 | loss: 0.14343 - acc: 0.9509 -- iter: 19800/28000
Training Step: 2637  | total loss: [1m[32m0.14003[0m[0m | time: 967.197s
| Adam | epoch: 005 | loss: 0.14003 - acc: 0.9518 -- iter: 19850/28000
Training Step: 2638  | total loss: [1m[32m0.13744[0m[0m | time: 968.358s
| Adam | epoch: 005 | loss: 0.13744 - acc: 0.9526 -- iter: 19900/28000
Training Step: 2639  | total loss: [1m[32m0.14608[0m[0m | time: 969.506s
| Adam | epoch: 005 | loss: 0.14608 - acc: 0.9493 -- iter: 19950/28000
Training Step: 2640  | total loss: [1m[32m0.14285[0m[0m | time: 970.661s
| Adam | epoch: 005 | loss: 0.14285 - acc: 0.9504 -- iter: 20000/28000
Training Step: 2641  | total loss: [1m[32m0.13659[0m[0m | time: 971.825s
| Adam | epoch: 005 | loss: 0.13659 - acc: 0.9534 -- iter: 20050/28000
Training Step: 2642  | total loss: [1m[32m0.13907[0m[0m | time: 972.974s
| Adam | epoch: 005 | loss: 0.13907 - acc: 0.9560 -- iter: 20100/28000
Training Step: 2643  | total loss: [1m[32m0.12808[0m[0m | time: 974.122s
| Adam | epoch: 005 | loss: 0.12808 - acc: 0.9584 -- iter: 20150/28000
Training Step: 2644  | total loss: [1m[32m0.12301[0m[0m | time: 975.273s
| Adam | epoch: 005 | loss: 0.12301 - acc: 0.9586 -- iter: 20200/28000
Training Step: 2645  | total loss: [1m[32m0.12216[0m[0m | time: 976.436s
| Adam | epoch: 005 | loss: 0.12216 - acc: 0.9587 -- iter: 20250/28000
Training Step: 2646  | total loss: [1m[32m0.11873[0m[0m | time: 977.588s
| Adam | epoch: 005 | loss: 0.11873 - acc: 0.9609 -- iter: 20300/28000
Training Step: 2647  | total loss: [1m[32m0.11210[0m[0m | time: 978.754s
| Adam | epoch: 005 | loss: 0.11210 - acc: 0.9628 -- iter: 20350/28000
Training Step: 2648  | total loss: [1m[32m0.11375[0m[0m | time: 979.922s
| Adam | epoch: 005 | loss: 0.11375 - acc: 0.9625 -- iter: 20400/28000
Training Step: 2649  | total loss: [1m[32m0.11513[0m[0m | time: 981.085s
| Adam | epoch: 005 | loss: 0.11513 - acc: 0.9622 -- iter: 20450/28000
Training Step: 2650  | total loss: [1m[32m0.10902[0m[0m | time: 1009.121s
| Adam | epoch: 005 | loss: 0.10902 - acc: 0.9660 | val_loss: 0.18535 - val_acc: 0.9385 -- iter: 20500/28000
--
Training Step: 2651  | total loss: [1m[32m0.10363[0m[0m | time: 1010.311s
| Adam | epoch: 005 | loss: 0.10363 - acc: 0.9674 -- iter: 20550/28000
Training Step: 2652  | total loss: [1m[32m0.09952[0m[0m | time: 1011.582s
| Adam | epoch: 005 | loss: 0.09952 - acc: 0.9687 -- iter: 20600/28000
Training Step: 2653  | total loss: [1m[32m0.11073[0m[0m | time: 1012.803s
| Adam | epoch: 005 | loss: 0.11073 - acc: 0.9638 -- iter: 20650/28000
Training Step: 2654  | total loss: [1m[32m0.11579[0m[0m | time: 1014.008s
| Adam | epoch: 005 | loss: 0.11579 - acc: 0.9574 -- iter: 20700/28000
Training Step: 2655  | total loss: [1m[32m0.11357[0m[0m | time: 1015.207s
| Adam | epoch: 005 | loss: 0.11357 - acc: 0.9557 -- iter: 20750/28000
Training Step: 2656  | total loss: [1m[32m0.10970[0m[0m | time: 1016.371s
| Adam | epoch: 005 | loss: 0.10970 - acc: 0.9581 -- iter: 20800/28000
Training Step: 2657  | total loss: [1m[32m0.11438[0m[0m | time: 1017.548s
| Adam | epoch: 005 | loss: 0.11438 - acc: 0.9543 -- iter: 20850/28000
Training Step: 2658  | total loss: [1m[32m0.11667[0m[0m | time: 1018.717s
| Adam | epoch: 005 | loss: 0.11667 - acc: 0.9549 -- iter: 20900/28000
Training Step: 2659  | total loss: [1m[32m0.10813[0m[0m | time: 1020.044s
| Adam | epoch: 005 | loss: 0.10813 - acc: 0.9594 -- iter: 20950/28000
Training Step: 2660  | total loss: [1m[32m0.10680[0m[0m | time: 1021.267s
| Adam | epoch: 005 | loss: 0.10680 - acc: 0.9594 -- iter: 21000/28000
Training Step: 2661  | total loss: [1m[32m0.11900[0m[0m | time: 1022.434s
| Adam | epoch: 005 | loss: 0.11900 - acc: 0.9575 -- iter: 21050/28000
Training Step: 2662  | total loss: [1m[32m0.11496[0m[0m | time: 1023.573s
| Adam | epoch: 005 | loss: 0.11496 - acc: 0.9598 -- iter: 21100/28000
Training Step: 2663  | total loss: [1m[32m0.10902[0m[0m | time: 1024.731s
| Adam | epoch: 005 | loss: 0.10902 - acc: 0.9618 -- iter: 21150/28000
Training Step: 2664  | total loss: [1m[32m0.11822[0m[0m | time: 1025.999s
| Adam | epoch: 005 | loss: 0.11822 - acc: 0.9596 -- iter: 21200/28000
Training Step: 2665  | total loss: [1m[32m0.11214[0m[0m | time: 1027.267s
| Adam | epoch: 005 | loss: 0.11214 - acc: 0.9596 -- iter: 21250/28000
Training Step: 2666  | total loss: [1m[32m0.10487[0m[0m | time: 1028.449s
| Adam | epoch: 005 | loss: 0.10487 - acc: 0.9637 -- iter: 21300/28000
Training Step: 2667  | total loss: [1m[32m0.11278[0m[0m | time: 1029.512s
| Adam | epoch: 005 | loss: 0.11278 - acc: 0.9573 -- iter: 21350/28000
Training Step: 2668  | total loss: [1m[32m0.11822[0m[0m | time: 1030.795s
| Adam | epoch: 005 | loss: 0.11822 - acc: 0.9576 -- iter: 21400/28000
Training Step: 2669  | total loss: [1m[32m0.11581[0m[0m | time: 1032.109s
| Adam | epoch: 005 | loss: 0.11581 - acc: 0.9578 -- iter: 21450/28000
Training Step: 2670  | total loss: [1m[32m0.12687[0m[0m | time: 1033.390s
| Adam | epoch: 005 | loss: 0.12687 - acc: 0.9580 -- iter: 21500/28000
Training Step: 2671  | total loss: [1m[32m0.14197[0m[0m | time: 1034.622s
| Adam | epoch: 005 | loss: 0.14197 - acc: 0.9562 -- iter: 21550/28000
Training Step: 2672  | total loss: [1m[32m0.15053[0m[0m | time: 1035.843s
| Adam | epoch: 005 | loss: 0.15053 - acc: 0.9546 -- iter: 21600/28000
Training Step: 2673  | total loss: [1m[32m0.15337[0m[0m | time: 1037.439s
| Adam | epoch: 005 | loss: 0.15337 - acc: 0.9491 -- iter: 21650/28000
Training Step: 2674  | total loss: [1m[32m0.14946[0m[0m | time: 1039.252s
| Adam | epoch: 005 | loss: 0.14946 - acc: 0.9482 -- iter: 21700/28000
Training Step: 2675  | total loss: [1m[32m0.15462[0m[0m | time: 1070.560s
| Adam | epoch: 005 | loss: 0.15462 - acc: 0.9434 | val_loss: 0.17924 - val_acc: 0.9356 -- iter: 21750/28000
--
Training Step: 2676  | total loss: [1m[32m0.15475[0m[0m | time: 1071.787s
| Adam | epoch: 005 | loss: 0.15475 - acc: 0.9411 -- iter: 21800/28000
Training Step: 2677  | total loss: [1m[32m0.14738[0m[0m | time: 1072.949s
| Adam | epoch: 005 | loss: 0.14738 - acc: 0.9410 -- iter: 21850/28000
Training Step: 2678  | total loss: [1m[32m0.14232[0m[0m | time: 1074.128s
| Adam | epoch: 005 | loss: 0.14232 - acc: 0.9449 -- iter: 21900/28000
Training Step: 2679  | total loss: [1m[32m0.14421[0m[0m | time: 1075.273s
| Adam | epoch: 005 | loss: 0.14421 - acc: 0.9484 -- iter: 21950/28000
Training Step: 2680  | total loss: [1m[32m0.14791[0m[0m | time: 1076.509s
| Adam | epoch: 005 | loss: 0.14791 - acc: 0.9475 -- iter: 22000/28000
Training Step: 2681  | total loss: [1m[32m0.16152[0m[0m | time: 1077.712s
| Adam | epoch: 005 | loss: 0.16152 - acc: 0.9428 -- iter: 22050/28000
Training Step: 2682  | total loss: [1m[32m0.15159[0m[0m | time: 1078.969s
| Adam | epoch: 005 | loss: 0.15159 - acc: 0.9465 -- iter: 22100/28000
Training Step: 2683  | total loss: [1m[32m0.15058[0m[0m | time: 1080.159s
| Adam | epoch: 005 | loss: 0.15058 - acc: 0.9479 -- iter: 22150/28000
Training Step: 2684  | total loss: [1m[32m0.14654[0m[0m | time: 1081.343s
| Adam | epoch: 005 | loss: 0.14654 - acc: 0.9491 -- iter: 22200/28000
Training Step: 2685  | total loss: [1m[32m0.14219[0m[0m | time: 1082.513s
| Adam | epoch: 005 | loss: 0.14219 - acc: 0.9462 -- iter: 22250/28000
Training Step: 2686  | total loss: [1m[32m0.13369[0m[0m | time: 1083.943s
| Adam | epoch: 005 | loss: 0.13369 - acc: 0.9515 -- iter: 22300/28000
Training Step: 2687  | total loss: [1m[32m0.13744[0m[0m | time: 1085.645s
| Adam | epoch: 005 | loss: 0.13744 - acc: 0.9484 -- iter: 22350/28000
Training Step: 2688  | total loss: [1m[32m0.13327[0m[0m | time: 1086.990s
| Adam | epoch: 005 | loss: 0.13327 - acc: 0.9516 -- iter: 22400/28000
Training Step: 2689  | total loss: [1m[32m0.13247[0m[0m | time: 1088.520s
| Adam | epoch: 005 | loss: 0.13247 - acc: 0.9544 -- iter: 22450/28000
Training Step: 2690  | total loss: [1m[32m0.13846[0m[0m | time: 1089.951s
| Adam | epoch: 005 | loss: 0.13846 - acc: 0.9510 -- iter: 22500/28000
Training Step: 2691  | total loss: [1m[32m0.14324[0m[0m | time: 1091.444s
| Adam | epoch: 005 | loss: 0.14324 - acc: 0.9479 -- iter: 22550/28000
Training Step: 2692  | total loss: [1m[32m0.13961[0m[0m | time: 1092.855s
| Adam | epoch: 005 | loss: 0.13961 - acc: 0.9491 -- iter: 22600/28000
Training Step: 2693  | total loss: [1m[32m0.12953[0m[0m | time: 1094.164s
| Adam | epoch: 005 | loss: 0.12953 - acc: 0.9542 -- iter: 22650/28000
Training Step: 2694  | total loss: [1m[32m0.12920[0m[0m | time: 1095.418s
| Adam | epoch: 005 | loss: 0.12920 - acc: 0.9568 -- iter: 22700/28000
Training Step: 2695  | total loss: [1m[32m0.12475[0m[0m | time: 1096.820s
| Adam | epoch: 005 | loss: 0.12475 - acc: 0.9591 -- iter: 22750/28000
Training Step: 2696  | total loss: [1m[32m0.11759[0m[0m | time: 1098.250s
| Adam | epoch: 005 | loss: 0.11759 - acc: 0.9632 -- iter: 22800/28000
Training Step: 2697  | total loss: [1m[32m0.11918[0m[0m | time: 1099.748s
| Adam | epoch: 005 | loss: 0.11918 - acc: 0.9609 -- iter: 22850/28000
Training Step: 2698  | total loss: [1m[32m0.13343[0m[0m | time: 1101.010s
| Adam | epoch: 005 | loss: 0.13343 - acc: 0.9548 -- iter: 22900/28000
Training Step: 2699  | total loss: [1m[32m0.13784[0m[0m | time: 1102.129s
| Adam | epoch: 005 | loss: 0.13784 - acc: 0.9513 -- iter: 22950/28000
Training Step: 2700  | total loss: [1m[32m0.13611[0m[0m | time: 1133.446s
| Adam | epoch: 005 | loss: 0.13611 - acc: 0.9502 | val_loss: 0.18248 - val_acc: 0.9348 -- iter: 23000/28000
--
Training Step: 2701  | total loss: [1m[32m0.13124[0m[0m | time: 1134.719s
| Adam | epoch: 005 | loss: 0.13124 - acc: 0.9511 -- iter: 23050/28000
Training Step: 2702  | total loss: [1m[32m0.12560[0m[0m | time: 1135.867s
| Adam | epoch: 005 | loss: 0.12560 - acc: 0.9560 -- iter: 23100/28000
Training Step: 2703  | total loss: [1m[32m0.12215[0m[0m | time: 1137.061s
| Adam | epoch: 005 | loss: 0.12215 - acc: 0.9564 -- iter: 23150/28000
Training Step: 2704  | total loss: [1m[32m0.12872[0m[0m | time: 1138.286s
| Adam | epoch: 005 | loss: 0.12872 - acc: 0.9508 -- iter: 23200/28000
Training Step: 2705  | total loss: [1m[32m0.12317[0m[0m | time: 1139.572s
| Adam | epoch: 005 | loss: 0.12317 - acc: 0.9537 -- iter: 23250/28000
Training Step: 2706  | total loss: [1m[32m0.12183[0m[0m | time: 1141.119s
| Adam | epoch: 005 | loss: 0.12183 - acc: 0.9503 -- iter: 23300/28000
Training Step: 2707  | total loss: [1m[32m0.11561[0m[0m | time: 1142.523s
| Adam | epoch: 005 | loss: 0.11561 - acc: 0.9513 -- iter: 23350/28000
Training Step: 2708  | total loss: [1m[32m0.11154[0m[0m | time: 1143.764s
| Adam | epoch: 005 | loss: 0.11154 - acc: 0.9562 -- iter: 23400/28000
Training Step: 2709  | total loss: [1m[32m0.10514[0m[0m | time: 1145.096s
| Adam | epoch: 005 | loss: 0.10514 - acc: 0.9586 -- iter: 23450/28000
Training Step: 2710  | total loss: [1m[32m0.11182[0m[0m | time: 1146.401s
| Adam | epoch: 005 | loss: 0.11182 - acc: 0.9567 -- iter: 23500/28000
Training Step: 2711  | total loss: [1m[32m0.11322[0m[0m | time: 1147.631s
| Adam | epoch: 005 | loss: 0.11322 - acc: 0.9550 -- iter: 23550/28000
Training Step: 2712  | total loss: [1m[32m0.10503[0m[0m | time: 1148.661s
| Adam | epoch: 005 | loss: 0.10503 - acc: 0.9575 -- iter: 23600/28000
Training Step: 2713  | total loss: [1m[32m0.09958[0m[0m | time: 1149.751s
| Adam | epoch: 005 | loss: 0.09958 - acc: 0.9598 -- iter: 23650/28000
Training Step: 2714  | total loss: [1m[32m0.10556[0m[0m | time: 1151.105s
| Adam | epoch: 005 | loss: 0.10556 - acc: 0.9558 -- iter: 23700/28000
Training Step: 2715  | total loss: [1m[32m0.10218[0m[0m | time: 1152.324s
| Adam | epoch: 005 | loss: 0.10218 - acc: 0.9582 -- iter: 23750/28000
Training Step: 2716  | total loss: [1m[32m0.09659[0m[0m | time: 1153.649s
| Adam | epoch: 005 | loss: 0.09659 - acc: 0.9604 -- iter: 23800/28000
Training Step: 2717  | total loss: [1m[32m0.09172[0m[0m | time: 1155.080s
| Adam | epoch: 005 | loss: 0.09172 - acc: 0.9624 -- iter: 23850/28000
Training Step: 2718  | total loss: [1m[32m0.08870[0m[0m | time: 1156.396s
| Adam | epoch: 005 | loss: 0.08870 - acc: 0.9661 -- iter: 23900/28000
Training Step: 2719  | total loss: [1m[32m0.08490[0m[0m | time: 1157.619s
| Adam | epoch: 005 | loss: 0.08490 - acc: 0.9675 -- iter: 23950/28000
Training Step: 2720  | total loss: [1m[32m0.10821[0m[0m | time: 1158.646s
| Adam | epoch: 005 | loss: 0.10821 - acc: 0.9648 -- iter: 24000/28000
Training Step: 2721  | total loss: [1m[32m0.10064[0m[0m | time: 1159.815s
| Adam | epoch: 005 | loss: 0.10064 - acc: 0.9663 -- iter: 24050/28000
Training Step: 2722  | total loss: [1m[32m0.10646[0m[0m | time: 1161.361s
| Adam | epoch: 005 | loss: 0.10646 - acc: 0.9637 -- iter: 24100/28000
Training Step: 2723  | total loss: [1m[32m0.10158[0m[0m | time: 1162.895s
| Adam | epoch: 005 | loss: 0.10158 - acc: 0.9633 -- iter: 24150/28000
Training Step: 2724  | total loss: [1m[32m0.10066[0m[0m | time: 1164.125s
| Adam | epoch: 005 | loss: 0.10066 - acc: 0.9610 -- iter: 24200/28000
Training Step: 2725  | total loss: [1m[32m0.10234[0m[0m | time: 1191.863s
| Adam | epoch: 005 | loss: 0.10234 - acc: 0.9609 | val_loss: 0.20207 - val_acc: 0.9347 -- iter: 24250/28000
--
Training Step: 2726  | total loss: [1m[32m0.10854[0m[0m | time: 1192.953s
| Adam | epoch: 005 | loss: 0.10854 - acc: 0.9588 -- iter: 24300/28000
Training Step: 2727  | total loss: [1m[32m0.10106[0m[0m | time: 1193.998s
| Adam | epoch: 005 | loss: 0.10106 - acc: 0.9629 -- iter: 24350/28000
Training Step: 2728  | total loss: [1m[32m0.11628[0m[0m | time: 1195.144s
| Adam | epoch: 005 | loss: 0.11628 - acc: 0.9626 -- iter: 24400/28000
Training Step: 2729  | total loss: [1m[32m0.14980[0m[0m | time: 1196.250s
| Adam | epoch: 005 | loss: 0.14980 - acc: 0.9583 -- iter: 24450/28000
Training Step: 2730  | total loss: [1m[32m0.15132[0m[0m | time: 1197.294s
| Adam | epoch: 005 | loss: 0.15132 - acc: 0.9525 -- iter: 24500/28000
Training Step: 2731  | total loss: [1m[32m0.16210[0m[0m | time: 1198.327s
| Adam | epoch: 005 | loss: 0.16210 - acc: 0.9533 -- iter: 24550/28000
Training Step: 2732  | total loss: [1m[32m0.17407[0m[0m | time: 1199.375s
| Adam | epoch: 005 | loss: 0.17407 - acc: 0.9459 -- iter: 24600/28000
Training Step: 2733  | total loss: [1m[32m0.16604[0m[0m | time: 1200.422s
| Adam | epoch: 005 | loss: 0.16604 - acc: 0.9453 -- iter: 24650/28000
Training Step: 2734  | total loss: [1m[32m0.16962[0m[0m | time: 1201.567s
| Adam | epoch: 005 | loss: 0.16962 - acc: 0.9448 -- iter: 24700/28000
Training Step: 2735  | total loss: [1m[32m0.16222[0m[0m | time: 1202.704s
| Adam | epoch: 005 | loss: 0.16222 - acc: 0.9463 -- iter: 24750/28000
Training Step: 2736  | total loss: [1m[32m0.15657[0m[0m | time: 1203.801s
| Adam | epoch: 005 | loss: 0.15657 - acc: 0.9497 -- iter: 24800/28000
Training Step: 2737  | total loss: [1m[32m0.15779[0m[0m | time: 1204.926s
| Adam | epoch: 005 | loss: 0.15779 - acc: 0.9427 -- iter: 24850/28000
Training Step: 2738  | total loss: [1m[32m0.15618[0m[0m | time: 1206.050s
| Adam | epoch: 005 | loss: 0.15618 - acc: 0.9425 -- iter: 24900/28000
Training Step: 2739  | total loss: [1m[32m0.15522[0m[0m | time: 1207.259s
| Adam | epoch: 005 | loss: 0.15522 - acc: 0.9442 -- iter: 24950/28000
Training Step: 2740  | total loss: [1m[32m0.15664[0m[0m | time: 1208.404s
| Adam | epoch: 005 | loss: 0.15664 - acc: 0.9438 -- iter: 25000/28000
Training Step: 2741  | total loss: [1m[32m0.15014[0m[0m | time: 1209.441s
| Adam | epoch: 005 | loss: 0.15014 - acc: 0.9454 -- iter: 25050/28000
Training Step: 2742  | total loss: [1m[32m0.14337[0m[0m | time: 1210.591s
| Adam | epoch: 005 | loss: 0.14337 - acc: 0.9489 -- iter: 25100/28000
Training Step: 2743  | total loss: [1m[32m0.14717[0m[0m | time: 1211.655s
| Adam | epoch: 005 | loss: 0.14717 - acc: 0.9500 -- iter: 25150/28000
Training Step: 2744  | total loss: [1m[32m0.15855[0m[0m | time: 1212.742s
| Adam | epoch: 005 | loss: 0.15855 - acc: 0.9470 -- iter: 25200/28000
Training Step: 2745  | total loss: [1m[32m0.15551[0m[0m | time: 1213.990s
| Adam | epoch: 005 | loss: 0.15551 - acc: 0.9443 -- iter: 25250/28000
Training Step: 2746  | total loss: [1m[32m0.14867[0m[0m | time: 1215.116s
| Adam | epoch: 005 | loss: 0.14867 - acc: 0.9439 -- iter: 25300/28000
Training Step: 2747  | total loss: [1m[32m0.14117[0m[0m | time: 1216.209s
| Adam | epoch: 005 | loss: 0.14117 - acc: 0.9455 -- iter: 25350/28000
Training Step: 2748  | total loss: [1m[32m0.13234[0m[0m | time: 1217.310s
| Adam | epoch: 005 | loss: 0.13234 - acc: 0.9509 -- iter: 25400/28000
Training Step: 2749  | total loss: [1m[32m0.12778[0m[0m | time: 1218.488s
| Adam | epoch: 005 | loss: 0.12778 - acc: 0.9518 -- iter: 25450/28000
Training Step: 2750  | total loss: [1m[32m0.14962[0m[0m | time: 1244.949s
| Adam | epoch: 005 | loss: 0.14962 - acc: 0.9446 | val_loss: 0.17779 - val_acc: 0.9377 -- iter: 25500/28000
--
Training Step: 2751  | total loss: [1m[32m0.16039[0m[0m | time: 1246.267s
| Adam | epoch: 005 | loss: 0.16039 - acc: 0.9442 -- iter: 25550/28000
Training Step: 2752  | total loss: [1m[32m0.15864[0m[0m | time: 1247.668s
| Adam | epoch: 005 | loss: 0.15864 - acc: 0.9438 -- iter: 25600/28000
Training Step: 2753  | total loss: [1m[32m0.15242[0m[0m | time: 1249.015s
| Adam | epoch: 005 | loss: 0.15242 - acc: 0.9474 -- iter: 25650/28000
Training Step: 2754  | total loss: [1m[32m0.14549[0m[0m | time: 1250.209s
| Adam | epoch: 005 | loss: 0.14549 - acc: 0.9486 -- iter: 25700/28000
Training Step: 2755  | total loss: [1m[32m0.13909[0m[0m | time: 1251.225s
| Adam | epoch: 005 | loss: 0.13909 - acc: 0.9498 -- iter: 25750/28000
Training Step: 2756  | total loss: [1m[32m0.14197[0m[0m | time: 1252.484s
| Adam | epoch: 005 | loss: 0.14197 - acc: 0.9468 -- iter: 25800/28000
Training Step: 2757  | total loss: [1m[32m0.15649[0m[0m | time: 1253.680s
| Adam | epoch: 005 | loss: 0.15649 - acc: 0.9421 -- iter: 25850/28000
Training Step: 2758  | total loss: [1m[32m0.14486[0m[0m | time: 1254.874s
| Adam | epoch: 005 | loss: 0.14486 - acc: 0.9479 -- iter: 25900/28000
Training Step: 2759  | total loss: [1m[32m0.14381[0m[0m | time: 1256.038s
| Adam | epoch: 005 | loss: 0.14381 - acc: 0.9491 -- iter: 25950/28000
Training Step: 2760  | total loss: [1m[32m0.14239[0m[0m | time: 1257.084s
| Adam | epoch: 005 | loss: 0.14239 - acc: 0.9522 -- iter: 26000/28000
Training Step: 2761  | total loss: [1m[32m0.13587[0m[0m | time: 1258.217s
| Adam | epoch: 005 | loss: 0.13587 - acc: 0.9550 -- iter: 26050/28000
Training Step: 2762  | total loss: [1m[32m0.14293[0m[0m | time: 1259.533s
| Adam | epoch: 005 | loss: 0.14293 - acc: 0.9535 -- iter: 26100/28000
Training Step: 2763  | total loss: [1m[32m0.13530[0m[0m | time: 1260.832s
| Adam | epoch: 005 | loss: 0.13530 - acc: 0.9561 -- iter: 26150/28000
Training Step: 2764  | total loss: [1m[32m0.14245[0m[0m | time: 1262.135s
| Adam | epoch: 005 | loss: 0.14245 - acc: 0.9525 -- iter: 26200/28000
Training Step: 2765  | total loss: [1m[32m0.13809[0m[0m | time: 1263.322s
| Adam | epoch: 005 | loss: 0.13809 - acc: 0.9553 -- iter: 26250/28000
Training Step: 2766  | total loss: [1m[32m0.13119[0m[0m | time: 1264.447s
| Adam | epoch: 005 | loss: 0.13119 - acc: 0.9557 -- iter: 26300/28000
Training Step: 2767  | total loss: [1m[32m0.12812[0m[0m | time: 1265.544s
| Adam | epoch: 005 | loss: 0.12812 - acc: 0.9562 -- iter: 26350/28000
Training Step: 2768  | total loss: [1m[32m0.12124[0m[0m | time: 1266.763s
| Adam | epoch: 005 | loss: 0.12124 - acc: 0.9606 -- iter: 26400/28000
Training Step: 2769  | total loss: [1m[32m0.11081[0m[0m | time: 1267.845s
| Adam | epoch: 005 | loss: 0.11081 - acc: 0.9645 -- iter: 26450/28000
Training Step: 2770  | total loss: [1m[32m0.10950[0m[0m | time: 1268.887s
| Adam | epoch: 005 | loss: 0.10950 - acc: 0.9660 -- iter: 26500/28000
Training Step: 2771  | total loss: [1m[32m0.10958[0m[0m | time: 1269.938s
| Adam | epoch: 005 | loss: 0.10958 - acc: 0.9634 -- iter: 26550/28000
Training Step: 2772  | total loss: [1m[32m0.12911[0m[0m | time: 1271.183s
| Adam | epoch: 005 | loss: 0.12911 - acc: 0.9551 -- iter: 26600/28000
Training Step: 2773  | total loss: [1m[32m0.14183[0m[0m | time: 1272.236s
| Adam | epoch: 005 | loss: 0.14183 - acc: 0.9516 -- iter: 26650/28000
Training Step: 2774  | total loss: [1m[32m0.14160[0m[0m | time: 1273.378s
| Adam | epoch: 005 | loss: 0.14160 - acc: 0.9504 -- iter: 26700/28000
Training Step: 2775  | total loss: [1m[32m0.13641[0m[0m | time: 1300.132s
| Adam | epoch: 005 | loss: 0.13641 - acc: 0.9514 | val_loss: 0.18174 - val_acc: 0.9357 -- iter: 26750/28000
--
Training Step: 2776  | total loss: [1m[32m0.13182[0m[0m | time: 1301.222s
| Adam | epoch: 005 | loss: 0.13182 - acc: 0.9522 -- iter: 26800/28000
Training Step: 2777  | total loss: [1m[32m0.12207[0m[0m | time: 1302.347s
| Adam | epoch: 005 | loss: 0.12207 - acc: 0.9570 -- iter: 26850/28000
Training Step: 2778  | total loss: [1m[32m0.11711[0m[0m | time: 1303.483s
| Adam | epoch: 005 | loss: 0.11711 - acc: 0.9573 -- iter: 26900/28000
Training Step: 2779  | total loss: [1m[32m0.10832[0m[0m | time: 1304.662s
| Adam | epoch: 005 | loss: 0.10832 - acc: 0.9616 -- iter: 26950/28000
Training Step: 2780  | total loss: [1m[32m0.11546[0m[0m | time: 1305.827s
| Adam | epoch: 005 | loss: 0.11546 - acc: 0.9614 -- iter: 27000/28000
Training Step: 2781  | total loss: [1m[32m0.12416[0m[0m | time: 1306.991s
| Adam | epoch: 005 | loss: 0.12416 - acc: 0.9553 -- iter: 27050/28000
Training Step: 2782  | total loss: [1m[32m0.12438[0m[0m | time: 1308.216s
| Adam | epoch: 005 | loss: 0.12438 - acc: 0.9518 -- iter: 27100/28000
Training Step: 2783  | total loss: [1m[32m0.12819[0m[0m | time: 1309.271s
| Adam | epoch: 005 | loss: 0.12819 - acc: 0.9466 -- iter: 27150/28000
Training Step: 2784  | total loss: [1m[32m0.12894[0m[0m | time: 1310.403s
| Adam | epoch: 005 | loss: 0.12894 - acc: 0.9439 -- iter: 27200/28000
Training Step: 2785  | total loss: [1m[32m0.12398[0m[0m | time: 1311.530s
| Adam | epoch: 005 | loss: 0.12398 - acc: 0.9455 -- iter: 27250/28000
Training Step: 2786  | total loss: [1m[32m0.13635[0m[0m | time: 1312.680s
| Adam | epoch: 005 | loss: 0.13635 - acc: 0.9430 -- iter: 27300/28000
Training Step: 2787  | total loss: [1m[32m0.12942[0m[0m | time: 1313.938s
| Adam | epoch: 005 | loss: 0.12942 - acc: 0.9467 -- iter: 27350/28000
Training Step: 2788  | total loss: [1m[32m0.13670[0m[0m | time: 1315.190s
| Adam | epoch: 005 | loss: 0.13670 - acc: 0.9460 -- iter: 27400/28000
Training Step: 2789  | total loss: [1m[32m0.13122[0m[0m | time: 1316.297s
| Adam | epoch: 005 | loss: 0.13122 - acc: 0.9474 -- iter: 27450/28000
Training Step: 2790  | total loss: [1m[32m0.14401[0m[0m | time: 1317.351s
| Adam | epoch: 005 | loss: 0.14401 - acc: 0.9447 -- iter: 27500/28000
Training Step: 2791  | total loss: [1m[32m0.14543[0m[0m | time: 1318.432s
| Adam | epoch: 005 | loss: 0.14543 - acc: 0.9462 -- iter: 27550/28000
Training Step: 2792  | total loss: [1m[32m0.13806[0m[0m | time: 1319.511s
| Adam | epoch: 005 | loss: 0.13806 - acc: 0.9496 -- iter: 27600/28000
Training Step: 2793  | total loss: [1m[32m0.15676[0m[0m | time: 1320.930s
| Adam | epoch: 005 | loss: 0.15676 - acc: 0.9426 -- iter: 27650/28000
Training Step: 2794  | total loss: [1m[32m0.14651[0m[0m | time: 1322.308s
| Adam | epoch: 005 | loss: 0.14651 - acc: 0.9464 -- iter: 27700/28000
Training Step: 2795  | total loss: [1m[32m0.14095[0m[0m | time: 1323.509s
| Adam | epoch: 005 | loss: 0.14095 - acc: 0.9477 -- iter: 27750/28000
Training Step: 2796  | total loss: [1m[32m0.13179[0m[0m | time: 1324.690s
| Adam | epoch: 005 | loss: 0.13179 - acc: 0.9530 -- iter: 27800/28000
Training Step: 2797  | total loss: [1m[32m0.14826[0m[0m | time: 1325.952s
| Adam | epoch: 005 | loss: 0.14826 - acc: 0.9517 -- iter: 27850/28000
Training Step: 2798  | total loss: [1m[32m0.16988[0m[0m | time: 1327.343s
| Adam | epoch: 005 | loss: 0.16988 - acc: 0.9485 -- iter: 27900/28000
Training Step: 2799  | total loss: [1m[32m0.16776[0m[0m | time: 1328.783s
| Adam | epoch: 005 | loss: 0.16776 - acc: 0.9476 -- iter: 27950/28000
Training Step: 2800  | total loss: [1m[32m0.16506[0m[0m | time: 1357.748s
| Adam | epoch: 005 | loss: 0.16506 - acc: 0.9489 | val_loss: 0.18672 - val_acc: 0.9312 -- iter: 28000/28000
--
Training Done & Model Saved...
